[
  {
    "name": "Google AI Courses",
    "link": "https://www.cloudskillsboost.google/paths/118",
    "difficulty": "Beginner",
    "type": "Course",
    "description": "A series of 5 courses offered by Google to learn about generative AI from the ground up, starting with an introduction to AI and progressing to a solid understanding of the field .",
    "categories": [
      "Introduction to AI and Fundamental Concepts",
      "Generative AI"
    ]
  },
  {
    "name": "Microsoft AI Course",
    "link": "https://microsoft.github.io/AI-For-Beginners/",
    "difficulty": "Beginner",
    "type": "Course",
    "description": "Covers the basics of AI, as well as more advanced topics such as neural networks and deep learning .",
    "categories": [
      "Introduction to AI and Fundamental Concepts",
      "Machine Learning",
      "Deep Learning"
    ]
  },
  {
    "name": "Introduction to AI with Python (Harvard University)",
    "link": "https://www.edx.org/learn/artificial-intelligence/harvard-university-cs50-s-introduction-to-artificial-intelligence-with-python",
    "difficulty": "Beginner",
    "type": "Course",
    "description": "A comprehensive 7-week course that explores the fundamental concepts and algorithms of AI, concluding with knowledge of AI principles and machine learning libraries .",
    "categories": [
      "Introduction to AI and Fundamental Concepts",
      "Machine Learning"
    ]
  },
  {
    "name": "Generative AI for Everyone",
    "link": "https://www.deeplearning.ai/courses/generative-ai-for-everyone/",
    "difficulty": "Beginner",
    "type": "Course",
    "description": "A free resource to get started with Generative AI .",
    "categories": [
      "Introduction to AI and Fundamental Concepts",
      "Generative AI"
    ]
  },
  {
    "name": "Getting Started with LLMs",
    "link": "https://www.deeplearning.ai/short-courses/getting-started-with-mistral/",
    "difficulty": "Beginner",
    "type": "Course",
    "description": "A free resource to begin working with Large Language Models (LLMs) .",
    "categories": [
      "Introduction to AI and Fundamental Concepts",
      "Large Language Models (LLMs)"
    ]
  },
  {
    "name": "AI Python for Beginners",
    "link": "https://www.deeplearning.ai/short-courses/ai-python-for-beginners/",
    "difficulty": "Beginner",
    "type": "Course",
    "description": "A free resource focused on teaching Python for beginners in the context of AI .",
    "categories": [
      "Introduction to AI and Fundamental Concepts"
    ]
  },
  {
    "name": "OpenAI AI Academy - OpenAI, LLMs & ChatGPT",
    "link": "https://academy.openai.com/public/collections",
    "difficulty": "Beginner",
    "type": "Course",
    "description": "Part of OpenAI Academy's free course offerings, introducing users to OpenAI, LLMs, and ChatGPT .",
    "categories": [
      "Introduction to AI and Fundamental Concepts",
      "Large Language Models (LLMs)"
    ]
  },
  {
    "name": "OpenAI AI Academy - Introduction to GPTs",
    "link": "https://academy.openai.com/public/collections/chatgpt-at-work-2025-02-14",
    "difficulty": "Beginner",
    "type": "Course",
    "description": "Another course from the OpenAI Academy, focused on introducing GPT models .",
    "categories": [
      "Introduction to AI and Fundamental Concepts",
      "Generative AI"
    ]
  },
  {
    "name": "Big Data, Artificial Intelligence, and Ethics (University of California - Davis)",
    "link": "https://www.coursera.org/learn/big-data-ai-ethics",
    "difficulty": "Intermediate",
    "type": "Course",
    "description": "A 4-module course covering Big Data opportunities, introducing IBM Watson, and addressing the limitations of AI .",
    "categories": [
      "Introduction to AI and Fundamental Concepts",
      "Big Data",
      "AI Ethics"
    ]
  },
  {
    "name": "Deep Reinforcement Learning Course (Hugging Face)",
    "link": "https://huggingface.co/learn/deep-rl-course/unit0/introduction",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "A free and open-source course that teaches Deep Reinforcement Learning from beginner to expert . It includes theoretical and practical content with popular libraries like Stable Baselines3, as well as agent training in various environments . Please note that this course is in a low-maintenance state, and some features like 'AI vs AI' challenges and the 'Leaderboard' are not operational . It recommends dedicating 3-4 hours per week per chapter .",
    "categories": [
      "Introduction to AI and Fundamental Concepts",
      "Deep Learning",
      "AI Agents"
    ]
  },
  {
    "name": "Prompt Engineering for ChatGPT (Vanderbilt University / Coursera)",
    "link": "https://www.coursera.org/learn/prompt-engineering",
    "difficulty": "Beginner",
    "type": "Course",
    "description": "A 6-module course designed for beginners looking to improve their skills for writing effective prompts .",
    "categories": [
      "Prompt Engineering"
    ]
  },
  {
    "name": "AI Applications and Prompt Engineering (edX)",
    "link": "https://www.edx.org/learn/computer-programming/edx-ai-applications-and-prompt-engineering",
    "difficulty": "Beginner",
    "type": "Course",
    "description": "An introductory course that not only covers the basics of prompt engineering but also teaches how to apply them to create AI applications .",
    "categories": [
      "Prompt Engineering"
    ]
  },
  {
    "name": "OpenAI AI Academy - Introduction to Prompt Engineering",
    "link": "https://academy.openai.com/public/videos/introduction-to-prompt-engineering-2025-02-13",
    "difficulty": "Beginner",
    "type": "Course",
    "description": "A fundamental course from the OpenAI Academy to learn the basics of prompt engineering .",
    "categories": [
      "Prompt Engineering"
    ]
  },
  {
    "name": "Prompt Engineering Guide (GitHub Repo)",
    "link": "https://github.com/dair-ai/Prompt-Engineering-Guide",
    "difficulty": "Beginner",
    "type": "Course",
    "description": "A comprehensive repository containing guides, articles, classes, notebooks, and other resources for learning and mastering prompt engineering .",
    "categories": [
      "Prompt Engineering"
    ]
  },
  {
    "name": "Anthropic's Interactive Prompt Engineering Tutorial - Beginner Chapters",
    "link": "https://github.com/anthropics/prompt-eng-interactive-tutorial",
    "difficulty": "Beginner",
    "type": "Course",
    "description": "This interactive tutorial from Anthropic includes initial chapters on basic prompt structure, being clear and direct, and assigning roles to the model .",
    "categories": [
      "Prompt Engineering"
    ]
  },
  {
    "name": "Anthropic's Interactive Prompt Engineering Tutorial - Intermediate Chapters",
    "link": "https://github.com/anthropics/prompt-eng-interactive-tutorial",
    "difficulty": "Intermediate",
    "type": "Course",
    "description": "Includes chapters on how to separate data from instructions, format model output and 'speak for Claude', the use of precognition ('thinking step by step'), and the utilization of examples .",
    "categories": [
      "Prompt Engineering"
    ]
  },
  {
    "name": "ChatGPT Prompt Engineering for Devs (OpenAI / DeepLearning AI)",
    "link": "https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/",
    "difficulty": "Intermediate",
    "type": "Course",
    "description": "A collaborative course between OpenAI and DeepLearning AI, focusing on best practices and hands-on experience in prompt engineering for developers .",
    "categories": [
      "Prompt Engineering"
    ]
  },
  {
    "name": "Anthropic's Interactive Prompt Engineering Tutorial - Advanced Chapters",
    "link": "https://github.com/anthropics/prompt-eng-interactive-tutorial",
    "difficulty": "Advanced",
    "type": "Course",
    "description": "This section addresses advanced techniques such as avoiding hallucinations and the construction of complex prompts for industry use cases .",
    "categories": [
      "Prompt Engineering"
    ]
  },
  {
    "name": "OpenAI AI Academy - Advanced Prompt Engineering",
    "link": "https://academy.openai.com/public/videos/advanced-prompt-engineering-2025-02-13",
    "difficulty": "Advanced",
    "type": "Course",
    "description": "A course within the OpenAI Academy that focuses on advanced prompt engineering techniques .",
    "categories": [
      "Prompt Engineering"
    ]
  },
  {
    "name": "AI Agents for Beginners (GitHub Repo)",
    "link": "https://github.com/microsoft/ai-agents-for-beginners",
    "difficulty": "Beginner",
    "type": "Course",
    "description": "A free 11-lesson course specifically designed for beginners who want to start building AI agents .",
    "categories": [
      "AI Agents"
    ]
  },
  {
    "name": "AI Agentic Design Patterns with AutoGen",
    "link": "https://www.deeplearning.ai/short-courses/ai-agentic-design-patterns-with-autogen/",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "A course available through DeepLearning AI that explores design patterns for AI agents using the AutoGen library .",
    "categories": [
      "AI Agents"
    ]
  },
  {
    "name": "Multi AI Agent Systems with crewAI",
    "link": "https://www.deeplearning.ai/short-courses/multi-ai-agent-systems-with-crewai/",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "A course of DeepLearning AI that focuses on the creation of systems with multiple AI agents using crewAI .",
    "categories": [
      "AI Agents"
    ]
  },
  {
    "name": "AI Agents in LangGraph",
    "link": "https://www.deeplearning.ai/short-courses/ai-agents-in-langgraph/",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "A course of DeepLearning AI that explores the use of LangGraph to build AI agents.",
    "categories": [
      "AI Agents"
    ]
  },
  {
    "name": "Fundamentals of AI agents using RAG and LangChain",
    "link": "https://www.coursera.org/learn/fundamentals-of-ai-agents-using-rag-and-langchain",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "A resource for learning the fundamentals of AI agents integrating RAG and LangChain .",
    "categories": [
      "AI Agents",
      "RAG (Retrieval-Augmented Generation)"
    ]
  },
  {
    "name": "Building Agentic RAG with LlamaIndex",
    "link": "https://www.deeplearning.ai/short-courses/building-agentic-rag-with-llamaindex/",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "A course of DeepLearning AI focused on the construction of RAG systems with agentic capabilities using LlamaIndex .",
    "categories": [
      "AI Agents",
      "RAG (Retrieval-Augmented Generation)"
    ]
  },
  {
    "name": "GenAI Agents (GitHub Repo)",
    "link": "https://github.com/NirDiamant/GenAI_Agents",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "A GitHub repository that provides tutorials and implementations for various Generative AI Agent techniques, covering from basic to advanced concepts .",
    "categories": [
      "AI Agents",
      "Generative AI"
    ]
  },
  {
    "name": "n8n AI Agents Tutorial (8-hour video)",
    "link": "https://youtu.be/Ey18PDiaAYI",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "A detailed video tutorial that teaches how to build AI agents using the n8n platform. It covers practical topics such as the creation of a RAG Chatbot, automation of Customer Support, and automated content generation for LinkedIn .",
    "categories": [
      "AI Agents",
      "RAG (Retrieval-Augmented Generation)",
      "Data Analysis and Specific Tool Usage",
      "Other AI Topics and Applications"
    ]
  },
  {
    "name": "Serverless Agentic Workflows with Amazon Bedrock (DeepLearning AI)",
    "link": "https://www.deeplearning.ai/short-courses/serverless-agentic-workflows-with-amazon-bedrock/",
    "difficulty": "Advanced",
    "type": "Course",
    "description": "A DeepLearning AI course on implementing serverless agentic workflows using Amazon Bedrock .",
    "categories": [
      "AI Agents",
      "LLMOps"
    ]
  },
  {
    "name": "Agentic Pattern Course by Miguel Otero",
    "link": "https://www.youtube.com/playlist?list=PLacQJwuclt_sK_pUPzBpfeWyiL1QOSMRQ",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "A youtube course based on practice and real-world projects for learning agentic patterns .",
    "categories": [
      "AI Agents"
    ]
  },
  {
    "name": "LLMOps (Google Cloud / DeepLearning AI)",
    "link": "https://www.deeplearning.ai/short-courses/llmops/",
    "difficulty": "Advanced",
    "type": "Course",
    "description": "A DeepLearning AI course in collaboration with Google Cloud that delves into the LLMOps pipeline, from pre-processing training data to adapting a supervised tuning pipeline to train and deploy a custom LLM .",
    "categories": [
      "LLMOps",
      "Large Language Models (LLMs)"
    ]
  },
  {
    "name": "OpenAI AI Academy - ChatGPT for Data Analysis",
    "link": "https://academy.openai.com/public/videos/chatgpt-for-data-analysis-2025-02-13",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "A course from the OpenAI Academy focused on using ChatGPT for data analysis tasks .",
    "categories": [
      "Data Analysis and Specific Tool Usage",
      "Large Language Models (LLMs)"
    ]
  },
  {
    "name": "ChatGPT Advanced Data Analysis (Coursera)",
    "link": "https://www.coursera.org/learn/chatgpt-advanced-data-analysis",
    "difficulty": "Beginner",
    "type": "Course",
    "description": "Mentioned in relation to the Code Interpreter .",
    "categories": [
      "Data Analysis and Specific Tool Usage",
      "Large Language Models (LLMs)"
    ]
  },
  {
    "name": "Hands on Large Language Models (GitHub Repo)",
    "link": "https://github.com/HandsOnLLM/Hands-On-Large-Language-Models",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "This repository contains code examples that complement the book 'Hands-On Large Language Models,' covering from the introduction to language models to fine-tuning techniques .",
    "categories": [
      "Other AI Topics and Applications",
      "Large Language Models (LLMs)"
    ]
  },
  {
    "name": "Prompt Engineering for VLMS",
    "link": "https://www.deeplearning.ai/short-courses/prompt-engineering-for-vision-models/",
    "difficulty": "Beginner",
    "type": "Course",
    "description": "A free resource for prompt engineering with Visual Language Models (VLMs) .",
    "categories": [
      "Prompt Engineering"
    ]
  },
  {
    "name": "Generative AI for Everyone",
    "link": "https://www.deeplearning.ai/courses/generative-ai-for-everyone/",
    "difficulty": "Beginner",
    "type": "Course",
    "description": "A free resource to get started with Generative AI.",
    "categories": [
      "Introduction to AI and Fundamental Concepts",
      "Generative AI"
    ]
  },
  {
    "name": "Generative AI Specialization for Data Analysts",
    "link": "https://www.coursera.org/specializations/generative-ai-for-data-analysts",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "A specialization focused on Generative AI for data analysts.",
    "categories": [
      "Generative AI",
      "Data Analysis"
    ]
  },
  {
    "name": "Microsoft AI Product Manager Professional Certificate",
    "link": "https://www.coursera.org/professional-certificates/microsoft-ai-product-manager",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "A professional certificate program for AI product managers.",
    "categories": [
      "AI Applications"
    ]
  },
  {
    "name": "Generative AI with Large Language Models",
    "link": "https://www.coursera.org/learn/generative-ai-with-llms",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "A course focusing on generative AI using Large Language Models.",
    "categories": [
      "Generative AI",
      "Large Language Models (LLMs)"
    ]
  },
  {
    "name": "Generative AI: Fundamentals of Signal Engineering",
    "link": "https://www.coursera.org/learn/generative-ai-prompt-engineering-for-everyone",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "A course covering the fundamentals of signal engineering in the context of Generative AI.",
    "categories": [
      "Generative AI",
      "Introduction to AI and Fundamental Concepts"
    ]
  },
  {
    "name": "IBM AI Developer Professional Certificate",
    "link": "https://www.coursera.org/professional-certificates/applied-artifical-intelligence-ibm-watson-ai",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "A professional certificate program for AI developers.",
    "categories": [
      "AI Applications"
    ]
  },
  {
    "name": "Machine Learning Specialization",
    "link": "https://www.coursera.org/specializations/machine-learning-introduction",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "A specialization focused on Machine Learning.",
    "categories": [
      "Machine Learning"
    ]
  },
  {
    "name": "Generative AI Specialization for Data Engineers",
    "link": "https://www.coursera.org/specializations/generative-ai-for-data-engineers",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "A specialization focused on Generative AI for data engineers.",
    "categories": [
      "Generative AI",
      "Data Analysis"
    ]
  },
  {
    "name": "Generative AI Specialization for Data Scientists",
    "link": "https://www.coursera.org/specializations/generative-ai-for-data-scientists",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "A specialization focused on Generative AI for data scientists.",
    "categories": [
      "Generative AI",
      "Data Analysis"
    ]
  },
  {
    "name": "IBM Generative AI Specialization for Cybersecurity Professionals",
    "link": "https://www.coursera.org/specializations/generative-ai-for-cybersecurity-professionals",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "An IBM specialization focused on Generative AI for cybersecurity professionals.",
    "categories": [
      "Generative AI",
      "AI Applications"
    ]
  },
  {
    "name": "Generative AI Specialization for Software Developers",
    "link": "https://www.coursera.org/specializations/generative-ai-for-software-developers",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "A specialization focused on Generative AI for software developers.",
    "categories": [
      "Generative AI",
      "AI Applications"
    ]
  },
  {
    "name": "Google AI Fundamentals Specialization",
    "link": "https://www.cloudskillsboost.google/paths/118",
    "difficulty": "Beginner",
    "type": "Course",
    "description": "A series of 5 courses offered by Google to learn about generative AI from the ground up, starting with an introduction to AI and progressing to a solid understanding of the field .",
    "categories": [
      "Introduction to AI and Fundamental Concepts",
      "Generative AI"
    ]
  },
  {
    "name": "Automation with Generative AI Specialization",
    "link": "https://www.coursera.org/specializations/generative-ai-automation",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "A specialization focused on automation using Generative AI.",
    "categories": [
      "Workflow Automation",
      "Generative AI"
    ]
  },
  {
    "name": "Introduction to Artificial Intelligence (AI)",
    "link": "https://www.coursera.org/learn/introduction-to-ai",
    "difficulty": "Beginner",
    "type": "Course",
    "description": "An introductory course to Artificial Intelligence. The specific destination link for this general course title is not explicitly provided in the source material .",
    "categories": [
      "Introduction to AI and Fundamental Concepts"
    ]
  },
  {
    "name": "MCP: Build Rich-Context AI Apps with Anthropic",
    "link": "https://learn.deeplearning.ai/courses/mcp-build-rich-context-ai-apps-with-anthropic",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "This course, built in partnership with Anthropic, teaches the core concepts of the Model Context Protocol (MCP) and how to implement it in your AI applications. MCP is an open protocol that standardizes how Large Language Model (LLM) applications can get access to context in terms of tools and data resources, based on a client-server architecture. It defines how communication takes place between an MCP client, hosted inside your own LLM application, and an MCP server that exposes tools, data resources, and prompt templates to your application. MCP originated as part of an internal Anthropic project to extend the capabilities of Claude Desktop, allowing it to interact with local file systems and other external systems. The protocol was found useful in many AI applications and was subsequently published as open source to make it available to more developers.\n\nThe MCP ecosystem is growing rapidly and includes a number of MCP services developed by the open-source community and Anthropic's MCP team . It is model agnostic and designed to be easy to plug into multiple applications. For instance, a research assistant agent could connect to GitHub, Google Drive, and File System services via MCP to access and summarize data, without you having to write custom LLM tools.\n\nThe course curriculum includes a deep dive into the MCP client-server architecture, making a chatbot application MCP compatible, building and testing an MCP server, and connecting the chatbot to it. You will also learn to connect your chatbot to other trusted third-party servers to extend its capabilities, re-use your MCP server with other MCP applications like Claude Desktop, and deploy your MCP server remotely. The instructor for this course is Elie Schoppik, Head of Technical Education at Anthropic. This technology significantly eases the process for LLM application developers to connect their systems to various tools and data resources, and for tool/data providers to make their offerings accessible to many developers.",
    "categories": [
      "MCP (Model Context Protocol)",
      "Large Language Models (LLMs)",
      "AI Agents",
      "AI Deployment"
    ]
  },
  {
    "name": "#2 Programming 'Hello World' with MCP: Python implementation and integration with Claude AI",
    "link": "https://www.youtube.com/watch?v=9-v2WOby2-Q&list=PLnNbmcjjevxsnAfDfQmDmhBYBWAt0rbMs&index=2",
    "difficulty": "Intermediate",
    "type": "Course",
    "description": "This youtube video provides a step-by-step guide on how to create a Model Context Protocol (MCP) client and server from scratch using Python. It covers the essential setup, including initializing a Python environment, creating a virtual environment, and installing necessary libraries such as `FasMCP`, `MCP_Types`, `asyncio`, and `Pathlib`. The tutorial demonstrates how to build an MCP server, defining tools (e.g., a 'Hello World' tool) and their descriptions, which are crucial for clients and Large Language Models (LLMs) to understand when to call them. It then walks through the process of creating an MCP client that connects to the server, makes requests to its tools, and processes the text-based responses. The video also includes a section on error handling for robust application development. A key highlight is the demonstration of how to integrate the custom-built MCP server with Claude Desktop by configuring the `claude_desktop_config` file, allowing Claude AI to interact with your local MCP server and its defined tools. This practical 'Hello World' example serves as a fundamental base for developing more complex AI applications that leverage the MCP client-server architecture. The MCP itself is an open protocol that standardizes how LLM applications can access context, tools, and data resources through a client-server model.",
    "categories": [
      "MCP (Model Context Protocol)",
      "AI Agents",
      "Large Language Models (LLMs)",
      "AI Deployment",
      "Systems Integration",
      "Workflow Automation"
    ]
  },
  {
    "name": "David Kriesel’s A Brief Introduction to Neural Networks",
    "link": "https://www.linkedin.com/posts/michael-erlihson-phd-8208616_a-brief-introduction-to-neural-networks-ugcPost-7342131671187501056-B_xA/",
    "difficulty": "Unspecified",
    "type": "Book",
    "description": "A mathematically grounded text that delves into the foundations of neural networks . It explains how neural networks actually work, layer by layer, and equation by equation, without assuming hype or shortcuts . The book covers a range of topics, from biology to perceptrons, backpropagation, RBFs, SOMs, Hopfield nets, and ART, providing clear derivations and precise illustrations . It is notable for balancing rigor and clarity, making it comprehensible for both math-inclined readers who can follow the formalism directly, and non-math-inclined readers who are carried forward by its prose . This resource also includes a complete Java neural network framework (SNIPE) for practical learning through coding . It addresses classic and less common architectures carefully, including delta rules, gradient procedures, growing RBF networks, and reinforcement learning with real examples . This foundational work is highly recommended for building a serious intuition about neural networks, rather than merely tuning them, and is noted to not age despite not being a new book .",
    "categories": [
      "Introduction to AI and Fundamental Concepts",
      "Deep Learning",
      "Machine Learning",
      "AI Agents"
    ]
  },
  {
    "name": "AI Frontier",
    "link": "https://www.linkedin.com/newsletters/ai-frontier-6861893793654935552/",
    "difficulty": "Unspecified",
    "type": "Newsletter",
    "description": "This is a witty, informative newsletter featuring curated AI news, the latest innovations and learning resources. It is published weekly and created by Steve Nouri, who is known for building the largest AI community, being an advisor to Fortune 500 companies, having 2 million followers, and being a keynote speaker.",
    "categories": [
      "Introduction to AI and Fundamental Concepts",
      "AI Applications"
    ]
  },
  {
    "name": "IBM AI Newsletter",
    "link": "https://www.linkedin.com/newsletters/ai-newsletter-7321517150936920066/",
    "difficulty": "Unspecified",
    "type": "Newsletter",
    "description": "This newsletter provides the latest in AI from IBM and the open source world, curated and delivered every two weeks. It is created by Jacobo Garnacho Pérez, who is a Principal Data Platform Sales Manager at IBM and a University Professor, with a passion for AI and Quantum. Featured editions include IBM AI June Highlights and Latest AI News, as well as coverage of IBM THINK 2025.",
    "categories": [
      "Introduction to AI and Fundamental Concepts",
      "AI Applications"
    ]
  },
  {
    "name": "Brain News",
    "link": "https://www.linkedin.com/newsletters/brain-news-6877555549521616896/",
    "difficulty": "Unspecified",
    "type": "Newsletter",
    "description": "This newsletter covers Marketing, Design, Business, and a wide variety of other topics ('Newsletter de todo lo que leo sobre Marketing, Diseño, Negocios y un largo etcétera') . It is published weekly  and is created by Andrés Karp, who is identified as a LinkedIn Top Voice specializing in Strategic Marketing, Artificial Intelligence, and Data . Past editions of the newsletter include a series titled ʙʀᴀɪɴs ɴᴇᴡs ʙᴏᴏᴋ: ʙᴜɪʟᴛ ɪɴ ᴘᴜʙʟɪᴄ .",
    "categories": [
      "Introduction to AI and Fundamental Concepts",
      "AI Applications",
      "Data Analysis"
    ]
  },
  {
    "name": "ACI.dev",
    "link": "https://github.com/aipotheosis-labs/aci",
    "difficulty": "Unspecified",
    "type": "Tool",
    "description": "ACI.dev is an open-source platform designed to connect AI agents to any tool for VibeOps and Agent Workflows . It provides capabilities for building AI agents through function calling or its unified Model Context Protocol (MCP) server . The platform is model agnostic  and aims to solve complex AI agent engineering challenges by offering multi-tenant authentication management and granular permissions .\n\nKey features of ACI.dev include:   Unified MCP Server: Allows connecting an MCP client to over 600+ integrations through a single connection, enabling AI agents to search, plan, and execute tasks based on intent .   Workflow Discovery: Grants AI agents the flexibility to dynamically discover the best tools and workflows for their current task .   Secure Authentication / Managed Agent Authentication: Facilitates end-users authorizing their AI agents with account access via OAuth, with ACI.dev handling token management and OAuth client setups .   Secure Agent Secrets Manager: Provides a mechanism for end-users to securely store and manage credentials for web browsing AI agents, with customizable access policies .   Natural Language Permissions: Enables setting granular permissions using natural language, which helps prevent erroneous API execution and significantly improves agent reliability .\n\nPre-built tool integrations include popular services like Gmail, HubSpot, Notion, and Slack . ACI.dev aims to provide everything necessary to create production-ready AI agents that can securely interact with various tools and services .",
    "categories": [
      "AI Agents",
      "Agent Development",
      "MCP (Model Context Protocol)",
      "Systems Integration",
      "Workflow Automation",
      "AI Deployment"
    ]
  },
  {
    "name": "n8n Workflows Collection (DragonJAR)",
    "link": "https://github.com/DragonJAR/n8n-workflows-es",
    "difficulty": "Unspecified",
    "type": "Tool",
    "description": "This GitHub repository, a fork of @Zie619's initiative, is a consolidated collection of n8n workflows ('Colección de Flujos de Trabajo de n8n'). It gathers workflows from various sources, including the official n8n.io site and its community forum, public GitHub examples, blogs, and other websites. Its primary purpose is to serve as a resource for inspiration, learning, and reuse of workflows in your own n8n projects .\n\nKey features and improvements implemented in this collection include:   Descriptions in Spanish: Each `.json` file has a clear description in Spanish, detailing the actions it performs .   Duplicate Elimination: A unique hash validation process ensures that each workflow is unique in content .   File Renaming: File names have been updated for more accurate functionality descriptions, aiding in search and selection .\n\nTo use a workflow, you typically import the desired `.json` file into your n8n instance, adjust any necessary credentials or webhook URLs, then save and execute . The repository also offers interactive support through a Custom GPT and an MCP (Model Context Protocol) server . The MCP server allows interaction with this knowledge base of documented n8n flows, with instructions for use in various AI-powered tools like Cursor, Claude Desktop, and VSCode . Contributions are welcome via pull requests .\n\nThe collection features a wide variety of workflows, including those for AI reports, marketing automation, integrations with services like Gumroad, MailerLite, LINE, ChatGPT, Google Drive, Connectwise, Microsoft Teams, Google Calendar, Outlook, Airtable, Notion, LinkedIn, Stripe, HubSpot, Slack, and many more [6-653]. It also includes complex AI-driven automations such as multi-agent AI conversations, AI-powered content creation for social media, email analysis and summarization with AI, AI agents for various tasks (e.g., calendar management, sales, HR), document processing (PDF to text/markdown), image generation and analysis, sentiment analysis, and web scraping.",
    "categories": [
      "n8n",
      "Workflow Automation",
      "Systems Integration",
      "AI Agents",
      "RAG (Retrieval-Augmented Generation)",
      "Large Language Models (LLMs)",
      "AI Applications",
      "Data Analysis",
      "Document Processing",
      "Image Analysis with AI",
      "Web Scraping with AI",
      "MCP (Model Context Protocol)"
    ]
  },
  {
    "name": "The Unwind AI",
    "link": "https://www.theunwindai.com/",
    "difficulty": "Unspecified",
    "type": "Newsletter",
    "description": "The Unwind AI is an online platform that serves as a go-to source for the latest AI news, cutting-edge tools, and in-depth tutorials specifically for AI Developers . It provides real-time updates on the rapidly evolving AI landscape . While not a traditional linear course, its continuous stream of tutorials and news updates provides a learning resource for developers.\n\nThe platform features a popular \"Daily Unwind\" section that offers concise summaries of impactful developments across various AI domains :   AI Agents & Workflows: Includes discussions on topics such as Microsoft’s Deep Research Agent for Large Codebases, general-purpose AI agents for long-running tasks, Software Development Agents in Your Terminal, Agentic Browsers that see all your tabs, and Multi-Agent AI Systems, including those built with 1000+ tools [1-3]. It also covers AI Super Agents in your browser and always-on AI agents that monitor the web .   Model Context Protocol (MCP): Highlights practical applications and potential, such as connecting React applications to an MCP server in just 3 lines of code, the potential for 500K+ AI apps to act as MCP servers, using an MCP server to help stop AI hallucination, and how to connect AI agents to 10,000+ tools via MCP [1-3]. The platform also notes the availability of an AI agent course as an MCP server and the Hugging Face MCP Server for Models, Datasets, and Papers .   Large Language Models (LLMs): Covers updates on new models like Gemini 2.5 Flash and Mistral's reasoning model, discussions on Apple's perspective on reasoning LLMs, and practical aspects like GitHub making Copilot project-specific .   Emerging AI Tools & Resources: Provides updates on developer-centric tools such as Claude Code now available in VS Code, Apple opensourcing a Docker alternative for Mac, Vibe code browser automation scripts, and free open-source alternatives to various services .   Memory-Augmented Generation & RAG: Explores concepts like an Operating System for Memory-augmented generation and the availability of an opensource plug-and-play RAG stack .\n\nIn addition to daily news, The Unwind AI offers an \"AI Tutorial\" section that provides step-by-step instructions to empower users to build real-world AI applications . An example tutorial highlighted is on building a fully functional, 100% open-source Agentic RAG App with Reasoning . The platform aims to be an essential resource for developers looking to stay ahead and master AI development .",
    "categories": [
      "AI Agents",
      "MCP (Model Context Protocol)",
      "Large Language Models (LLMs)",
      "RAG (Retrieval-Augmented Generation)",
      "AI Applications",
      "Workflow Automation",
      "Systems Integration",
      "Prompt Engineering",
      "Introduction to AI and Fundamental Concepts"
    ]
  },
  {
    "name": "Firecrawl",
    "link": "https://www.firecrawl.dev/",
    "difficulty": "Unspecified",
    "type": "Tool",
    "description": "Firecrawl is an open-source API and platform designed to transform websites into LLM-ready data . It offers industry-leading web scraping and crawling capabilities to enhance AI applications .\n\nKey features and functionalities of Firecrawl include:   Web Scraping and Search: It allows users to search the web and scrape the results with a single API call . It can extract content in markdown and JSON formats, and even capture screenshots .   Advanced Crawling: Firecrawl can gather clean data from all accessible subpages, even without a sitemap . It handles complex web elements such as rotating proxies, orchestration, rate limits, and JavaScript-blocked content, ensuring zero configuration for the user .   Dynamic Content & Reliability: The tool is built for reliability and scales with user needs, handling JavaScript, Single-Page Applications (SPAs), and dynamic content loading with minimal setup . It also features a 'Smart Wait' function to intelligently wait for content to load, making scraping faster and more dependable .   Media Parsing & Actions: Firecrawl can parse and output content from web-hosted PDFs, DOCX, and HTML files . It supports various actions to interact with web pages before extraction, including click, scroll, write, wait, and press .   Integration: It is fully integrated with existing tools and workflows . Developers can easily start using it by installing the `@mendable/firecrawl-js` package .\n\nFirecrawl is used for various AI-powered solutions , including:   AI Chats: Powering AI assistants with real-time, accurate web content .   Lead Enrichment: Enhancing sales data with web information .   MCPs (Model Context Protocols): Adding powerful scraping capabilities to code editors .   AI Platforms: Enabling customers to build AI apps with web data .   Deep Research: Extracting comprehensive information for in-depth research .\n\nUsers have reported significant benefits, such as simplifying data preparation, being 50 times faster than alternatives like Apify for web scraping, and providing major savings in time and money by reducing token consumption (e.g., saving 2/3 tokens and allowing GPT-3.5 Turbo usage over GPT-4) [7-9]. Firecrawl offers a free plan (500 credits) and various paid plans with increasing credits and concurrent browser support, with transparent pricing and add-ons like auto-recharge and credit packs.",
    "categories": [
      "Web Scraping with AI",
      "Large Language Models (LLMs)",
      "AI Applications",
      "AI Assistants",
      "MCP (Model Context Protocol)",
      "Workflow Automation",
      "Systems Integration",
      "Data Analysis"
    ]
  },
  {
    "name": "Awesome LLM Apps",
    "link": "https://github.com/Shubhamsaboo/awesome-llm-apps",
    "difficulty": "Unspecified",
    "type": "Tool",
    "description": "The Awesome LLM Apps repository is a curated collection of impressive applications built using Large Language Models (LLMs) . These applications leverage various advanced AI concepts including Retrieval Augmented Generation (RAG), AI Agents (both single and multi-agent teams), Model Context Protocol (MCP), and Voice Agents . The repository features apps developed with leading models from OpenAI, Anthropic, Google, and a range of open-source models like DeepSeek, Qwen, or Llama that can be run locally .\n\nThis collection serves as a valuable resource to:   Discover practical and creative applications of LLMs across diverse domains, from code repositories to email inboxes .   Explore apps that combine LLMs with AI Agents, Agent Teams, MCP, and RAG .   Learn from well-documented projects and contribute to the expanding open-source ecosystem of LLM-powered applications .\n\nThe repository organizes its featured AI projects into several key categories, showcasing a wide array of functionalities:   AI Agents: Includes a variety of 'Starter AI Agents' (e.g., AI Blog to Podcast Agent, AI Data Analysis Agent, AI Travel Agent) and 'Advanced AI Agents' (e.g., AI Deep Research Agent, AI System Architect Agent, AI Financial Coach Agent), as well as 'Autonomous Game Playing Agents' (e.g., AI Chess Agent) . The collection also features a 'Web Scrapping AI Agent (Local & Cloud)' .   Multi-agent Teams: Highlights systems where multiple AI agents collaborate, such as 'AI Competitor Intelligence Agent Team', 'AI Finance Agent Team', and 'Multimodal Coding Agent Team' .   Voice AI Agents: Focuses on agents designed for voice interaction, including 'AI Audio Tour Agent' and 'Customer Support Voice Agent' .   MCP AI Agents: Demonstrates agents integrated with the Model Context Protocol, such as 'Browser MCP Agent', 'GitHub MCP Agent', and 'Notion MCP Agent' .   RAG (Retrieval Augmented Generation): Features various implementations of RAG, including 'Agentic RAG', 'Corrective RAG (CRAG)', 'Hybrid Search RAG', and 'Local RAG Agent' .   LLM Apps with Memory Tutorials: Provides examples of LLM applications capable of maintaining memory, like 'AI ArXiv Agent with Memory' and 'Local ChatGPT Clone with Memory' .   Chat with X Tutorials: Offers guides on building LLM applications that can chat with various data sources, such as 'Chat with GitHub', 'Chat with Gmail', 'Chat with PDF', and 'Chat with YouTube Videos' .   LLM Fine-tuning Tutorials: Includes resources like 'Llama 3.2 Fine-tuning' .\n\nTo get started, users can clone the repository, navigate to the desired project directory, install dependencies using `pip install -r requirements.txt`, and follow the project-specific instructions provided in each `README.md` file . The repository encourages contributions, inviting users to create issues or submit pull requests for new apps or improvements . The project is primarily developed in Python (59.7%), with significant contributions in JavaScript (31.8%) and TypeScript (7.9%) . It has garnered substantial community support, with 45.7k stars and 5.2k forks .",
    "categories": [
      "AI Agents",
      "RAG (Retrieval-Augmented Generation)",
      "Large Language Models (LLMs)",
      "MCP (Model Context Protocol)",
      "AI Applications",
      "Workflow Automation",
      "Systems Integration",
      "Prompt Engineering",
      "Web Scraping with AI",
      "Data Analysis"
    ]
  },
  {
    "name": "Google ADK (Agent Development Kit) Free Course by Carlos Alarcón",
    "link": "https://www.youtube.com/playlist?list=PLgQnGGtCss_gvACLOw9F-amI_8CFjcVBN",
    "difficulty": "Beginner",
    "type": "Course",
    "description": "This free YouTube course by Carlos Alarcón teaches step-by-step how to build intelligent AI agents capable of executing tasks, using tools, and accessing external information using Google's Agent Development Kit (ADK) . The first three available classes cover an introduction to Google ADK and AI agents, advanced handling of Large Language Models (LLMs), and the creation of tools and custom functions . Future classes are planned to cover Agent-to-Agent (A2A) communication, Model Context Protocol (MCP), and deployments . Each class includes practical examples and provides a GitHub repository with code for hands-on learning . This course is ideal for developers, AI enthusiasts, and teams looking to incorporate AI agents into their workflows .",
    "categories": [
      "AI Agents",
      "Agent Development",
      "Large Language Models (LLMs)",
      "MCP (Model Context Protocol)",
      "AI Deployment",
      "Workflow Automation",
      "Systems Integration",
      "Introduction to AI and Fundamental Concepts",
      "Machine Learning",
      "Multi-Agent Systems"
    ]
  },
  {
    "name": "Anthropic Prompt Engineering Overview Guide",
    "link": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview",
    "difficulty": "Beginner",
    "type": "Course",
    "description": "This overview guide from Anthropic provides comprehensive insights into prompt engineering for Claude models. It outlines fundamental assumptions before starting prompt engineering, such as having clear success criteria and empirical testing methods. The guide emphasizes that prompt engineering is significantly faster, more resource-efficient, and cost-effective than fine-tuning for controlling model behavior. Key advantages include maintaining model updates, time-saving, minimal data needs, flexibility for rapid iteration, better domain adaptation, improved comprehension of external content like retrieved documents, preservation of general knowledge, and increased transparency. The guide organizes prompt engineering techniques from most broadly effective to more specialized, suggesting a specific order for troubleshooting performance, which includes being clear and direct, using examples (multishot prompting), letting the model think (chain of thought), utilizing XML tags, assigning roles (system prompts), prefilling responses, chaining complex prompts, and tips for long contexts. It also touches upon related topics such as tools (e.g., Bash, Code execution, Web search) and their implementation, the Model Context Protocol (MCP), various use cases like customer support and legal summarization, strategies for testing and evaluation, and methods to strengthen guardrails against issues like hallucinations and prompt leaks. The guide also references interactive tutorials available on GitHub and Google Sheets for a hands-on learning experience.",
    "categories": [
      "Prompt Engineering",
      "Large Language Models (LLMs)",
      "AI Agents",
      "MCP (Model Context Protocol)",
      "AI Deployment",
      "Workflow Automation",
      "Systems Integration",
      "Introduction to AI and Fundamental Concepts",
      "AI Applications"
    ]
  },
  {
    "name": "Microsoft ML for Beginners Curriculum",
    "link": "https://github.com/microsoft/ML-For-Beginners",
    "difficulty": "Beginner",
    "type": "Course",
    "description": "This is a 12-week, 26-lesson curriculum from Microsoft Cloud Advocates, designed for beginners to learn classic Machine Learning. The curriculum includes 52 quizzes and emphasizes a hands-on, project-based pedagogy with a common theme of exploring world cultures through data. It primarily uses Scikit-learn and focuses on traditional ML techniques, intentionally avoiding deep learning (which is covered in their separate 'AI for Beginners' curriculum). Each lesson provides pre- and post-quizzes, written instructions, solutions, assignments, and knowledge checks. Projects progressively increase in complexity, covering topics such as introduction to ML, fairness, regression (e.g., North American pumpkin prices), building web applications, classification (e.g., Asian/Indian cuisines, recommender apps), clustering (e.g., Nigerian Musical Tastes), Natural Language Processing (NLP) including sentiment analysis and translation, Time Series forecasting (ARIMA, SVR), and Reinforcement Learning (Q-Learning, Gym). The course is primarily in Python, with many lessons also available in R (using R Markdown files with '.rmd' extension). Students are encouraged to fork and clone the repository to complete exercises. Supplemental resources, including video walkthroughs on the Microsoft Developer YouTube channel and a PDF version of the curriculum, are available. The repository also has offline access options and encourages community contributions, boasting significant community support with 73.2k stars and 16k forks.",
    "categories": [
      "Machine Learning",
      "Introduction to AI and Fundamental Concepts",
      "AI Applications",
      "Data Analysis",
      "AI Agents"
    ]
  },
  {
    "name": "Neural Networks: Zero to Hero by Andrej Karpathy",
    "link": "https://github.com/karpathy/nn-zero-to-hero",
    "difficulty": "Intermediate",
    "type": "Course",
    "description": "This repository by Andrej Karpathy focuses on building modern deep learning systems from scratch, including the construction of GPTs (Generative Pre-trained Transformers). It is designed for those who have grasped the foundations of AI/ML and are ready to dive deeper into deep learning.",
    "categories": [
      "Deep Learning",
      "Large Language Models (LLMs)",
      "Machine Learning"
    ]
  },
  {
    "name": "DL Paper Implementations",
    "link": "https://github.com/labmlai/annotated_deep_learning_paper_implementations",
    "difficulty": "Intermediate",
    "type": "Course",
    "description": "This repository provides well-documented PyTorch implementations of over 60 research papers covering advanced deep learning architectures such as Transformers, Generative Adversarial Networks (GANs), and Diffusion models. It is suitable for those who have a fundamental understanding of AI, ML, and DL and wish to study how leading architectures work.",
    "categories": [
      "Deep Learning",
      "Large Language Models (LLMs)",
      "Generative AI"
    ]
  },
  {
    "name": "Made With ML",
    "link": "https://github.com/GokuMohandas/Made-With-ML",
    "difficulty": "Intermediate",
    "type": "Course",
    "description": "This resource teaches the entire process of designing, developing, deploying, and iterating on real-world Machine Learning systems. It focuses on practical aspects of MLOps, Continuous Integration/Continuous Delivery (CI/CD), and best practices for moving ML projects from notebooks to production.",
    "categories": [
      "LLMOps",
      "AI Deployment",
      "Machine Learning",
      "AI Applications"
    ]
  },
  {
    "name": "Hands-on LLMs",
    "link": "https://github.com/HandsOnLLM/Hands-On-Large-Language-Models",
    "difficulty": "Intermediate",
    "type": "Course",
    "description": "This visually rich repository provides comprehensive coverage of Large Language Models (LLMs), including topics such as tokenization, fine-tuning, and Retrieval-Augmented Generation (RAG). It is designed for learners who have explored GPTs and LLMs and are looking to apply their knowledge hands-on.",
    "categories": [
      "Large Language Models (LLMs)",
      "RAG (Retrieval-Augmented Generation)",
      "Prompt Engineering",
      "AI Agents"
    ]
  },
  {
    "name": "Advanced RAG Techniques",
    "link": "https://github.com/NirDiamant/RAG_Techniques",
    "difficulty": "Advanced",
    "type": "Course",
    "description": "This repository covers over 30 advanced methods designed to make Retrieval-Augmented Generation (RAG) systems faster, smarter, and more accurate. Techniques like HyDE and GraphRAG are included, making it suitable for those who already have a good grasp of RAG systems and wish to enhance their capabilities.",
    "categories": [
      "RAG (Retrieval-Augmented Generation)",
      "AI Agents",
      "Large Language Models (LLMs)"
    ]
  },
  {
    "name": "AI Agents for Beginners by Microsoft",
    "link": "https://github.com/microsoft/ai-agents-for-beginners",
    "difficulty": "Beginner",
    "type": "Course",
    "description": "This is a hands-on course specifically designed for beginners who want to learn how to build AI agents. It covers fundamental concepts and practical implementation using frameworks such as AutoGen.",
    "categories": [
      "AI Agents"
    ]
  },
  {
    "name": "Agents Towards Production",
    "link": "https://github.com/NirDiamant/agents-towards-production",
    "difficulty": "Advanced",
    "type": "Course",
    "description": "This practical playbook provides guidance on shipping AI agents to production environments. It covers essential aspects such as agent memory management, orchestration, deployment strategies, and security considerations for building robust and deployable AI agents.",
    "categories": [
      "AI Agents",
      "Agent Development",
      "AI Deployment",
      "LLMOps",
      "Agent Memory",
      "Workflow Automation"
    ]
  },
  {
    "name": "AI Engineering Hub",
    "link": "https://github.com/patchy631/ai-engineering-hub",
    "difficulty": "Advanced",
    "type": "Course",
    "description": "This hub offers over 70 real-world examples, tutorials, and agent applications that users can build, adapt, and deploy. It serves as a comprehensive resource for truly mastering Large Language Models (LLMs), Retrieval-Augmented Generation (RAG), and AI agents through practical projects.",
    "categories": [
      "AI Agents",
      "Large Language Models (LLMs)",
      "RAG (Retrieval-Augmented Generation)",
      "AI Applications",
      "Workflow Automation"
    ]
  },
  {
    "name": "Large Language Models: A Survey",
    "link": "https://doi.org/10.48550/arXiv.2402.06196",
    "difficulty": "Advanced",
    "type": "Book",
    "description": "This is a comprehensive research article on Large Language Models (LLMs), authored by Shervin Minaee, Tomas Mikolov, and Richard Socher, among others . It was initially submitted on February 9, 2024, and last revised on March 23, 2025 . The paper explores the rapid evolution of LLMs since the release of ChatGPT in November 2022, highlighting their powerful performance on a wide range of natural language processing tasks .\n\nThe article delves into how LLMs acquire their general-purpose language understanding and generation capabilities by training billions of parameters on massive amounts of text data, as predicted by scaling laws . The research reviews the most prominent LLM families, including GPT, LLaMA, and PaLM, and discusses their features, contributions, and limitations . It also provides an overview of techniques developed for building and augmenting LLMs . Furthermore, the paper analyzes popular datasets used for training, fine-tuning, and evaluating LLMs, and reviews widely used evaluation metrics. It includes a comparison of the performance of several popular LLMs on a set of representative benchmarks. The paper concludes by discussing open challenges and future research directions in the field of LLMs. The topics covered are Computer Science and Language (cs.CL) and Artificial Intelligence (cs.AI).",
    "categories": [
      "Large Language Models (LLMs)",
      "Deep Learning",
      "Machine Learning",
      "Introduction to AI and Fundamental Concepts",
      "AI Applications"
    ]
  },
  {
    "name": "Postman's AI Agent Builder",
    "link": "https://www.postman.com/explore/mcp-generator",
    "difficulty": "Unspecified",
    "type": "Tool",
    "description": "Postman’s new AI Agent Builder is a powerful platform that enables users to turn any API (from over 100,000 available APIs) into an MCP (Model Context Protocol) server in seconds, with no code required . This tool is designed to simplify the process of connecting existing APIs to AI agents and Large Language Models (LLMs) .\n\nUpon creation, the custom MCP server generated by this builder is ready for immediate use in various AI-powered applications such as Cursor, Windsurf, Claude Desktop, Docker, and more .\n\nThe process for generating and deploying an MCP server is streamlined into three easy steps :\n1.  Selection and Download: Users begin by accessing the platform via the provided link and can mix and match any desired API endpoints. The builder then generates a custom zip file for download. This comprehensive zip file includes a README with setup instructions, the selected endpoints, all necessary files to run the MCP server locally (compatible with environments like Cursor, Windsurf, and Docker), and an `.env` file with prefilled variables, requiring only the addition of API keys .\n2.  Local Setup: Once the zip file is unzipped, users open their terminal, run `npm install` in the root folder to install dependencies, and then insert their API keys into the `.env` file .\n3.  Connection: The final step involves navigating to an MCP-compatible application, such as Postman Desktop, Claude, or Cursor. Users select the MCP option, point to their `mcpServer.js` file, and click 'connect' to bring their custom MCP server live and operational .\n\nThis tool has been demonstrated to effectively crawl entire websites and map product URLs using Firecrawl’s `map_multiple_urls` endpoint via a custom MCP server . Users have also highlighted its utility, with one reporting successfully converting text into audio and saving MP3 files locally using the OpenAI API through this builder . It is recognized as a 'great addition to the ecosystem' for MCP functionality .",
    "categories": [
      "AI Agents",
      "Agent Development",
      "MCP (Model Context Protocol)",
      "Systems Integration",
      "Workflow Automation",
      "AI Deployment",
      "API Management"
    ]
  },
  {
    "name": "Prompt Engineering with Llama Models",
    "link": "https://www.deeplearning.ai/short-courses/prompt-engineering-with-llama-2/",
    "difficulty": "Beginner",
    "type": "Course",
    "description": "A resource focusing on prompt engineering specifically with Llama models .",
    "categories": [
      "Prompt Engineering",
      "Large Language Models (LLMs)"
    ]
  },
  {
    "name": "Structured LLM Output",
    "link": "https://www.deeplearning.ai/short-courses/getting-structured-llm-output/",
    "difficulty": "Beginner",
    "type": "Course",
    "description": "A resource covering techniques for generating structured output from Large Language Models (LLMs) . This often involves specific prompt engineering strategies.",
    "categories": [
      "Large Language Models (LLMs)",
      "Prompt Engineering"
    ]
  },
  
  {
    "name": "LLM Basics and Architecture",
    "link": "https://huggingface.co/learn/llm-course/en/chapter1/1",
    "difficulty": "Intermediate",
    "type": "Course",
    "description": "A resource delving into the fundamental concepts and architectural designs of Large Language Models (LLMs) .",
    "categories": [
      "Large Language Models (LLMs)",
      "Introduction to AI and Fundamental Concepts"
    ]
  },
  {
    "name": "In-depth on LLM Agents",
    "link": "https://llmagents-learning.org/f24",
    "difficulty": "Intermediate",
    "type": "Course",
    "description": "A comprehensive resource for understanding Large Language Model (LLM) agents in detail .",
    "categories": [
      "AI Agents",
      "Large Language Models (LLMs)"
    ]
  },
  {
    "name": "Agent Memory",
    "link": "https://www.deeplearning.ai/short-courses/llms-as-operating-systems-agent-memory/",
    "difficulty": "Intermediate",
    "type": "Course",
    "description": "A resource focused on the concept and implementation of memory mechanisms for AI agents .",
    "categories": [
      "Agent Memory",
      "AI Agents"
    ]
  },
    {
    "name": "Evaluation of AI Agents",
    "link": "https://www.deeplearning.ai/short-courses/evaluating-ai-agents/",
    "difficulty": "Advanced",
    "type": "Course",
    "description": "A resource dedicated to the methods and strategies for evaluating the performance and reliability of AI agents .",
    "categories": [
      "AI Agents"
    ]
  },
  {
    "name": "Multi-Agent System Design",
    "link": "https://www.deeplearning.ai/short-courses/multi-ai-agent-systems-with-crewai/",
    "difficulty": "Advanced",
    "type": "Course",
    "description": "A resource covering the principles and practices of designing systems with multiple collaborating AI agents .",
    "categories": [
      "Multi-Agent Systems",
      "AI Agents"
    ]
  },
  
  {
    "name": "Reliable AI via Guardrails",
    "link": "https://www.deeplearning.ai/short-courses/safe-and-reliable-ai-via-guardrails/",
    "difficulty": "Advanced",
    "type": "Course",
    "description": "A resource focused on ensuring the reliability of AI systems through the implementation of guardrails . This touches upon AI Ethics and practical AI Applications.",
    "categories": [
      "AI Ethics",
      "AI Applications"
    ]
  },

  {
    "name": "No-Code Tools for Building Agents (Demo)",
    "link": "https://www.youtube.com/watch?v=6N5HLxqJ-cY",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "A demo showcasing a full workflow using no-code tools to build AI agents, highlighting their potential for boosting efficiency and scaling operations without needing to code . This falls under workflow automation.",
    "categories": [
      "AI Agents",
      "Workflow Automation"
    ]
  },
 {
  "name": "Gemini for Education",
  "link": "https://edu.google.com/ai/gemini-for-education/",
  "difficulty": "Unspecified",
  "type": "Tool",
  "description": "Gemini for Education is a private and secure generative AI assistant from Google, specifically designed to transform teaching, learning, and administrative tasks for educators, students, and staff . It is included free of charge in all Google Workspace for Education editions for qualifying institutions . Built with Gemini 2.5 Pro, incorporating LearnLM, it is optimized for learning, making it the worlds leading model for learning . The tool provides enterprise-grade data protection, ensuring user data is not human-reviewed or used to train AI models and supports compliance with regulations like COPPA, FERPA, HIPPA, and FedRamp . Students under 18 receive a distinct product experience with fine-tuned guardrails, youth AI literacy resources, and trustworthy response features .Key capabilities for teaching include: rapidly drafting lesson plans, differentiating course material, and generating assessments and practice materials . For learning, it offers on-demand support for concepts, personalized practice quizzes for exam prep, and assistance with writing, brainstorming, and research, including citation generation . For administrative work, Gemini helps save time on communications, streamline tasks like summarizing documents and analyzing data, and enhance research efforts .Specific features include:- Deep Research: Provides detailed reports with citations and allows for follow-up questions to sharpen insights .- Gems: Enables users to create or utilize customized AI experts on any topic without coding, acting as a learning coach or brainstorming partner .- Gemini Canvas: An interactive space for creating quizzes, study guides, visual timelines, interactive prototypes, code snippets, and scripts .- Audio Overviews: Converts any file or Deep Research reports into podcasts for on-the-go learning .- Gemini Live: Facilitates real-time voice conversations for brainstorming, concept simplification, and presentation rehearsal, with screen/camera sharing for tailored help .Access to Gemini for Education is available via gemini.google.com or the Gemini mobile app .",
  "categories": [
    "AI Assistants",
    "AI in Education",
    "Generative AI",
    "Large Language Models (LLMs)",
    "AI Applications",
    "Content Generation with AI",
    "AI Ethics",
    "Workflow Automation",
    "Data Analysis"
  ]
},
{
  "name": "Mathematical theory of deep learning",
  "link": "https://doi.org/10.48550/arXiv.2407.18384",
  "difficulty": "Unspecified",
  "type": "Book",
  "description": "This book provides an introduction to the mathematical analysis of deep learning . It covers fundamental results in approximation theory, optimization theory, and statistical learning theory, which are the three main pillars of deep neural network theory . Serving as a guide for students and researchers in mathematics and related fields, the book aims to equip readers with foundational knowledge on the topic . It prioritizes simplicity over generality, and presents rigorous yet accessible results to help build an understanding of the essential mathematical concepts underpinning deep learning . The authors are Philipp Petersen and Jakob Zech . The paper was submitted on 25 Jul 2024 (v1) and last revised on 7 Apr 2025 (v3) .",
  "categories": [
    "Deep Learning",
    "Machine Learning",
    "Introduction to AI and Fundamental Concepts"
  ]
},
{
  "name": "CS229 Lecture Notes",
  "link": "https://cs229.stanford.edu/main_notes.pdf",
  "difficulty": "Unspecified",
  "type": "Book",
  "description": "These lecture notes, authored by Andrew Ng and Tengyu Ma and updated on June 11, 2023, provide a comprehensive introduction to machine learning, covering both foundational concepts and advanced topics. The material is mathematically grounded and delves into the theoretical underpinnings of various algorithms and models.The notes are structured into five main parts:- Supervised Learning: This section covers linear regression, classification (including logistic regression), generalized linear models, generative learning algorithms (Gaussian Discriminant Analysis and Naive Bayes), kernel methods, and Support Vector Machines (SVMs). It explores concepts like cost functions, gradient descent, batch and stochastic gradient descent, normal equations, and probabilistic interpretations of models [2-12].- Deep Learning: This part introduces neural networks, including multi-layer perceptrons (MLPs), various activation functions (e.g., ReLU, sigmoid, tanh, GELU), and modern neural network modules such as residual connections and layer normalization. It details the backpropagation algorithm for efficient gradient computation in neural networks [13-17].- Generalization and Regularization: Here, the notes discuss the critical aspects of model performance on unseen data. Key topics include the bias-variance tradeoff, the double descent phenomenon, sample complexity bounds, different regularization techniques (like L2 regularization/weight decay), and implicit regularization effects of optimizers. Model selection methods such as cross-validation are also covered, along with an overview of Bayesian statistics in regularization [18-26].- Unsupervised Learning: This section explores methods for discovering patterns in unlabeled data. It includes clustering algorithms (e.g., k-means), Expectation-Maximization (EM) algorithms for density estimation (e.g., mixture of Gaussians), Principal Components Analysis (PCA) for dimensionality reduction and noise reduction, and Independent Components Analysis (ICA) for source separation [19, 27-32].- Reinforcement Learning and Control: The final part delves into how agents learn to make sequential decisions to maximize rewards. It defines Markov Decision Processes (MDPs) and covers algorithms like Value Iteration and Policy Iteration. It also addresses continuous state MDPs through discretization and value function approximation, and introduces Linear Quadratic Regulation (LQR), Differential Dynamic Programming (DDP), Linear Quadratic Gaussian (LQG), and Policy Gradient methods (REINFORCE) [33-42].Additionally, the notes touch upon Self-supervised learning and Foundation Models, explaining the paradigm of pretraining and adaptation, pretraining methods in computer vision (supervised and contrastive learning), and pretrained large language models (Transformers, zero-shot learning, and in-context learning) [33, 43-47].",
  "categories": [
    "Introduction to AI and Fundamental Concepts",
    "Machine Learning",
    "Deep Learning",
    "Large Language Models (LLMs)",
    "AI Ethics",
    "AI Agents",
    "RAG (Retrieval-Augmented Generation)",
    "AI Applications",
    "Data Analysis"
  ]
},{
  "name": "RAGFlow",
  "link": "https://github.com/infiniflow/ragflow",
  "difficulty": "Unspecified",
  "type": "Tool",
  "description": "RAGFlow is an open-source RAG (Retrieval-Augmented Generation) engine based on deep document understanding [1-3]. It provides a streamlined RAG workflow designed for businesses of any scale, combining Large Language Models (LLMs) to deliver truthful question-answering capabilities backed by well-founded citations from various complex formatted data .Key features of RAGFlow include:- Deep document understanding-based knowledge extraction from unstructured data with complicated formats, capable of finding relevant information within unlimited tokens .- Template-based chunking that is intelligent and explainable, offering numerous template options .- Grounded citations with reduced hallucinations, providing visualization of text chunking for human intervention, quick access to key references, and traceable citations .- Compatibility with heterogeneous data sources, supporting a wide array of formats such as Word, slides, Excel, TXT, images, scanned copies, structured data, and web pages .- An automated and effortless RAG workflow that includes configurable LLMs and embedding models, multiple recall paired with fused re-ranking, and intuitive APIs for seamless business integration .Recent updates have added a Python/JavaScript code executor component to the Agent , support for cross-language queries , the ability to use a multi-modal model for understanding images within PDF or DOCX files , and integration with Internet search (Tavily) for Deep Research-like reasoning . It also supports text-to-SQL statements through RAG .RAGFlow is designed for easy deployment, with prerequisites including Docker and Docker Compose , and offers pre-built Docker images in various editions . It also supports launching the service from source for development purposes . RAGFlow has gained significant community support, evidenced by its 61k stars and 6.2k forks on GitHub .",
  "categories": [
    "RAG (Retrieval-Augmented Generation)",
    "Large Language Models (LLMs)",
    "Deep Learning",
    "AI Applications",
    "Document Processing",
    "Content Generation with AI",
    "Workflow Automation",
    "Systems Integration",
    "Data Analysis",
    "AI Agents"
  ]
},
{
  "name": "AutoAgent: Fully-Automated and Zero-Code LLM Agent Framework",
  "link": "https://github.com/HKUDS/AutoAgent",
  "difficulty": "Unspecified",
  "type": "Tool",
  "description": "AutoAgent is a Fully-Automated and highly Self-Developing framework that enables users to create and deploy LLM agents through Natural Language Alone . It serves as a personal AI assistant, designed to be dynamic, extensible, customized, and lightweight .Key capabilities include:- Agentic-RAG with Native Self-Managing Vector Database: It outperforms industry-leading solutions like LangChain by being equipped with a native self-managing vector database .- Zero-Code Agent and Workflow Creation: Users can effortlessly build ready-to-use tools, agents, and workflows using natural language, requiring no coding [2-5].- Universal LLM Support: It seamlessly integrates with a wide range of LLMs, including OpenAI, Anthropic, Deepseek, vLLM, Grok, and Huggingface .- Flexible Interaction Modes: Supports both function-calling and ReAct interaction modes .- High Performance: Delivers comparable performance to many Deep Research Agents on the GAIA Benchmark . It is presented as a cost-effective, open-source alternative to OpenAIs Deep Research subscription .- User-Friendly Interface: Offers an easy-to-deploy CLI interface and handles file uploads for enhanced data interaction .The framework provides an out-of-the-box multi-agent system in its user mode . Installation can be done via CLI or Docker , and it supports adding API keys for various LLMs and third-party tool platforms like RapidAPI . Future developments include expanding evaluations to SWE-bench and WebArena, supporting Computer-Use agents with GUI interaction, integrating with more tool platforms (e.g., Composio), adding code sandboxes (e.g., E2B), and developing a comprehensive web interface . AutoAgent is primarily written in Python (99.8%) . It has garnered significant community interest with 5.7k stars and 774 forks on GitHub .",
  "categories": [
    "AI Agents",
    "Agent Development",
    "Multi-Agent Systems",
    "Large Language Models (LLMs)",
    "RAG (Retrieval-Augmented Generation)",
    "Workflow Automation",
    "Systems Integration",
    "AI Applications",
    "Content Generation with AI",
    "Deep Learning"
  ]
},
{
  "name": "LLaMA-Factory: Unified Efficient Fine-Tuning of 100+ LLMs & VLMs (ACL 2024)",
  "link": "https://github.com/hiyouga/LLaMA-Factory",
  "difficulty": "Unspecified",
  "type": "Tool",
  "description": "LLaMA-Factory is an open-source tool designed for Unified Efficient Fine-Tuning of over 100 Large Language Models (LLMs) and Vision Language Models (VLMs) [13-15]. It simplifies the fine-tuning process with both a zero-code CLI and an intuitive Web UI .Key features and capabilities include:- Extensive Model Support: Compatible with a wide array of models such as LLaMA, LLaVA, Mistral, Mixtral-MoE, Qwen, DeepSeek, Yi, Gemma, ChatGLM, and Phi, among others . It provides day-N support for cutting-edge models .- Integrated Training Approaches: Supports various methods including (continuous) pre-training, (multimodal) supervised fine-tuning, reward modeling, and advanced preference learning algorithms like PPO, DPO, KTO, ORPO, and SimPO .- Scalable Resource Optimization: Offers 16-bit full-tuning, freeze-tuning, LoRA, and 2/3/4/5/6/8-bit QLoRA, leveraging multiple quantization techniques (AQLM/AWQ/GPTQ/LLM.int8/HQQ/EETQ) .- Advanced Algorithms & Practical Tricks: Integrates sophisticated algorithms such as GaLore, BAdam, APOLLO, Adam-mini, Muon, DoRA, LongLoRA, LLaMA Pro, Mixture-of-Depths, LoRA+, LoftQ, and PiSSA [14, 23-29]. It also includes performance-enhancing tricks like FlashAttention-2, Unsloth, Liger Kernel, RoPE scaling, and NEFTune .- Wide Task Support: Handles diverse tasks including multi-turn dialogue, tool using, image understanding, visual grounding, video recognition, and audio understanding .- Experiment Monitoring & Faster Inference: Supports various experiment monitors (LlamaBoard, TensorBoard, Wandb, MLflow, SwanLab)  and provides faster inference via OpenAI-style API, Gradio UI, and CLI with vLLM or SGLang workers .- Hardware Compatibility: Works across different hardware, including CUDA, Ascend NPU, and AMD ROCm [35-40].LLaMA-Factory is utilized by major companies like Amazon, NVIDIA, and Aliyun . It has achieved significant community traction with 55k stars and 6.8k forks on GitHub . The underlying research is detailed in the paper LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models .",
  "categories": [
    "Machine Learning",
    "Deep Learning",
    "Large Language Models (LLMs)",
    "AI Applications",
    "Model Management",
    "LLMOps",
    "AI Automation",
    "Content Generation with AI",
    "Multimodal AI"
  ]
},
{
  "name": "Transformer Lab: Open Source Application for Advanced LLM + Diffusion Engineering",
  "link": "https://github.com/transformerlab/transformerlab-app",
  "difficulty": "Unspecified",
  "type": "Tool",
  "description": "Transformer Lab is an open-source application designed for Advanced LLM + Diffusion Engineering, allowing users to interact, train, fine-tune, and evaluate large language models on their own computer . This app is supported by Mozilla through the Mozilla Builders Program .Its features are accessible through a simple cross-platform GUI and include:- One-click Model Downloads: Easily download hundreds of popular models such as DeepSeek, Llama3, Qwen, Phi4, Gemma, Mistral, Mixtral, and Command-R, and any LLM from Hugging Face .- Flexible Fine-tuning and Training: Supports fine-tuning using MLX on Apple Silicon and Huggingface on GPU .- RLHF and Preference Optimization: Incorporates DPO, ORPO, SIMPO, and Reward Modeling .- Cross-Operating System Compatibility: Available as Windows, MacOS, and Linux applications .- Comprehensive Chat Functionality: Offers chat, completions, preset prompts, chat history, parameter tweaking, batched inference, and tool use/function calling (in alpha) .- Multiple Inference Engines: Utilizes MLX on Apple Silicon, Huggingface Transformers, vLLM, and Llama CPP .- Model Evaluation and RAG: Includes model evaluation capabilities and Retrieval-Augmented Generation (RAG) with a drag-and-drop file UI, compatible with various engines .- Dataset Building: Enables users to build datasets by pulling from hundreds of common Hugging Face datasets or providing their own via drag-and-drop .- Utilities and API: Features embedding calculation, a full REST API, and the ability to run in the cloud (with the UI locally or entirely on a single machine) .- Model Conversion and Plugins: Allows conversion of models between Huggingface, MLX, and GGUF formats, and supports custom plugins for extended functionality .- Developer Tools: Provides an embedded Monaco Code Editor for plugin editing and viewing behind-the-scenes operations, along with prompt editing and inference logs .This project is actively developed and has a strong community presence with 3.7k stars and 325 forks on GitHub . It is primarily written in TypeScript (96.7%) and JavaScript (2.8%) .",
  "categories": [
    "Large Language Models (LLMs)",
    "Deep Learning",
    "Generative AI",
    "Machine Learning",
    "AI Applications",
    "Model Management",
    "AI Deployment",
    "Prompt Engineering",
    "RAG (Retrieval-Augmented Generation)",
    "AI Agents",
    "Data Analysis",
    "Multimodal AI"
  ]
},
{
  "name": "xpander.ai: Backend-as-a-Service for AI Agents",
  "link": "https://github.com/xpander-ai/xpander.ai",
  "difficulty": "Unspecified",
  "type": "Tool",
  "description": "xpander.ai is a framework-agnostic Backend-as-a-Service (BaaS) infrastructure for autonomous AI agents . It abstracts away infrastructure complexity, allowing users to focus on building intelligent, effective, and production-ready AI agents .Its core offerings include:- Comprehensive Agent Capabilities: Provides memory, tools, multi-user state, various agent triggering options (MCP, A2A, API, Web interfaces), storage, and agent-to-agent messaging .- Framework Flexibility: Compatible with popular agent frameworks like OpenAI ADK, Agno, CrewAI, and LangChain, as well as direct LLM APIs .- Robust Tool Integration: Offers access to a comprehensive MCP-compatible tools library and pre-built integrations .- Scalable Hosting & Deployment: Enables effortless deployment and scaling of agents on managed infrastructure .- Real-time Event Management: Features event streaming capabilities for Slackbots, ChatUIs, Agent2Agent communication, and Webhook integrations .- API Guardrails: Implements strong guardrails using an Agent-Graph-System to define and manage dependencies between API actions for tool-use .xpander.ai offers quick setup for integrating a backend into agents, providing Python and Node.js SDKs, and a CLI for agent creation and deployment [53-56]. It supports deploying Docker containers to the cloud . The platform showcases featured open-source AI agents built using its services, such as a Kubernetes operations agent, a Coding Agent, and an NVIDIA Meeting Recorder . It has 362 stars and 57 forks on GitHub  and is mainly developed in Jupyter Notebook (58.5%), Python (36.2%), and JavaScript (3.8%) .",
  "categories": [
    "AI Agents",
    "Agent Development",
    "Multi-Agent Systems",
    "Agent Memory",
    "MCP (Model Context Protocol)",
    "AI Deployment",
    "Workflow Automation",
    "Systems Integration",
    "AI Applications"
  ]
},
{
    "name": "VibeKit: Run coding agents in a secure sandbox",
    "link": "https://github.com/superagent-ai/vibekit",
    "difficulty": "Unspecified",
    "type": "Tool",
    "description": "VibeKit es un SDK sencillo para ejecutar agentes de codificación de forma segura en un entorno aislado (sandbox) . Está diseñado para integrar potentes agentes de codificación como Claude Code, OpenAI Codex, Gemini CLI y SST Opencode en tus aplicaciones o flujos de trabajo . Permite generar y ejecutar código real de forma segura, transmitir la salida a interfaces de usuario en tiempo real y operar con aislamiento y flexibilidad, principalmente en la nube (con soporte local planificado) .Entre sus características clave se incluyen:   Sandboxing seguro: Proporciona un entorno robusto para la ejecución segura de código .   SDK drop-in: Facilita la integración con diversos agentes de codificación .   Ejecución basada en la nube: Permite el funcionamiento de agentes en la nube (con soporte local planificado) .   Automatización de GitHub: Soporta funciones como la creación de ramas, commits y pull requests .   Historial de prompts y continuidad de contexto: Mantiene el contexto conversacional para los agentes .   Salida en streaming: Permite actualizaciones en tiempo real a las interfaces de usuario .   Soporte OpenTelemetry: Para trazabilidad y métricas .   Compatibilidad con tiempos de ejecución de sandboxes: Compatible con E2B, Daytona, Northflank, Cloudflare y Dagger .   Ejecución de comandos arbitrarios: Capacidad de ejecutar cualquier comando dentro de entornos sandbox .Los casos de uso de VibeKit incluyen la creación de herramientas internas de depuración, el lanzamiento de funciones impulsadas por IA, la automatización de tareas de codificación repetitivas y la prueba segura de la salida de LLMs en entornos de producción o prototipos . El proyecto tiene licencia MIT  y está desarrollado principalmente en TypeScript (97.8%) . Ha captado el interés de la comunidad con 772 estrellas y 103 forks en GitHub .",
    "categories": [
        "AI Agents",
        "Agent Development",
        "Systems Integration",
        "AI Applications",
        "Large Language Models (LLMs)",
        "Workflow Automation",
        "AI Deployment"
    ]
},
{
    "name": "Context Engineering Template (GitHub Repo)",
    "link": "https://github.com/coleam00/context-engineering-intro",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "This repository is presented as a comprehensive template for getting started with Context Engineering, a discipline focused on designing the context that AI coding assistants need to complete tasks from beginning to end . It claims to be \"10 times better than prompt engineering and 100 times better than vibe coding\" , offering a complete system that includes documentation, examples, rules, patterns, and validation, unlike prompt engineering which is limited to task formulation . Its benefits include reducing AI failures (mostly context failures), ensuring consistency with project patterns, enabling complex features, and allowing autocorrection through validation loops . While centered around Claude Code , the strategy is applicable to any AI coding assistant . The template structure includes .claude/ for commands and settings, PRPs/ for Product Requirements Prompts, examples/ for code examples (critical for success as AI performs better with patterns) , CLAUDE.md for global rules , INITIAL.md for feature requests , and README.md . It provides a quick start guide  and a step-by-step guide  covering setting up global rules (e.g., project awareness, code structure, testing, style, documentation standards) , creating initial feature requests (e.g., specific functionality, examples, documentation links, other considerations) [11-13], generating comprehensive PRPs (Product Requirements Prompts)  which are blueprints with context, steps, validation, and test requirements , and executing PRPs for feature implementation . The repository has 7.3k stars and 1.5k forks on GitHub  and is primarily developed in Python (53.3%) and TypeScript (46.6%) . It explicitly notes that it doesnt focus on RAG and tools for context engineering as more is planned for the future . Best practices include being explicit in INITIAL.md, providing comprehensive examples (both what to do and what not to do, and error handling patterns), using validation gates (PRPs include test commands), leveraging documentation, and customizing CLAUDE.md .",
    "categories": [
      "Prompt Engineering",
      "AI Agents",
      "Large Language Models (LLMs)",
      "AI Deployment",
      "Workflow Automation"
    ]
  },
  {
    "name": "GitHub MCP Server",
    "link": "https://github.com/github/github-mcp-server",
    "difficulty": "Unspecified",
    "type": "Tool",
    "description": "This is the official GitHub MCP Server designed to connect AI tools directly to the GitHub platform . It empowers AI agents, assistants, and chatbots to interact with repositories and code files, manage issues and Pull Requests (PRs), analyze code, and automate workflows, all through natural language interactions . Key use cases include repository management (browsing, searching, analyzing commits), issue & PR automation (creating, updating, triaging bugs, reviewing code), CI/CD & workflow intelligence (monitoring GitHub Actions, analyzing failures), code analysis (security findings, Dependabot alerts), and team collaboration (discussions, notifications) . It is built for developers wanting to connect AI tools to GitHub context and capabilities, from simple natural language queries to complex multi-step agent workflows . The server can be implemented as a remote GitHub-hosted server  or locally (requiring Docker) . It supports secure management of Personal Access Tokens (PATs) using environment variables for security best practices (e.g., minimum scopes, separate tokens, never committing, regular rotation, file permissions) . Users can enable or disable specific groups of functionalities via toolsets to control which GitHub API capabilities are available to AI tools . Available toolsets include context (strongly recommended), actions, code_security, dependabot, discussions, issues, notifications, orgs, pull_requests, repos, secret_protection, and users (all on by default) . A special all toolset enables all functionalities . The server also includes dynamic tool discovery (beta feature) to help models avoid confusion from too many tools  and can run in read-only mode to prevent modifications . It supports GitHub Enterprise Server and Enterprise Cloud with data residency via the --gh-host flag or GITHUB_HOST environment variable . The tool descriptions can be overridden using a JSON config file or environment variables for internationalization (i18n) . The repository is developed primarily in Go (98.9%)  and is licensed under MIT . It has accumulated 19.3k stars and 1.6k forks on GitHub .",
    "categories": [
      "AI Agents",
      "MCP (Model Context Protocol)",
      "Systems Integration",
      "Workflow Automation",
      "AI Automation",
      "AI Deployment",
      "AI Applications"
    ]
  },
  {
    "name": "MCP Project End to End + Explanation of MCP (Sreemanti Dey)",
    "link": "https://www.youtube.com/watch?v=8dzuX4Y06Io",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "This resource is an \"End to End MCP Project\" that includes a detailed explanation of the Model Context Protocol (MCP) . It is part of a collection of practical Machine Learning and Data Science projects recommended by Sreemanti Dey for hands-on learning . The MCP is an open protocol that standardizes how Large Language Model (LLM) applications can access context in terms of tools and data resources, based on a client-server architecture [conversation history, drawing from 151]. This project likely guides users through the complete implementation of an MCP-based application [conversation history, implied by 93].",
    "categories": [
      "MCP (Model Context Protocol)",
      "AI Agents",
      "Large Language Models (LLMs)",
      "Machine Learning",
      "AI Deployment",
      "AI Applications",
      "Agent Development"
    ]
  },
  {
    "name": "RAG Project End to End + Explanation of RAG (Sreemanti Dey)",
    "link": "https://www.youtube.com/watch?v=ERijuxJAaoQ",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "This resource focuses on an \"End to End RAG Project\" with a detailed explanation of Retrieval Augmented Generation (RAG) . It is part of a collection of practical Machine Learning and Data Science projects recommended by Sreemanti Dey for hands-on learning . RAG is a technique that enhances the capabilities of LLMs by allowing them to retrieve relevant information from an external knowledge base to inform their responses, which reduces hallucinations and provides up-to-date information [conversation history, drawing from 135]. This project aims to offer a practical guide for building and deploying an RAG-based application from start to finish [conversation history, implied by 93].",
    "categories": [
      "RAG (Retrieval-Augmented Generation)",
      "AI Agents",
      "Large Language Models (LLMs)",
      "Machine Learning",
      "AI Applications",
      "Agent Development"
    ]
  },
  {
    "name": "Google Cloud Skills Boost Catalog",
    "link": "https://www.cloudskillsboost.google/",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "This is Google Clouds comprehensive learning catalog, offering over 980+ learning activities designed to help users apply their skills in the Google Cloud console . The catalog features a variety of activity formats, including bite-size individual labs and multi-module courses that combine videos, documents, labs, and quizzes . Students are provided with temporary credentials to actual cloud resources, enabling hands-on practice with real Google Cloud environments . Users can earn badges for completing activities and define, track, and measure their success . Among its wide-ranging offerings, the catalog includes resources for learning about generative AI from the ground up and gaining a solid understanding of the field .",
    "categories": [
      "Introduction to AI and Fundamental Concepts",
      "Generative AI",
      "Machine Learning",
      "AI Deployment",
      "AI Applications",
      "Large Language Models (LLMs)"
    ]
  },
  {
    "name": "AI Engineering: Building Applications with Foundation Models (Chip Huyen, December 2024)",
    "link": "https://www.linkedin.com/posts/muhammad-zarar-4995aa246_aiengineering-activity-7347964728385626113-mtsL/?utm_source=share&utm_medium=member_desktop&rcm=ACoAAADvoYgBR6OaUkic3fcx6maIpbJDW9ElVf0",
    "difficulty": "Unspecified",
    "type": "Book",
    "description": "This book, authored by Chip Huyen and slated for December 2024, addresses the significant surge in AI applications and the reduced entry barrier for developers following advancements like ChatGPT . It positions AI as a powerful development tool accessible to everyone, noting that its even possible to build applications without writing a single line of code . The book highlights that modern AI adoption is built upon existing techniques, such as language modeling (since the 1950s) and Retrieval-Augmented Generation (RAG), which leverages older retrieval technology . It emphasizes that the best practices for deploying traditional machine learning applications—systematic experimentation, rigorous evaluation, and relentless optimization for faster and cheaper models—remain crucial for foundation model-based applications . The book aims to help readers understand AI engineering and how it differs from traditional machine learning engineering, learn the process of developing an AI application including challenges and solutions, explore model adaptation techniques (prompt engineering, RAG, fine-tuning, agents, dataset engineering), examine latency and cost bottlenecks when serving foundation models, and choose the right model, dataset, evaluation benchmarks, and metrics . It has received positive feedback from experts, with comments like Chip Huyen nails it again .",
    "categories": [
      "AI Applications",
      "Large Language Models (LLMs)",
      "Prompt Engineering",
      "RAG (Retrieval-Augmented Generation)",
      "AI Agents",
      "Machine Learning",
      "AI Deployment",
      "LLMOps",
      "Introduction to AI and Fundamental Concepts"
    ]
  },
  {
    "name": "GPT-4.1 Prompting Guide",
    "link": "N/A (OpenAI Documentation Guide)",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "The GPT-4.1 Prompting Guide provides important prompting tips derived from extensive internal testing to help developers fully leverage the improved abilities of the new GPT-4.1 model family . GPT-4.1 represents a significant step forward from GPT-4o in capabilities across coding, instruction following, and long context . While many typical best practices (like providing context examples, specific instructions, and inducing planning) still apply, GPT-4.1 is trained to follow instructions more closely and literally than its predecessors, making it highly steerable with well-specified prompts . The guide emphasizes building agentic workflows with GPT-4.1, noting its state-of-the-art performance on SWE-bench Verified (solving 55% of problems) . Key recommendations for agent prompts include Persistence (to ensure multi-message turns) , Tool-calling (to encourage tool use and reduce hallucination) , and optional Planning (to ensure explicit planning and reflection between tool calls) . Adhering to these simple instructions significantly improved internal SWE-bench Verified scores . The guide strongly encourages using the tools field in OpenAI API requests for passing tools to minimize errors and observed a 2% increase in pass rate . Developers should use clear names and detailed descriptions for tools and their parameters . Prompting-induced planning or \"chain-of-thought\" (CoT) can be effective to break down problems, improve output quality, and increase pass rates (e.g., 4% increase on SWE-bench Verified) by making the model \"think out loud\" . The guide also covers long context usage (up to 1M tokens)  and advises placing instructions at both the beginning and end of long context for optimal performance . For instruction following, GPT-4.1 exhibits outstanding performance, but developers may need to be explicit about what to do or not to do, as the model follows instructions more literally . A recommended workflow for developing instructions involves starting with high-level guidance, adding specific detail sections, and using ordered lists for workflow steps . It also highlights common failure modes, such as models hallucinating tool inputs if not enough information is available . The guide includes a sample prompt for SWE-bench Verified  and a customer service agent example , demonstrating best practices for workflow, problem-solving strategy, and instruction diversity . It provides general advice on prompt structure  and delimiters like Markdown, XML, and JSON [19-21]. The GPT-4.1 family also features substantially improved diff capabilities, with a recommended diff format (V4A diff format) extensively trained on, and a reference Python implementation for an apply_patch tool [22-25].",
    "categories": [
      "Introduction to AI and Fundamental Concepts",
      "Generative AI",
      "Large Language Models (LLMs)",
      "Prompt Engineering",
      "Prompt Optimization",
      "Prompting Strategies",
      "Interaction with LLMs",
      "AI Agents",
      "Agent Development",
      "AI Applications"
    ]
  },
  {
    "name": "WebMCP",
    "link": "https://github.com/MiguelsPizza/WebMCP",
    "difficulty": "Unspecified",
    "type": "Tool",
    "description": "WebMCP (Model Context Protocol - Browser-specific) is a tool that allows websites to function as MCP servers, exposing their existing functionality (e.g., APIs, forms, state) as structured tools that AI agents can directly call . It operates securely within the browsers existing authentication and security model [26-28]. For users, the MCP-B Chrome extension enables AI interactions on MCP-enabled sites by auto-detecting tools and allowing chat-based AI queries (e.g., \"Add to cart\") that execute tools across tabs using browser sessions for authentication . For website owners, implementation involves installing npm packages and adding an MCP server to register tools wrapping existing logic [28-30]. These tools can be dynamic and page-scoped, with the extension automatically injecting clients, making the site securely AI-ready within the browser sandbox . From an AIs perspective, it receives domain-prefixed tools from open tabs and calls them with JSON parameters for deterministic actions . The extension handles routing and navigation, with responses enabling chaining of actions . This approach supports dynamic updates and authentication via browser context, reducing errors compared to visual automation . The project provides quick start guides , a live demo , and examples like a simple todo app where AI agents can manage tasks (e.g., add, update, delete todos) . It also features a native host that bridges the browser to local MCP clients (e.g., Claude Desktop, Cursor), allowing tools from your website to be called from desktop apps . WebMCP emphasizes security, respecting the browser sandbox and same-origin policy, collecting no data, and allowing users to audit tool calls via the extension .",
    "categories": [
      "AI Agents",
      "Agent Development",
      "MCP (Model Context Protocol)",
      "Systems Integration",
      "Workflow Automation",
      "AI Deployment",
      "AI Applications"
    ]
  },
  {
    "name": "LLaMA-Factory: Unified Efficient Fine-Tuning of 100+ LLMs & VLMs",
    "link": "https://github.com/hiyouga/LLaMA-Factory",
    "difficulty": "Unspecified",
    "type": "Tool",
    "description": "LLaMA-Factory is a powerful open-source tool designed for the unified efficient fine-tuning of over 100 Large Language Models (LLMs) and Visual Language Models (VLMs) . It simplifies the fine-tuning process with both a zero-code Command Line Interface (CLI) and a user-friendly Web UI (LLaMA Board GUI) . The platform supports a diverse range of cutting-edge models including LLaMA, LLaVA, Mistral, Mixtral-MoE, Qwen, DeepSeek, Yi, Gemma, ChatGLM, Phi, and more, offering day-N support for newly released models . It integrates various training methods such as (Continuous) pre-training, (multimodal) supervised fine-tuning, reward modeling, PPO, DPO, KTO, and ORPO . For scalable and efficient resource utilization, LLaMA-Factory supports 16-bit full-tuning, freeze-tuning, LoRA, and 2/3/4/5/6/8-bit QLoRA through different quantization techniques (AQLM/AWQ/GPTQ/LLM.int8/HQQ/EETQ) . It also incorporates advanced algorithms like GaLore, BAdam, APOLLO, Muon, DoRA, LongLoRA, LLaMA Pro, Mixture-of-Depths, LoRA+, LoftQ, and PiSSA, alongside practical tricks such as FlashAttention-2, Unsloth, Liger Kernel, RoPE scaling, NEFTune, and rsLoRA for optimized performance [38, 45-47]. LLaMA-Factory is capable of handling a wide array of tasks, including multi-turn dialogue, tool using, image understanding, visual grounding, and video/audio recognition . It provides faster inference capabilities via an OpenAI-style API, Gradio UI, and CLI, with support for vLLM or SGLang workers . The tool is widely recognized and used by prominent organizations like Amazon, NVIDIA, and Aliyun . It includes comprehensive support for various datasets used in pre-training, supervised fine-tuning, and preference learning . Users can easily get started by installing it from source, pre-built Docker images, or setting up a virtual environment, with specific instructions for Windows and Ascend NPU users also provided [52-56].",
    "categories": [
      "Large Language Models (LLMs)",
      "Machine Learning",
      "Deep Learning",
      "AI Deployment",
      "AI Applications",
      "Generative AI",
      "LLMOps",
      "Prompt Engineering",
      "AI Agents"
    ]
  },
  {
    "name": "Google for Education: Impulsando la educación con la IA",
    "link": "https://edu.google.com/intl/ALL_es/ai/education/",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "Google for Education se compromete a hacer que la IA sea útil para todo el mundo en el ámbito educativo, tanto dentro como fuera de clase, aplicando la tecnología de forma responsable y con un enfoque centrado en las personas . La IA no busca reemplazar la experiencia, el conocimiento ni la creatividad de un docente, sino ser una herramienta útil para mejorar y enriquecer las experiencias de enseñanza y aprendizaje . Google garantiza la protección de datos reforzada en sus herramientas de IA, como Gemini, y asegura que los datos de cliente de los servicios principales de Google Workspace for Education no se usan sin permiso para entrenar modelos de IA generativa y de lenguaje extenso en los que se basan Gemini y otros sistemas externos .Para los docentes, la IA puede optimizar su labor, impulsando su creatividad y productividad al ahorrarles tiempo que pueden invertir en ellos mismos y en sus alumnos . Gemini, como asistente de IA integrado en Google Workspace for Education, ayuda a los docentes a ahorrar tiempo, obtener inspiración con ideas innovadoras y crear experiencias de aprendizaje atractivas en un entorno privado y seguro . Próximamente, la IA también sugerirá preguntas sobre vídeos de YouTube en Classroom para hacer las clases más atractivas . La gama Chromebook Plus es una opción económica y potente que integra lo mejor de la IA de Google para el profesorado y el personal .Para los alumnos, la IA ayuda a personalizar el aprendizaje al adaptarse a sus necesidades individuales . Las prácticas guiadas de Google Classroom pueden ofrecer comentarios automáticos en tiempo real y consejos útiles a los alumnos que tengan dificultades . Además, la IA integrada en los Chromebooks proporciona funciones avanzadas de conversión de texto a voz, dictado y subtítulos automáticos .La seguridad y privacidad son una prioridad fundamental . Google bloquea el 99.9% del spam, phishing y malware con sistemas de detección basados en IA, y los dispositivos ChromeOS no han registrado ataques de ransomware . La infraestructura de Google Workspace for Education cumple rigurosos estándares de cumplimiento como el RGPD, FERPA y COPPA . Google se toma muy en serio la seguridad de los usuarios, especialmente la de los niños, diseñando funciones y productos de IA con protecciones adaptadas a diferentes grupos de edad y realizando pruebas rigurosas para minimizar daños potenciales .Google ofrece una variedad de recursos de formación, kits de herramientas y guías sobre IA para docentes, incluyendo cursos sobre IA generativa, habilidades digitales aplicadas y cursos de IA y aprendizaje automático de Grow with Google . También se compromete a una colaboración continua con centros educativos, docentes y expertos para desarrollar y mejorar sus herramientas de IA, introduciendo nuevas funciones de forma gradual para que los centros puedan decidir qué es lo que más les conviene .",
    "categories": [
      "Introduction to AI and Fundamental Concepts",
      "Generative AI",
      "Large Language Models (LLMs)",
      "Prompt Engineering",
      "AI Agents",
      "AI Applications",
      "AI in Education",
      "AI Ethics",
      "Workflow Automation",
      "AI Deployment"
    ]
  },
  {
    "name": "MarkItDown",
    "link": "https://github.com/microsoft/markitdown",
    "difficulty": "Unspecified",
    "type": "Tool",
    "description": "MarkItDown es una utilidad ligera de Python diseñada para convertir varios tipos de archivos a formato Markdown, pensada principalmente para su uso con modelos de lenguaje grandes (LLMs) y otras pipelines de análisis de texto . Su objetivo es preservar la estructura y el contenido importante del documento, incluyendo encabezados, listas, tablas y enlaces, aunque no está destinada a conversiones de alta fidelidad para consumo humano, sino para ser procesada por herramientas de análisis de texto .Tipos de archivos compatibles: MarkItDown soporta la conversión de una amplia gama de formatos, tales como: PDF, PowerPoint, Word, Excel, imágenes (incluyendo metadatos EXIF y OCR), audio (metadatos EXIF y transcripción de voz), HTML, formatos basados en texto (CSV, JSON, XML), archivos ZIP (iterando sobre su contenido), URLs de YouTube y EPUBs, entre otros .¿Por qué Markdown? Este formato es muy similar al texto plano, con un marcado mínimo, pero permite representar la estructura fundamental del documento . Los LLMs principales, como GPT-4o de OpenAI, hablan Markdown de forma nativa y a menudo lo incorporan en sus respuestas sin ser solicitados, lo que sugiere que han sido entrenados con grandes volúmenes de texto formateado en Markdown y lo entienden bien . Además, las convenciones de Markdown son altamente eficientes en el uso de tokens .Características clave y uso:   Integración con MCP (Model Context Protocol): MarkItDown ofrece un servidor MCP para la integración con aplicaciones LLM, como Claude Desktop . El MCP es un protocolo abierto que estandariza cómo las aplicaciones LLM acceden a herramientas y recursos de datos . Esto permite que MarkItDown añada capacidades de scraping (rastreo web) potentes a editores de código a través de MCP .   Requisitos: Requiere Python 3.10 o superior y se recomienda el uso de un entorno virtual .   Instalación: Se instala fácilmente con pip install markitdown[all] para incluir todas las dependencias opcionales, o se pueden instalar individualmente (ej. [pdf, docx, pptx]) [7-9]. También se puede instalar desde el código fuente .   Uso por línea de comandos (CLI): Permite la conversión de archivos como markitdown path-to-file.pdf -o document.md o el encadenamiento de contenido a través de pipes .   Plugins: Soporta plugins de terceros, que están deshabilitados por defecto y se pueden habilitar mediante comandos específicos .   Integración con Azure Document Intelligence: Puede utilizarse con Microsoft Document Intelligence para conversiones avanzadas .   API de Python: Ofrece una API de Python para la conversión programática de documentos, incluyendo la integración con Azure Document Intelligence y la descripción de imágenes usando LLMs (ej. GPT-4o) .   Docker: Se puede construir y ejecutar en un contenedor Docker .Comunidad y contribuciones: El proyecto MarkItDown es de código abierto (licencia MIT) y acoge contribuciones [13-15]. Sigue el Código de Conducta de Código Abierto de Microsoft y anima a la comunidad a revisar issues y pull requests . El repositorio tiene una gran cantidad de soporte comunitario, con 69.5 mil estrellas y 3.7 mil forks , y ha sido utilizado por 1.7 mil proyectos , con contribuciones de 65 desarrolladores . Su desarrollo es principalmente en Python .",
    "categories": [
      "Document Processing",
      "Large Language Models (LLMs)",
      "MCP (Model Context Protocol)",
      "AI Applications",
      "AI Agents",
      "Workflow Automation",
      "Systems Integration",
      "AI Deployment"
    ]
  },
    {
    "name": "MCP Toolbox for Databases",
    "link": "https://github.com/googleapis/genai-toolbox",
    "difficulty": "Unspecified",
    "type": "Tool",
    "description": "El MCP Toolbox for Databases es un servidor MCP (Model Context Protocol) de código abierto diseñado para bases de datos [1-3]. Su objetivo principal es simplificar y acelerar el desarrollo de herramientas de IA, haciéndolas más seguras al manejar complejidades como la gestión de conexiones, la autenticación y más . Originalmente llamado Gen AI Toolbox for Databases, fue renombrado para alinearse con su compatibilidad con MCP . Actualmente se encuentra en fase beta, lo que implica que podría haber cambios significativos hasta su primera versión estable (v1.0) .¿Por qué utilizar Toolbox?   Desarrollo simplificado: Permite integrar herramientas a un agente con menos de 10 líneas de código, reutilizar herramientas entre múltiples agentes o frameworks, y desplegar nuevas versiones de herramientas con mayor facilidad .   Mejor rendimiento: Implementa las mejores prácticas como el connection pooling y la autenticación .   Seguridad mejorada: Ofrece autenticación integrada para un acceso más seguro a los datos .   Observabilidad de extremo a extremo: Proporciona métricas y trazas listas para usar con soporte integrado para OpenTelemetry .Optimización del flujo de trabajo con un asistente de IA para bases de datos :   Consultas en lenguaje natural: Permite interactuar con los datos utilizando lenguaje natural directamente desde el IDE, eliminando la necesidad de escribir SQL para preguntas complejas .   Automatización de la gestión de bases de datos: El asistente de IA puede generar consultas, crear tablas, añadir índices y gestionar la base de datos describiendo simplemente las necesidades de datos .   Generación de código consciente del contexto: Capacita al asistente de IA para generar código de aplicación y pruebas con un conocimiento profundo del esquema de la base de datos en tiempo real, acelerando el ciclo de desarrollo .   Reducción de la sobrecarga de desarrollo: Ayuda a reducir drásticamente el tiempo dedicado a configuraciones de base de datos tediosas, código repetitivo y migraciones de esquema propensas a errores .Arquitectura y funcionamiento: Toolbox se sitúa entre el framework de orquestación de la aplicación y la base de datos, actuando como un control plane para modificar, distribuir o invocar herramientas. Centraliza la gestión de herramientas, permitiendo compartirlas entre agentes y aplicaciones y actualizarlas sin necesidad de redesplegar la aplicación .Instalación: Se puede instalar como un binario , como una imagen de contenedor Docker , usando Homebrew en macOS o Linux , o compilando desde el código fuente con Go .Configuración y herramientas: La configuración principal se realiza a través de un archivo tools.yaml . Este archivo define:   sources: Las fuentes de datos a las que Toolbox tendrá acceso (ej. PostgreSQL) .   tools: Las acciones que un agente puede realizar, incluyendo el tipo de herramienta, la(s) fuente(s) a la que afecta, los parámetros y la sentencia .   toolsets: Grupos de herramientas que pueden cargarse conjuntamente, útiles para definir diferentes conjuntos según el agente o la aplicación .Integración con aplicaciones (SDKs de cliente): Toolbox ofrece SDKs para varios frameworks y lenguajes:   Python: toolbox-core , toolbox-langchain , toolbox-llamaindex .   Javascript/Typescript: @toolbox-sdk/core con integraciones para LangChain/LangGraph  y Genkit .   Go: mcp-toolbox-sdk-go con integraciones para LangChain Go , Genkit , Go GenAI  y OpenAI Go .El proyecto es de código abierto bajo licencia Apache-2.0  y da la bienvenida a las contribuciones, siguiendo un Código de Conducta para Contribuidores . Cuenta con una comunidad activa en Discord . El repositorio de GitHub muestra un notable apoyo comunitario con 8 mil estrellas y 584 forks, y 48 contribuidores . Está desarrollado principalmente en Go (99.4%) .",
    "categories": [
      "MCP (Model Context Protocol)",
      "AI Agents",
      "Large Language Models (LLMs)",
      "Generative AI",
      "Agent Development",
      "Systems Integration",
      "Workflow Automation",
      "AI Deployment",
      "Data Analysis",
      "LLMOps",
      "AI Applications"
    ]
  },
   {
    "name": "Vibe Kanban",
    "link": "https://github.com/microsoft/vibe-kanban",
    "difficulty": "Unspecified",
    "type": "Tool",
    "description": "Vibe Kanban is a Kanban board designed to manage AI coding agents [conversation history]. Its primary purpose is to simplify and optimize the workflow of human engineers who spend most of their time planning, reviewing, and orchestrating tasks for AI coding agents [conversation history].This board allows users to [conversation history]:   Easily switch between different coding agents (such as Claude Code, Gemini CLI, Codex, Amp, and others).   Orchestrate the execution of multiple coding agents, either in parallel or in sequence.   Quickly review work and launch development servers.   Track the status of tasks that coding agents are working on.   Centralize the configuration of Model Context Protocol (MCP) settings for coding agents.Key Features and Usage [conversation history]:   License: The project is open-source under the Apache-2.0 license.   Installation: Requires authentication with the desired coding agent, then running npx vibe-kanban in the terminal.   Documentation and Support: The latest documentation and user guides are available on its website. To report bugs or request features, an issue should be opened in the GitHub repository.   Development: Prerequisites include Rust (latest stable version), Node.js (>=18), and pnpm (>=8). It can be built from the source code.   Environment Variables: Allows configuring build variables (such as GITHUB_CLIENT_ID for GitHub OAuth authentication and PostHog API keys for analytics) and runtime variables (such as server ports and host). It is possible to use a custom GitHub OAuth application for self-hosting or custom branding.   Community and Contributions: The project has considerable community support, with 3.4k stars and 297 forks on GitHub. It has had 8 contributors, and its code is developed primarily in Rust (59.8%) and TypeScript (37.3%). The repository has 48 releases, with the most recent being v0.0.55, dated July 20, 2025.   Related Topics: Includes agent, management, kanban, task-manager, and ai-agents.",
    "categories": [
      "AI Agents" ,
      "Agent Development" ,
      "Multi-Agent Systems" ,
      "MCP (Model Context Protocol)" ,
      "Workflow Automation" ,
      "Systems Integration" ,
      "AI Applications" ,
      "Large Language Models (LLMs)" ,
      "LLMOps" 
    ]
  },
   {
    "name": "Magnific AI",
    "link": "https://magnific.ai",
    "difficulty": "Unspecified",
    "type": "Tool",
    "description": "Magnific AI is a generative Artificial Intelligence tool designed to upscale, enhance, transform, and generate images, offering insanely high-resolution image upscaling that feels like magic [1-3]. It allows users to reimagine as many details as they wish, guided by a text prompt and various parameters or controls [1-3].Key Features and Functionality:   Advanced AI Technology: It is powered by Generative AI to achieve incredibly high-detail image upscaling and enhancement . The technology aims to provide new levels of resolution and detail in AI generations, photos, and illustrations .   Process Control: Users can direct the upscaling process using a text description and sliders such as Creativity, HDR, and Resemblance . The Creativity slider specifically allows control over the level of hallucinations (new details generated by the AI) that users want to add . Most artifacts, if they occur, can be controlled using these parameters .   Versatile Applications: Magnific AI is ideal for enhancing a wide range of image types, including portraits, illustrations, graphic designs, video game assets, landscapes, 3D renders, science fiction, fantasy, horror, films, photography, interior design, and food photoshoots . It has also been used for vintage photo restoration , beautifying Minecraft builds , and in the creation of AI-powered animations .   User-Oriented Design: The tool is designed to be accessible and user-friendly for creators of all backgrounds and skill levels . It features an intuitive interface, in-depth tutorials, and a community to support the users creative journey .   Broad Target Audience: Magnific AI caters to professionals and enthusiasts in photography, graphic design, digital art, and illustration who require high-resolution images and meticulous detail enhancements . It also serves AI artists and creators looking to upscale their AI-generated images for more resolution and depth, as well as businesses and individuals aiming to enhance their visual content .   Developers: It was built by Javi Lopez (@javilopen) and Emilio Nicolas (@emailnicolas), two indie entrepreneurs known for other projects .   Community Reception and Capabilities: The project has received considerable community support and praise, with notable users like Elon Musk , Emad Mostaque , Claire Silver , and Beeple  expressing astonishment at the quality and detail it can add. It has demonstrated the ability to upscale images 8x, from 1.2MP to 77MP (resulting in 64x more pixels) , and even 16x upscaling/enhancement has been reported .   Pricing and Payments: Applicable prices are displayed on their pricing page, potentially in local currency, and VAT/local taxes may be added . An annual subscription offers two months free . Payments are accepted via credit or debit cards (including VISA, MasterCard, American Express, iDeal, SOFORT, and many more) . Financial processing is handled by Stripe, ensuring a 100% secure payment service without retaining card details . Currently, PayPal or cryptocurrency are not accepted .   Cancellation and Refunds: Subscriptions can be canceled at any time through Stripes billing portal . However, refunds are not offered due to the substantial expenses associated with the AI-driven GPU processing time required for image upscaling, as their upstream providers do not offer refunds for this service . Users agree to waive their right to a refund for this reason upon signup .",
    "categories": [
      "Generative AI",
      "AI Applications",
      "Content Generation with AI",
      "Image Analysis with AI",
      "Prompt Engineering"
    ]
  },
   {
    "name": "Agent File (.af)",
    "link": "https://github.com/letta-ai/agent-file",
    "difficulty": "Unspecified",
    "type": "Tool",
    "description": "The Agent File (.af) is an open file format for serializing stateful AI agents with persistent memory and behavior [1-3]. Originally designed for the Letta framework, this standard provides a portable way to share agents with their complete state, enabling easy checkpointing and version control across compatible frameworks [1-3].Key Purposes of Agent File (.af):   Portability: Allows agents to be moved between different systems or deployed to new environments .   Collaboration: Facilitates sharing agents with other developers and the community .   Preservation: Enables archiving agent configurations to preserve your work .   Versioning: Provides a standardized format for tracking changes to agents over time .Included State: An .af file packages all components necessary to re-create the exact same stateful agent . This comprehensive state includes:   System prompts : Initial instructions that define the agents behavior.   Editable memory : Such as personality and user information.   Tool configurations : Complete tool definitions including source code and JSON schema.   LLM settings : Like model name, context window limit, and embedding model name.   Message history : Complete chat history with an in_context field indicating if a message is in the current context window.   Memory blocks : In-context memory segments.   Tool rules : Definitions of how tools should be sequenced or constrained.   Environment variables : Configuration values for tool execution.Current Limitations & Roadmap:   The format currently does not support Passages (the units of Archival Memory in Letta/MemGPT), but support for them is on the roadmap .   Future plans for the .af format include support for MCP servers/configs, archival memory passages, data sources (e.g., files), migration support between schema changes, multi-agent .af files, and converters between different frameworks .Compatibility with other Frameworks: While originally for Letta, other frameworks could theoretically load .af files by converting the state into their own representations . Adding support involves mapping Agent File components to the frameworks equivalent featureset and implementing import/export functionality . Concepts like context window blocks might need adaptation if not natively supported by other frameworks .Secrets Handling: When agents with associated secrets for tool execution are exported, the secrets are automatically set to null within the .af file for security .Usage with Letta:   Users can import and export .af files to and from any Letta Server (self-deployed with Docker, Letta Desktop, or via Letta Cloud) .   This can be done using the visual Agent Development Environment (ADE), or through REST APIs or developer SDKs (available for Python and TypeScript) [7-11].   Example agents are available for download from the repository, including MemGPT, Deep Research, Customer Support, Stateless Workflow, and Composio Tools agents, each with separate instructions for use .Community and Development:   The project is open-source under the Apache-2.0 license .   It has garnered significant community interest, boasting 875 stars and 80 forks on GitHub .   As of the source material, it has 3 contributors .   Community contributions are welcomed and encouraged through various means, such as sharing example agents by opening pull requests, joining discussions on their Discord server, providing feedback via GitHub issues (for suggestions, features, or compatibility challenges), and helping refine the format as it evolves .",
    "categories": [
      "AI Agents",
      "Agent Development",
      "Multi-Agent Systems",
      "Agent Memory",
      "MCP (Model Context Protocol)",
      "Workflow Automation",
      "Systems Integration"
    ]
  },
  {
    "name": "opencode: AI Coding Agent",
    "link": "https://github.com/sst/opencode",
    "difficulty": "Unspecified",
    "type": "Tool",
    "description": "opencode is an AI coding agent built for the terminal [1-3]. It is an open-source project licensed under the MIT license [3-5].Key Features and Philosophy:   Provider-Agnostic: Unlike tools coupled to specific AI providers, opencode is designed to work with various models, including Anthropic (recommended), OpenAI, Google, or even local models. This emphasizes its adaptability as models evolve and pricing changes .   Terminal-Focused User Interface (TUI): Developed by Neovim users and the creators of terminal.shop, opencode aims to push the boundaries of whats possible within the terminal environment .   Client/Server Architecture: This design allows for flexible deployment, such as running the agent on one computer while controlling it remotely, for example, from a mobile application. The TUI frontend is just one potential client .Installation:   Installation can be done via a YOLO curl command (curl -fsSL https://opencode.ai/install | bash), package managers like npm, bun, pnpm, yarn (npm i -g opencode-ai@latest), or homebrew for macOS (brew install sst/tap/opencode), and paru for Arch Linux (paru -S opencode-bin) .   Its advised to remove versions older than 0.1.x before installing .   The install script respects a priority order for the installation path: $OPENCODE_INSTALL_DIR, $XDG_BIN_DIR, $HOME/bin, and finally $HOME/.opencode/bin as a default fallback .Contributing:   While opencode is an opinionated tool and does not accept Pull Requests (PRs) for core features, contributions are welcomed for bug fixes, improvements to LLM performance, support for new providers, fixes for environment-specific quirks, missing standard behavior, and documentation .   To run opencode locally, Bun and Golang 1.24.x are required .Community and Development:   The project has garnered significant community interest with 15k stars and 918 forks on GitHub .   It lists 110 contributors .   The primary languages used in its development are TypeScript (46.4%) and Go (46.2%) .   Users can join the community via Discord, YouTube, or X.com .",
    "categories": [
      "AI Agents",
      "Agent Development",
      "Software Development",
      "Workflow Automation",
      "Systems Integration"
    ]
  },
  {
    "name": "Gemini for Google Workspace: Prompting Guide",
    "link": "https://www.gstatic.com/bricks/pdf/5ed3a163-8d70-4ece-96f0-0fe228765889/Gemini-for-google-workspace-prompting-guide_ES.pdf",
    "difficulty": "Beginner",
    "type": "Book",
    "description": "This guide, titled Introducción a la creación de instrucciones para Gemini for Google Workspace (Introduction to Prompt Creation for Gemini for Google Workspace) , is an essential resource for learning to write effective prompts to maximize the benefits of Generative AI within Google Workspace applications . It aims to provide users with basic skills to enhance productivity and efficiency in daily tasks .Key Concepts of Prompting:   Prompt as a Question: A prompt is considered a question to initiate a conversation with an AI-powered assistant . The process often involves writing multiple prompts as the conversation progresses .   Four Main Areas for Effective Prompts: The guide highlights four crucial elements to consider: Persona, Task, Context, and Format . While not all four are always necessary, using some of them is beneficial . An example prompt utilizing all four areas is provided for Gmail and Google Docs .Tips for Effective Prompt Writing:   Use Natural Language: Write as if speaking to another person, using complete sentences .   Be Specific and Iterate: Clearly state what Gemini for Workspace needs to do (e.g., summarize, write, change tone, create content) and provide as much context as possible. Adjust and refine prompts iteratively if results are not satisfactory [13-15].   Be Brief and Avoid Complexity: Frame requests concisely and specifically, avoiding jargon. Successful prompts average 21 words .   Make it a Conversation: Engage in a back-and-forth dialogue with the AI to refine outputs .   Break Down Tasks: For multiple related tasks, divide them into separate prompts .   Set Limits: Include details like character counts or the number of options desired to generate specific results .   Assign a Role: Encourage creativity by assigning a persona to Gemini (e.g., You are a Program Manager in Google Cloud.) .   Ask for Feedback: In a conversation with Gemini, describe the project and desired outcome, then ask questions like What questions do you have for me that could help you provide the best result? .   Consider the Tone: Adapt prompts to the target audience and desired tone (formal, informal, technical, creative, casual) .Integration and Capabilities with Google Workspace:   Geminis Generative AI features are seamlessly integrated into everyday applications such as Gmail, Google Docs, Google Sheets, and Google Slides, and Google Meet . Users can also chat with Gemini at gemini.google.com .   It can assist with improving writing, organizing data, creating original images, summarizing information, displaying statistics, fostering connections, researching unfamiliar topics, detecting trends, identifying business opportunities, and synthesizing information .   Specific features include Help me write (Docs, Gmail), Help me organize (Sheets), Create image with Gemini (Slides), and Create background images (Meet) .Important Considerations:   Generative AI is still evolving, and responses can be unpredictable .   Always review AI-generated output for clarity, relevance, and accuracy before applying it . The final result remains the users responsibility .Target Audience and Use Cases:   The guide includes practical situations and example prompts for various roles, including Customer Service, Executives and Entrepreneurs (e.g., CEO, COO, CMO, CTO, CIO), Human Resources (e.g., HR Manager, Recruiter), Marketing (e.g., Brand Manager, Digital Marketing Manager, Content Marketing Manager), Project Management, and Sales (e.g., Account Executive, Sales Director, Account Manager, Business Development Manager) .",
    "categories": [
      "Prompt Engineering",
      "Generative AI",
      "Large Language Models (LLMs)",
      "AI Assistants",
      "AI Applications",
      "Workflow Automation",
      "Introduction to AI and Fundamental Concepts"
    ]
  },
  {
    "name": "AI Business Consultant Agent with Gemini 2.5 Flash",
    "link": "https://www.theunwindai.com/p/build-an-ai-consultant-agent-with-gemini-2-5-flash",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "The AI Business Consultant Agent is a fully functional, open-source agentic application built for business consultation . This powerful AI business consultant is developed using Googles Agent Development Kit (ADK) and leverages the Gemini 2.5 Flash model for its AI capabilities . It is combined with Perplexity AI for real-time web research to provide current market data, trends, and competitor intelligence . The application is designed to offer real-time insights, data-driven strategies, and rapid responses to market changes for various business needs .Key Features of the AI Business Consultant Agent include :   Real-time Web Research: Gathers current market data and trends using Perplexity AI search.   Market Analysis: Processes research data to generate structured insights with confidence scores.   Strategic Recommendations: Creates actionable business advice, complete with timelines and implementation plans.   Risk Assessment: Identifies potential risks and provides mitigation strategies.   Interactive UI: Features a clean web interface powered by Google ADK for easy user consultation.   Evaluation System: Includes built-in evaluation and debugging capabilities with session tracking.How it Works: Users initiate a consultation by submitting business questions through the Google ADK web interface . The agent then conducts real-time web research using Perplexity AI , followed by market analysis to process the data and generate structured insights . Strategic recommendation tools are used to create actionable business advice . Finally, the agent synthesizes its web research findings and analysis results into a comprehensive consultation report, providing users with professional consultation that includes citations, action items, and measurable success metrics .Technical Details and Setup: To build this agent, users are recommended to have Python (version 3.10 or higher) installed, along with their Google Gemini and Perplexity API keys . Basic familiarity with Python programming is also a prerequisite . The code walkthrough involves cloning the awesome-llm-apps GitHub repository, navigating to the ai_consultant_agent folder, installing required dependencies using pip install -r requirements.txt, and setting up API keys as environment variables . The main implementation resides in ai_consultant_agent.py, while agent.py and __init__.py are essential files for Google ADKs web interface discovery system . The agents behavior is defined by its instructions, which configure it as a \"senior AI business consultant specializing in market analysis and strategic planning\" . Once set up, the app can be launched via the Google ADK web interface, typically accessed at http://localhost:8000 .Community and Expandability: This project is part of the broader awesome-llm-apps repository, which has garnered significant community support with 45.7k stars and 5.2k forks on GitHub . Future expansion possibilities for the consultant agent include industry specialization, integration with business intelligence tools or CRM systems, implementation of advanced evaluation metrics, and the addition of collaboration features for team strategic planning sessions . The project is open-source and encourages community contributions and sharing on social channels .",
    "categories": [
      "AI Agents",
      "Agent Development",
      "Large Language Models (LLMs)",
      "AI Applications",
      "Workflow Automation",
      "Systems Integration",
      "Prompt Engineering",
      "Data Analysis",
      "Web Scraping with AI"
    ]
  },
    {
    "name": "RunAgent",
    "link": "https://github.com/runagent-dev/runagent",
    "difficulty": "Unspecified",
    "type": "Tool",
    "description": "RunAgent is an AI Agent Deployment Platform designed to simplify the serverless deployment of AI agents . It provides a powerful Command Line Interface (CLI), multi-language SDK support, and built-in agent invocation and streaming support . The platform aims to allow developers to focus on agent development rather than repetitive processes like implementing REST and streaming APIs .Core Purpose and Philosophy:   Universal AI Agent Platform: RunAgent allows you to write AI agents once in Python and access them natively from any language .   Simplified Deployment: It simplifies the serverless deployment of AI agents, whether locally or on its upcoming cloud platform .   Developer Focus: The platform is built for developers who are using AI agents to empower or enhance their products, freeing them from concerns about deployment complexities .Key Features:   Powerful CLI: Deploy AI agents using the RunAgent CLI with a simple configuration file . Commands include init, serve, deploy, run, setup, template, upload, start, and teardown .   Multi-language SDKs: Provides native-feeling SDKs for Python, JavaScript/TypeScript, Rust, and Go with full type safety, intellisense, automatic error handling, and seamless authentication . This eliminates the need to wrestle with REST APIs directly .   Native Streaming Support: One of its most powerful features is native streaming support across all languages, allowing real-time agent responses that feel natural in each programming language, using iterators in Python, async iterators in JavaScript, futures streams in Rust, and context-aware iterators in Go .   Local Development Environment: Offers a powerful local development environment with a built-in FastAPI server that includes hot reload, logging, monitoring, and debugging tools .   Framework Agnostic: RunAgent works with any Python AI framework, including LangChain, LangGraph, CrewAI, Letta, Agno, or custom frameworks, providing pre-built templates and specific guides .   Smart Architecture: Supports local development now and provides a clear path to production-scale serverless cloud deployment in the future .   Production Ready (Self-Hosting): The current open-source foundation is suitable for self-hosting and enterprise deployments .   Agent Lifecycle Management: Features smart project initialization, environment management, and one-command deployment .   Webhooks: Supports webhook integrations for event-driven architectures .How it Works (Quick Start):1.  Installation: Install the RunAgent CLI and Python SDK using pip install runagent .2.  Initialize from a Template: Create a new agent project using runagent init <project_name> with options for specific frameworks like --langgraph or --crewai .3.  Configure Your Agent: The crucial runagent.config.json file defines entrypoints, which are the functions from your codebase that will be exposed through the RunAgent server and accessible via SDKs . For streaming entrypoints, the tag should include a _stream suffix to indicate streaming capability . Example Python functions mock_response and mock_response_stream are shown as entrypoints .4.  Serve Your Agent Locally: Start a local development server using runagent serve . (if in the project directory) or runagent serve <project_dir/name> . This provides an agent_id, host:port URL, WebSocket endpoints, and a development dashboard for monitoring .5.  Use a Deployed Agent: Connect to your agent using the RunAgentClient from any RunAgent SDK, specifying the agent_id or host:port and the entrypoint_tag . You can then invoke the run method like a native function . For streaming functions, you can iterate over the returned object naturally.Target Audience & Use Cases:RunAgent is perfect for polyglot teams where Python AI experts collaborate with frontend (JS/TS) and backend (Rust/Go) developers . It is also suitable for microservices architecture, enabling centralized AI logic with distributed access, legacy integration for adding AI capabilities without rewrites, performance-critical apps by consuming Python AI logic from high-performance languages, and rapid prototyping with a seamless path to production .Roadmap and Future Plans:   Foundation (Available Now): The current open-source foundation provides cross-language SDKs, a local development server, framework-agnostic support, pre-built templates, a comprehensive CLI, real-time debugging, and is suitable for self-hosting and enterprise deployments .   Cloud Platform (Coming Q2 2025): This serverless cloud platform will offer one-command deployment, auto-scaling from zero to thousands of requests, a global edge network for minimal latency, built-in monitoring, webhook integrations, team collaboration features, usage analytics, and an API Gateway .   Enterprise Features (Coming 2025): An enterprise tier will provide private cloud deployment (AWS, GCP, Azure), advanced security (SOC2 Type II compliance, encryption), compliance features (GDPR, HIPAA), role-based access control, custom SLAs (99.9% uptime), custom runtime environments, enterprise integrations (SSO, LDAP), and advanced analytics .Community and Contribution:RunAgent is hosted on GitHub as runagent-dev/runagent . It has 247 stars and 32 forks , with 4 contributors . Its primary languages are Python (53.1%), Rust (30.5%), Go (9.7%), and TypeScript (5.4%) . RunAgent welcomes community contributions, including bug reports, feature requests, code contributions, documentation improvements, community support, and SDK development . Users can join the community via Discord, GitHub Discussions, or Twitter .License:RunAgent is licensed under the Elastic License 2.0 (ELv2) . This license allows free use for development, testing, and production, as well as modification and distribution . However, it cannot be offered as a managed service without permission .",
    "categories": [
      "AI Agents",
      "Agent Development",
      "AI Deployment",
      "LLMOps",
      "Large Language Models (LLMs)",
      "Systems Integration",
      "Workflow Automation"
    ]
  },
  {
    "name": "any-llm",
    "link": "https://github.com/mozilla-ai/any-llm",
    "difficulty": "Unspecified",
    "type": "Tool",
    "description": "any-llm is a single interface designed to communicate with different Large Language Model (LLM) providers [1-3]. Developed by mozilla-ai, it aims to address the fragmented ecosystem of LLM provider interfaces where variations exist despite the OpenAI API becoming a de facto standard . The project provides a light wrapper to handle these differences consistently .Key Features offered by any-llm include :   Simple, unified interface: Allows switching models with just a string change using a single function for all providers.   Developer friendly: Provides full type hints for better IDE support and clear, actionable error messages.   Leverages official provider SDKs: Reduces maintenance burden and ensures compatibility by utilizing official SDKs when available.   Framework-agnostic: Can be used across various projects and use cases.   Actively maintained: The tool is used in Mozilla AIs own product, any-agent, ensuring continued support.   No Proxy or Gateway server required: Eliminates the need for setting up additional services to interact with LLM providers.The motivation behind any-llm is to overcome limitations of existing solutions, such as LiteLLM (which reimplements provider interfaces potentially leading to compatibility issues), AISuite (lacks active maintenance and modern typing), framework-specific solutions (causing fragmentation), and proxy-only solutions like OpenRouter and Portkey (which require a hosted proxy) .To get started, users need Python 3.11 or newer and API keys for their chosen LLM provider . Installation is done via pip, allowing users to include specific providers or install support for all using any-llm-sdk[all] . API keys can be set as environment variables (e.g., MISTRAL_API_KEY) or passed directly as an api_key parameter in the completion call . Basic usage involves importing the completion function from any_llm and specifying the model parameter as <provider_id>/<model_id> (e.g., mistral/mistral-small-latest), along with messages .any-llm is licensed under the Apache-2.0 license . The GitHub repository, mozilla-ai/any-llm, has garnered 558 stars and 31 forks, with 99.7% of its codebase in Python and 0.3% in Shell . Its topics include inference, text-completion, and LLMs .",
    "categories": [
      "Large Language Models (LLMs)",
      "Systems Integration",
      "AI Agents",
      "Agent Development",
      "AI Deployment",
      "LLMOps"
    ]
  },
   {
    "name": "Talkpal AI Language Teacher",
    "link": "https://talkpal.ai/",
    "difficulty": "Unspecified",
    "type": "Tool",
    "description": "A GPT-powered AI language teacher that helps users boost their speaking, listening, writing, and pronunciation skills, allowing them to learn 5x faster. It offers immersive conversations, real-time personalized feedback, and personalization tailored to the user's style and pace. The tool supports 57+ languages and uses advanced AI to imitate real-life tutoring scenarios.",
    "categories": [
      "Generative AI",
      "Large Language Models (LLMs)",
      "AI Applications",
      "Content Generation with AI",
      "AI in Education",
      "AI Assistants"
    ]
  },
  {
    "name": "Cleanup.pictures",
    "link": "https://cleanup.pictures/",
    "difficulty": "Unspecified",
    "type": "Tool",
    "description": "An advanced editing tool based on Artificial Intelligence for 'inpainting' (object removal). It allows photographers and creative agents to eliminate unwanted people, objects, text, logos, or watermarks from any photo in seconds with incredible quality. The AI is capable of reconstructing what was behind the unwanted element, unlike traditional clone tools. The free version is limited to 720px export resolution.",
    "categories": [
      "Generative AI",
      "AI Applications",
      "Image Analysis with AI"
    ]
  },
  {
    "name": "Durable AI Website Builder",
    "link": "https://durable.co/",
    "difficulty": "Unspecified",
    "type": "Tool",
    "description": "An AI business partner and the #1 AI website builder for creating and growing a business. It instantly generates professionally designed, customizable, and search-optimized (SEO) websites. The platform includes AI-powered features such as a copywriter for high-converting content, an AI blog that writes and publishes new content weekly, and integrated tools for contact management/CRM and simple invoicing.",
    "categories": [
      "AI Applications",
      "Content Generation with AI",
      "Workflow Automation",
      "Business Automation",
      "AI for Productivity"
    ]
  },
  {
    "name": "Playground AI Design Tool",
    "link": "https://playground.com/",
    "difficulty": "Unspecified",
    "type": "Tool",
    "description": "A free AI design tool for creating content such as logos, T-shirts, social media graphics (implied by title), art, stickers, wallpapers, and a variety of templates including posters and e-book covers.",
    "categories": [
      "Generative AI",
      "AI Applications",
      "Content Generation with AI"
    ]
  },
  {
    "name": "Gamma AI Presentation Maker & Website Builder",
    "link": "https://gamma.app/",
    "difficulty": "Unspecified",
    "type": "Tool",
    "description": "An AI design partner for effortless creation of presentations, websites (no coding required), social media posts, and documents (like proposals and PDFs). It allows users to generate content from an idea or outline, edit with AI to instantly add smart layouts and AI-generated images, and export content to PPT, PDF, or Google Slides.",
    "categories": [
      "Generative AI",
      "AI Applications",
      "Content Generation with AI",
      "Workflow Automation",
      "AI for Productivity"
    ]
  },
  {
    "name": "Looka Logo Design & Brand Identity",
    "link": "https://looka.com/",
    "difficulty": "Unspecified",
    "type": "Tool",
    "description": "An AI-powered platform for designing logos and building a complete brand identity. It uses AI software to instantly generate hundreds of custom logo mockups without requiring design skills. The Brand Kit uses the logo's design, colors, and fonts to instantly create over 300 templates for marketing materials (such as business cards and email signatures).",
    "categories": [
      "Generative AI",
      "AI Applications",
      "Content Generation with AI",
      "AI for Productivity"
    ]
  },
  {
    "name": "SOUNDRAW AI Music Generator",
    "link": "https://soundraw.io/",
    "difficulty": "Unspecified",
    "type": "Tool",
    "description": "The fastest AI beat-maker that creates and customizes royalty-free music in seconds. Users can edit every instrument, adjust intensity, set the perfect length using the Mixer, and even blend genres (e.g., Hip-Hop + Orchestra). The AI is trained exclusively on original SOUNDRAW music, ensuring users receive a perpetual, worldwide commercial license and keep 100% of the royalties. High-quality WAV and separate STEMS files are available for download.",
    "categories": [
      "Generative AI",
      "AI Applications",
      "Content Generation with AI"
    ]
  },
  {
    "name": "Stockimg AI",
    "link": "https://stockimg.ai/",
    "difficulty": "Unspecified",
    "type": "Tool",
    "description": "An all-in-one design and content creation tool powered by AI. It can generate logos, illustrations, stock photos, and visual content. It also functions as a social media manager, capable of generating, editing, and scheduling posts endlessly. It can generate viral videos (Reels/Tiktok) complete with audio and subtitles in just 30 seconds, supporting integration with over 10 social media platforms.",
    "categories": [
      "Generative AI",
      "AI Applications",
      "Content Generation with AI",
      "Workflow Automation",
      "AI for Productivity"
    ]
  },
  {
  "name": "Futurepedia",
  "link": "https://www.futurepedia.io/",
  "difficulty": "Unspecified",
  "type": "Tool",
  "description": "Futurepedia is a leading AI resource platform, dedicated to empowering professionals across various industries to leverage AI technologies for innovation and growth . The platform provides access to an array of cutting-edge solutions that simplify AI integration into professional practices . It offers comprehensive directories (categorizing 2525 AI tools into 10 categories) , easy-to-follow guides, a weekly newsletter, and an informative YouTube channel . Committed to making AI understandable and practical, it caters to diverse professional needs .",
  "categories": [
    "Introduction to AI and Fundamental Concepts",
    "AI Applications",
    "AI for Productivity",
    "Workflow Automation"
  ]
},
{
  "name": "DEEP LEARNING",
  "link": "https://www.uhu.es/publicaciones/?q=libros&code=1252",
  "difficulty": "Unspecified",
  "type": "Book",
  "description": "Authored by Isaac Pérez and Manuel Emilio Gegúndez, this book offers an updated introduction to deep learning from a different perspective . It focuses on the fundamentals and basic concepts necessary to understand the functioning of these models, independent of the programming language and libraries used . While deep learning has brought a revolution to Artificial Intelligence, this resource aims to fill the gap left by most existing resources that primarily focus on practical application . It provides historical context, addresses the main deep learning models (including convolutional and recurrent neural networks), and outlines the guidelines and techniques for their training . The book is available for free download as an interactive PDF or ePUB .",
  "categories": [
    "Introduction to AI and Fundamental Concepts",
    "Deep Learning",
    "Machine Learning"
  ]
},
{
  "name": "Learning opencv eBook (PDF)",
  "link": "http://riptutorial.com/ebook/opencv",
  "difficulty": "Unspecified",
  "type": "Book",
  "description": "This free eBook provides a comprehensive tutorial for learning OpenCV, covering 20 chapters of essential concepts and practical implementations. Topics include getting started with OpenCV, understanding Basic Structures, Image Processing, Edge Detection, Blob Detection, and Object Detection using Cascade Classifiers . It offers guides for specific environments and languages, such as building and compiling for Python on Windows, Drawing Functions in Java and C++, Image Content Modification, and using `VideoCapture` with OpenCV Python .",
  "categories": [
    "Image Analysis with AI",
    "AI Applications",
    "Introduction to AI and Fundamental Concepts"
  ]
},
{
  "name": "Grok CLI",
  "link": "https://github.com/superagent-ai/grok-cli",
  "difficulty": "Unspecified",
  "type": "Tool",
  "description": "Grok CLI is an open-source AI agent that brings the power of Grok directly into your terminal . It functions as a conversational AI CLI tool powered by Grok-3 . Key features include a Natural Language Interface ; Smart File Operations that allow the AI to automatically view, create, and edit files ; and Bash Integration to execute shell commands through natural conversation . The tool uses Automatic Tool Selection  and supports extending capabilities via MCP Tools (Model Context Protocol servers) . It is designed with an interactive terminal UI built with Ink  and supports Headless Mode for scripting and CI/CD pipelines . Furthermore, it uses OpenAI-compatible APIs, allowing configuration with providers like X.AI (Grok), OpenAI, OpenRouter, and Groq . Optional features include Morph Fast Apply for high-speed code editing at 4,500+ tokens/sec , which becomes available when the Morph API key is configured .",
  "categories": [
    "AI Agents",
    "Large Language Models (LLMs)",
    "MCP (Model Context Protocol)",
    "Workflow Automation",
    "Systems Integration",
    "AI Applications",
    "Agent Development"
  ]
},
{
  "name": "Eigent",
  "link": "https://github.com/eigent-ai/eigent",
  "difficulty": "Unspecified",
  "type": "Tool",
  "description": "Eigent is the World's First Multi-agent Workforce desktop application, designed to unlock exceptional productivity by allowing users to build, manage, and deploy a custom AI workforce that automates complex workflows . Built on CAMEL-AI's open-source project, the system enhances productivity through parallel execution, customization, and privacy protection . Key features include being 100% Open Source and supporting Local Deployment (Community Edition) . It features advanced Multi-Agent Coordination to solve complex tasks by activating specialized agents to work in parallel . Pre-defined agents include a Developer Agent (writes/executes code), a Search Agent (searches the web), a Document Agent (creates/manages documents), and a Multi-Modal Agent (processes images/audio) . Eigent also offers MCP (Model Context Protocol) Tools Integration with massive built-in tools (e.g., Notion, Google suite, Slack) and supports custom tools . If a task encounters uncertainty, the system automatically requests Human-in-the-Loop input . It supports complex use cases like generating detailed travel itineraries, automating financial reports from CSV data, and performing market research .",
  "categories": [
    "AI Agents",
    "Agent Development",
    "Multi-Agent Systems",
    "MCP (Model Context Protocol)",
    "Workflow Automation",
    "Systems Integration",
    "AI Applications",
    "AI for Productivity"
  ]
},
 {
    "name": "Unsloth AI",
    "link": "https://github.com/unslothai/unsloth",
    "difficulty": "Unspecified",
    "type": "Tool",
    "description": "Unsloth is a highly efficient library for Fine-tuning & Reinforcement Learning (RL) for LLMs . It uses optimized kernels written in Triton and manual backpropagation to accelerate training by up to 2x faster and reduce VRAM usage by 70% or more [2, 4-6]. It guarantees 0% loss in accuracy . Unsloth supports a wide array of models including OpenAI gpt-oss, DeepSeek-R1, Qwen3, Gemma 3, and various Llama models . It enables significantly longer context windows (e.g., Llama 3.1 (8B) up to 342K context on 80GB VRAM) . The library supports full fine-tuning, pretraining, LoRA, and various quantization methods (4-bit, 8-bit, 16-bit) . It is the most efficient library for RL methods like GRPO, GSPO, DrGRPO, DAPO, and DPO . Unsloth works on Linux, WSL, and Windows, and integrates seamlessly with Hugging Face's `transformers` and `TRL` . It can export trained models to formats like GGUF, Ollama, vLLM, or Hugging Face .",
    "categories": [
      "Deep Learning",
      "Large Language Models (LLMs)",
      "Machine Learning",
      "Generative AI",
      "AI Applications",
      "Agent Development",
      "Multimodal AI",
      "LLMOps",
      "AI Deployment"
    ]
  },
  {
    "name": "HuggingFace TRL",
    "link": "https://github.com/huggingface/trl",
    "difficulty": "Unspecified",
    "type": "Tool",
    "description": "TRL (Transformer Reinforcement Learning) is a comprehensive library for post-training foundation models using advanced techniques . Built on the 🤗 Transformers ecosystem , TRL provides dedicated trainers like SFTTrainer (Supervised Fine-Tuning) , DPOTrainer (Direct Preference Optimization, used for Llama 3) , and GRPOTrainer . The library is efficient and scalable, leveraging 🤗 Accelerate for multi-node clusters (DDP and DeepSpeed) and full integration with 🤗 PEFT for LoRA/QLoRA training on modest hardware . TRL accelerates training by integrating optimized kernels from Unsloth . It features a Command Line Interface (CLI) that simplifies fine-tuning without requiring users to write code .",
    "categories": [
      "Deep Learning",
      "Large Language Models (LLMs)",
      "Machine Learning",
      "Generative AI",
      "AI Applications",
      "Agent Development"
    ]
  },
  {
    "name": "Axolotl",
    "link": "https://github.com/axolotl-ai-cloud/axolotl",
    "difficulty": "Unspecified",
    "type": "Tool",
    "description": "Axolotl is a free and open-source LLM Fine-tuning Framework designed to streamline post-training and fine-tuning for the latest large language models . It supports training diverse models (LLaMA, Mistral, Mixtral, Pythia, GPT-OSS, Gemma) . Key capabilities include Multimodal Training for vision-language models (VLMs) and audio models . It integrates numerous training methods, including Full fine-tuning, LoRA, QLoRA, GPTQ, QAT, Preference Tuning (DPO, IPO, KTO, ORPO), and RL (GRPO) . Axolotl simplifies the pipeline through easy YAML configuration  and includes performance optimizations such as Flash Attention, DeepSpeed, FSDP, and Sequence Parallelism (SP) . The framework is Cloud Ready, shipping PyPI packages and Docker images for use on platforms like RunPod and Vast.ai .",
    "categories": [
      "Deep Learning",
      "Large Language Models (LLMs)",
      "Machine Learning",
      "Generative AI",
      "AI Applications",
      "Model Management",
      "LLMOps",
      "Multimodal AI",
      "Agent Development"
    ]
  },
  {
    "name": "LLaMA-Factory",
    "link": "https://github.com/hiyouga/LLaMA-Factory",
    "difficulty": "Unspecified",
    "type": "Tool",
    "description": "LLaMA-Factory is a powerful open-source tool for the unified efficient fine-tuning of over 100 Large Language Models (LLMs) and Visual Language Models (VLMs) . It offers both a zero-code Command Line Interface (CLI) and a user-friendly Web UI (LLaMA Board GUI) . It supports extensive models and integrates various training methods, including pre-training, (multimodal) supervised fine-tuning, reward modeling, PPO, DPO, KTO, and ORPO . For efficiency, it utilizes 16-bit full-tuning, freeze-tuning, LoRA, and 2/3/4/5/6/8-bit QLoRA via multiple quantization techniques . The platform incorporates advanced algorithms (e.g., GaLore, BAdam, DoRA, PiSSA) and practical tricks like FlashAttention-2 and Unsloth for optimized performance . LLaMA-Factory handles diverse tasks, including multi-turn dialogue, tool using, and multi-modal recognition . It is used by major companies like Amazon, NVIDIA, and Aliyun  and provides faster inference via an OpenAI-style API and vLLM/SGLang workers .",
    "categories": [
      "Large Language Models (LLMs)",
      "Machine Learning",
      "Deep Learning",
      "AI Deployment",
      "AI Applications",
      "Generative AI",
      "LLMOps",
      "Prompt Engineering",
      "AI Agents",
      "Model Management",
      "AI Automation",
      "Multimodal AI"
    ]
  },
  {
  "name": "Opal: Google's Visual AI App Builder",
  "link": "https://opal.withgoogle.com/landing/",
  "difficulty": "Unspecified",
  "type": "Tool",
  "description": "Opal is Google's free visual AI app builder and workflow automation tool, inspired by platforms like n8n, but simplified for non-developers. Users interact with the tool using natural language (e.g., \"Create an app that turns meeting notes into action items\"), and Opal automatically builds the visual workflow . The platform connects Google's AI models, search, and various tools automatically. Users can edit the generated workflow using a drag-and-drop builder, where the logic is displayed as connected blocks . This allows users to chain prompts, AI models, and tools together to create custom mini-AI apps . Creations can be shared with anyone who has a Google account. The visual app builder supports interconnected steps for tasks like obtaining user information, asset creation, and outputting content (e.g., generating a blog post) . Access requires signing in to view or make mini-apps .",
  "categories": [
    "Workflow Automation",
    "Generative AI",
    "AI Applications",
    "AI for Productivity",
    "Systems Integration",
    "Prompt Engineering",
    "AI Agents",
    "Business Automation",
    "n8n"
  ]
},
{
  "name": "LLM Engineer Toolkit",
  "link": "https://github.com/KalyanKS-NLP/llm-engineer-toolkit",
  "difficulty": "Unspecified",
  "type": "Tool",
  "description": "This repository contains the LLM Engineer Toolkit, a comprehensive, curated list of 120+ LLM libraries categorized by function . It serves as a centralized resource for staying updated with Generative AI, LLMs, Agents, and RAG technologies . The toolkit covers nearly every aspect of the LLM development lifecycle, including extensive categories such as: LLM Training and Fine-Tuning ; LLM Application Development (Frameworks, Multi-API Access, Memory, Low Code, Cache) ; Retrieval-Augmented Generation (RAG) ; LLM Inference and Serving ; LLM Data Extraction and Generation ; LLM Agents ; LLM Evaluation and Monitoring ; LLM Prompts and Structured Outputs ; and LLM Safety and Security . The project has substantial community support, evidenced by its 7.2k stars and 1.1k forks .",
  "categories": [
    "Large Language Models (LLMs)",
    "Generative AI",
    "AI Agents",
    "RAG (Retrieval-Augmented Generation)",
    "Agent Development",
    "LLMOps",
    "Prompt Engineering",
    "AI Applications",
    "Machine Learning",
    "Deep Learning",
    "AI Ethics"
  ]
},
{
  "name": "Qwen3-Coder-30B-A3B-Instruct",
  "link": "https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct",
  "difficulty": "Unspecified",
  "type": "Tool",
  "description": "The Qwen3-Coder-30B-A3B-Instruct is a streamlined Causal Language Model designed for advanced coding tasks . This model, which has 30.5B parameters total (3.3B activated) and is built with a Mixture of Experts (MoE) architecture (128 experts, 8 activated) , provides significant performance among open models on Agentic Coding and Agentic Browser-Use . It supports long-context capabilities with native support for 256K tokens (262,144 natively), which can be extended up to 1M tokens using Yarn, making it optimized for repository-scale understanding . The model excels in tool calling capabilities  and supports agentic coding platforms like Qwen Code and CLINE using a specific function call format . It can be integrated using the `transformers` library  and is compatible with applications like Ollama, LMStudio, and llama.cpp for local use . The model supports non-thinking mode and does not generate `<think></think>` blocks in its output .",
  "categories": [
    "Large Language Models (LLMs)",
    "Deep Learning",
    "Generative AI",
    "AI Agents",
    "Agent Development",
    "AI Applications",
    "Multimodal AI"
  ]
},
{
  "name": "Academic Prompt Generator (Gemini-Powered Research Assistant)",
  "link": "https://gemini.google.com/share/4af4639b6049",
  "difficulty": "Unspecified",
  "type": "Tool",
  "description": "An AI-powered personal research assistant created with the help of Gemini to streamline the often-frustrating literature review process. This tool, developed under a Creative Commons license, acts as an Academic Prompt Generator. Its functionalities include: Refining Your Topic (AI helps reformulate ideas and discover key multilingual concepts); Precise Targeting (allowing selection from major open-access databases like Scielo and CORE); Full Control (filtering by document type, dates, and output format, including BibTeX for direct import into citation managers like Zotero or Mendeley); and Generation (obtaining a customized JSON prompt optimized for deep searches). The project is included in the Educational Applications Repository, a collection of resources from the Vibe Coding Educativo community.",
  "categories": [
    "Prompt Engineering",
    "Prompt Optimization",
    "AI Assistants",
    "AI in Education",
    "AI Applications",
    "Generative AI",
    "Large Language Models (LLMs)",
    "Workflow Automation"
  ]
},
{
  "name": "MIT FUTURE OF AI: Foundation Models & Generative AI (6.S087)",
  "link": "https://www.futureofai.mit.edu/",
  "difficulty": "Unspecified",
  "type": "Course",
  "description": "This is a non-technical series of lectures (6.S087) from MIT, created by MIT researcher Rickard Brüel Gabrielsson, specializing in foundation models and generative AI . The course focuses on Foundation Models and Generative AI, which are driving recent major breakthroughs like ChatGPT, Copilot, CLIP, Dall-E, Stable-Diffusion, and AlphaFold . The lectures address the 'secret sauce' behind these technologies and discuss the possibility that Artificial General Intelligence (AGI) has already been achieved . The curriculum is accessible to all backgrounds  and covers applications in both science and business . Topics include: a short history of AI , the limitations of supervised learning and reinforcement learning , and the foundational implications of foundation models achieved via self-supervised learning . Specific concepts covered include: ChatGPT & LLMs , Image Generation , Autonomy , Biology , Neural Networks , Generative AI , GANs (adversarial) , and Ethics (Lecture 8) . Lectures are scheduled to start on Tuesday, January 9 .",
  "categories": [
    "Generative AI",
    "Large Language Models (LLMs)",
    "Deep Learning",
    "Machine Learning",
    "Introduction to AI and Fundamental Concepts",
    "AI Applications",
    "Multimodal AI",
    "AI Ethics"
  ]
},
{
  "name": "MIT Efficient ML Course (YouTube)",
  "link": "https://www.youtube.com/playlist?list=PL80kAHvQbh-pT4lCkDT53zT8DKmhE0idB",
  "difficulty": "Unspecified",
  "type": "Course",
  "description": "This MIT course focuses on Efficient Machine Learning (Efficient ML), described as the 'most underrated skill in AI today'. The training addresses the challenge that large models (LLMs, diffusion models, etc.) are often compute-hungry, memory-intensive, and difficult to deploy on real-world devices. It is considered a must for anyone serious about scalable, accessible AI. The curriculum teaches core techniques to make AI fast, light, and deployable [Source: Query]. Specific techniques covered include: Compression & Pruning, Quantization, Neural Architecture Search, Distributed & Parallel Training, On-device Fine-tuning, Gradient Compression, Acceleration for LLMs, diffusion, video, point cloud, and Quantum ML [Source: Query]. The course includes a hands-on project to Deploy LLaMA 2 on your laptop [Source: Query]. The underlying philosophy is that 'the future isn’t just about building bigger models — it’s about making them run everywhere' [Source: Query].",
  "categories": [
    "Deep Learning",
    "Large Language Models (LLMs)",
    "LLMOps",
    "AI Deployment",
    "Machine Learning",
    "Generative AI",
    "AI Applications",
    "Multimodal AI"
  ]
},
{
  "name": "Modern Deep Learning Foundations",
  "link": "https://www.researchgate.net/publication/395663230_Modern_Deep_Learning_Foundations",
  "difficulty": "Unspecified",
  "type": "Book",
  "description": "Authored by Dr. Barak Or and released in September 2025 , this book serves as the official course notes for the ArtificialGate program . It offers a systematic, hands-on introduction to the principles, architectures, and practices of contemporary deep learning . It is designed for engineers and applied researchers familiar with Python and basic mathematics, but without required prior machine learning background, guiding them from first principles through industrial deployment . The text emphasizes conceptual clarity, practical proficiency, reproducibility, explainability, and system-level thinking . Topics covered include: the structure and training of Neural Networks (NN) , Optimization (Gradient Descent, AdamW, Lion, Adafactor) , Regularization (Dropout, L2, Early Stopping) , Convolutional (CNN) and Recurrent (RNN, GRU, LSTM) architectures , Attention and Transformer models , Autoencoders , Evaluation Metrics (e.g., Confusion Matrix, AUC, R-squared) , Mixed Precision Training , Transfer Learning and Fine-Tuning , and Deployment Practices (using FastAPI and TorchScript) . Each chapter integrates theoretical foundations with PyTorch code implementations and engineering checklists .",
  "categories": [
    "Introduction to AI and Fundamental Concepts",
    "Deep Learning",
    "Machine Learning",
    "Generative AI",
    "Large Language Models (LLMs)",
    "LLMOps",
    "AI Deployment",
    "AI Applications",
    "Image Analysis with AI"
  ]
},
{
  "name": "MIT 18.S096 Matrix Calculus for Machine Learning and Beyond, January IAP 2022",
  "link": "https://dspace.mit.edu/handle/1721.1/155680",
  "difficulty": "Unspecified",
  "type": "Course",
  "description": "This course (18.S096), authored by Alan Edelman and Steven G. Johnson, addresses the need for matrix calculus in modern applications such as machine learning . It serves as the logical next step beyond univariate and vector calculus, covering a coherent approach to matrix derivatives . The curriculum teaches techniques for conceptualizing a matrix holistically and computing derivatives of important matrix factorizations . A major focus is placed on truly understanding forward and reverse modes of differentiation . Topics discussed include adjoint methods, custom Jacobian matrix vector products, and the principle that modern automatic differentiation is often more computer science than mathematics, as it is neither symbolic nor based on finite differences . The course content package is available for download and was issued in 2022 by the MIT Department of Mathematics .",
  "categories": [
    "Introduction to AI and Fundamental Concepts",
    "Machine Learning",
    "Deep Learning",
    "AI Applications"
  ]
},
{
  "name": "Microsoft Model Context Protocol (MCP) Curriculum for Beginners",
  "link": "https://github.com/microsoft/mcp-for-beginners/",
  "difficulty": "Beginner",
  "type": "Course",
  "description": "This open-source curriculum introduces the fundamentals of Model Context Protocol (MCP) through structured, practical techniques for building modular, scalable, and secure AI workflows . Designed for developers, system architects, and software engineers, it provides real-world, cross-language examples in C# (.NET), Java, TypeScript, JavaScript, Rust, and Python . The curriculum is divided into modules covering Core Concepts, Security in MCP, Getting Started, Practical Implementation, and Advanced Topics . Key areas of focus include session setup, service orchestration , and comprehensive deployment strategies . The curriculum is highly hands-on, featuring a 13-lab learning path dedicated to building production-ready MCP servers with PostgreSQL database integration . It also covers multi-modality, scaling, and enterprise use . The repository has significant community support, boasting 11.3k stars and 3.4k forks , and offers multi-language support .",
  "categories": [
    "MCP (Model Context Protocol)",
    "Introduction to AI and Fundamental Concepts",
    "AI Agents",
    "Agent Development",
    "Systems Integration",
    "AI Deployment",
    "LLMOps",
    "Workflow Automation"
  ]
},
{
  "name": "Extending GitHub Copilot Coding Agent with Model Context Protocol (MCP) Documentation",
  "link": "https://docs.github.com/es/copilot/how-tos/use-copilot-agents/coding-agent/extend-coding-agent-with-mcp",
  "difficulty": "Unspecified",
  "type": "Tool",
  "description": "This documentation details how repository administrators can use the Model Context Protocol (MCP) to extend the capabilities of the GitHub Copilot Coding Agent . The extension works by configuring MCP servers using a specific JSON format within the repository settings on GitHub.com . Once configured, Copilot can use the tools provided by the server autonomously without requiring explicit user approval . The configuration requires specifying the tools to be enabled and the server type (which can be `\"local\"`, `\"http\"`, or `\"sse\"`) . For security, any required keys or secrets must be added to the Copilot environment and prefixed with `COPILOT_MCP_` . The guide provides examples for integrating various external services via MCP, including Sentry (for exception access) , Notion (for notes and content access) , Azure (for Cosmos DB and Storage) , Cloudflare , and Azure DevOps (for work items and pipelines) . It also covers customizing the integrated GitHub MCP Server, which is enabled by default for read-only repository access, to allow broader access using a fine-grained Personal Access Token [17-19]. A validation process is included to ensure the MCP configuration is set up correctly .",
  "categories": [
    "MCP (Model Context Protocol)",
    "AI Agents",
    "Agent Development",
    "Systems Integration",
    "Workflow Automation",
    "AI Deployment",
    "AI Applications"
  ]
},
{
  "name": "Copilot 3D (Microsoft Copilot Labs)",
  "link": "https://copilot.microsoft.com/labs/experiments/copilot-3d",
  "difficulty": "Unspecified",
  "type": "Tool",
  "description": "Copilot 3D is a free, experimental tool from Microsoft Copilot Labs that transforms 2D images into fully rendered 3D models, aiming to democratize 3D content creation [Query]. It offers a single-click conversion from any image and produces GLB output that is compatible with 3D viewers, design tools, and game engines [Query]. The tool boasts a zero learning curve, meaning no prior 3D modeling experience is required [Query]. It is completely free to use via Copilot Labs with a Microsoft account [Query]. It supports PNG/JPG files under 10MB on any modern browser, and models are stored on the 'My Creations' page for 28 days with manual deletion options [Query]. While the quality may not be the best available, it is recognized for being super easy to use and excellent for quickly testing ideas [Query].",
  "categories": [
    "Generative AI",
    "Multimodal AI",
    "AI Applications",
    "Content Generation with AI",
    "Image Analysis with AI"
  ]
},
{
  "name": "Data Formulator: Create Rich Visualizations with AI (Microsoft Research)",
  "link": "https://github.com/microsoft/data-formulator",
  "difficulty": "Unspecified",
  "type": "Tool",
  "description": "Data Formulator is an application from Microsoft Research that uses large language models (LLMs) to transform data and expedite the practice of data visualization iteratively with AI . It is an AI-powered tool for analysts to create rich visualizations by combining user interface interactions (UI) and natural language (NL) inputs, delegating data transformation to AI .Key features and capabilities include:*   AI Agents for Data Transformation: Agents generate SQL queries to transform data and create rich visualizations . Users can anchor intermediate datasets for cleaner follow-up analysis .*   Large Data Handling: It supports working with *large* data by uploading files to a local database (powered by DuckDB) and dynamically fetching data for visualizations with ⚡️⚡️⚡️ speeds .*   External Data Loaders: It includes an external data loader class to simplify data import, supporting sources like MySQL, Azure Data Explorer (Kusto), Azure Blob, Amazon S3 (json, parquet, csv), Postgresql, and mssql .*   Multi-Dataset Support: It supports working with multiple datasets at once, figuring out how to join tables to create a visualization based on the encoding shelf input .*   Model Agnostic Support: It supports various models, including OpenAI, Azure, Ollama, and Anthropic models, powered by LiteLLM . Models with strong code generation and instruction following capabilities (e.g., gpt-4o, claude-3-5-sonnet) are recommended .*   Iterative Workflow: Users can create visualizations beyond the initial dataset by typing names of fields that do not exist in the current data in the encoding shelf, letting the AI compute the transformation . Explorations are trackable in the Data Threads panel .*   Installation/Deployment: It can be installed easily via Python PIP for local running, or through Codespaces for a pre-configured environment . It is licensed under the MIT license .",
  "categories": [
    "Data Analysis",
    "Generative AI",
    "AI Applications",
    "Large Language Models (LLMs)",
    "AI Agents",
    "Agent Development",
    "Workflow Automation"
  ]
},
{
  "name": "Qwen-Image: 20B MMDiT Image Generation Foundation Model",
  "link": "https://github.com/QwenLM/Qwen-Image",
  "difficulty": "Unspecified",
  "type": "Tool",
  "description": "Qwen-Image is a powerful image generation foundation model (a 20B MMDiT model) capable of achieving significant advances in complex text rendering and precise image editing . It shows exceptional performance in text rendering, particularly for Chinese .Key features and functionalities include:*   Image Generation (Text-to-Image): Supports generating images based on text prompts and allows for various aspect ratios (e.g., 1:1, 16:9) .*   Image Editing: Features the specialized Qwen-Image-Edit architecture . The latest iteration, Qwen-Image-Edit-2509, supports multi-image editing (optimal for 1 to 3 inputs) and significantly enhances single-image consistency .*   Enhanced Consistency: The Edit-2509 version offers improved consistency in Person Editing (better facial identity preservation), Product Editing (better preservation of product identity), and Text Editing (supporting modification of content, fonts, colors, and materials) .*   Control and Integration: Provides Native Support for ControlNet image conditions (depth maps, edge maps, keypoint maps) . It is also natively supported in ComfyUI .*   Prompt Enhancement: Recommended workflows utilize the official Prompt Enhancement Tool (powered by Qwen-Plus or Qwen-VL-Max) to improve stability and optimize T2I and editing prompts .*   Deployment and Optimization: Supports Multi-GPU API Server deployment with a Gradio-based web interface, parallel processing, and queue management . Inference acceleration is available via cache-dit .*   Evaluation Platform: Introduces AI Arena, an open benchmarking platform built on the Elo rating system to compare image generation capabilities against closed-source APIs .The repository has strong community support, boasting 5.5k stars and 296 forks , and is licensed under the Apache-2.0 license .",
  "categories": [
    "Generative AI",
    "Multimodal AI",
    "Deep Learning",
    "Large Language Models (LLMs)",
    "Content Generation with AI",
    "Image Analysis with AI",
    "AI Deployment",
    "LLMOps",
    "Prompt Engineering"
  ]
},
{
  "name": "Firecrawl: The Web Data API for AI",
  "link": "https://github.com/firecrawl/firecrawl",
  "difficulty": "Unspecified",
  "type": "Tool",
  "description": "Firecrawl is an open-source Web Data API and platform designed to transform entire websites into LLM-ready markdown or structured data 🔥 . It aims to empower AI applications with clean data by featuring advanced scraping, crawling, and data extraction capabilities . The tool is available as an API service (hosted or self-hosted) .Key Features & Endpoints :*   Crawl: Crawls a URL and all accessible subpages, returning content in LLM-ready format, without needing a sitemap . It is optimized for Reliability and handles challenging issues like proxies, anti-bot mechanisms, dynamic content (JS-rendered), and orchestration .*   Scrape: Scrapes a single URL and returns content in formats including markdown, structured data (via LLM Extract), screenshot, or HTML . Supports batch scraping .*   Extract (Beta): Uses AI to get structured data from single pages, multiple pages, or entire domains using a prompt and/or a schema (supports Pydantic/Zod schemas in SDKs) [6, 10-12].*   Map: Quickly retrieves all URLs present on a website, which can be combined with a search parameter .*   Search: Performs web searches and optionally scrapes the full content from results in one operation .*   Actions (Cloud-only): Allows performing interactions on a web page before extraction, such as `click`, `scroll`, `input`, and `wait`, useful for dynamic content .Integration & Licensing: Firecrawl provides SDKs for Python and Node and integrates directly with major LLM frameworks like Langchain, Llama Index, and Crew.ai [17-19]. It is licensed under the AGPL-3.0, though SDKs and some components use the MIT License . The project has substantial community support with 61.3k stars and 5k forks .",
  "categories": [
    "Web Scraping with AI",
    "Large Language Models (LLMs)",
    "RAG (Retrieval-Augmented Generation)",
    "AI Agents",
    "Agent Development",
    "Data Analysis",
    "AI Applications",
    "Workflow Automation",
    "Systems Integration"
  ]
},
{
  "name": "agenticSeek: Private, Fully Local Autonomous AI Agent",
  "link": "https://github.com/Fosowl/agenticSeek",
  "difficulty": "Unspecified",
  "type": "Tool",
  "description": "AgenticSeek is a 100% local alternative to Manus AI and other cloud-based agents, providing a private, autonomous AI assistant that runs entirely on your local hardware . This open-source project is designed for users who want an agent that thinks, browses the web, and codes without relying on APIs or incurring monthly cloud bills . The agent keeps all data, conversations, and searches private on the user's device .Key Features:*   Fully Local Operation: Everything runs on the user's machine, ensuring privacy and zero cloud dependency . It is compatible with local providers like Ollama and LM Studio , though it also supports cloud APIs (OpenAI, Google Gemini, Deepseek, etc.) as an optional setup .*   Autonomous Capabilities: Acts as a Smart Web Browser (searching, reading, extracting information, and filling web forms) and an Autonomous Coding Assistant (writing, debugging, and running code in languages like Python, C, Go, and Java) .*   Task Planning: Can split complex tasks (from trip planning to complex projects) into steps and execute them using a Smart Agent Selection system and multiple AI agents .*   Interface: Supports both a web interface (runnable via Docker on `http://localhost:3000/`) and a command-line interface (CLI) .*   Voice Capability: Voice-enabled features (speech-to-text and text-to-speech) are included and are currently a work in progress (CLI mode only, experimental) .The project is licensed under GPL-3.0  and has substantial community backing, noted by its 22k stars and 2.3k forks .",
  "categories": [
    "AI Agents" ,
    "Agent Development" ,
    "Multi-Agent Systems" ,
    "Large Language Models (LLMs)" ,
    "AI Assistants",
    "AI Deployment",
    "Workflow Automation",
    "Web Scraping with AI",
    "AI Applications"
  ]
},
{
  "name": "Awesome Generative AI (Curated List)",
  "link": "https://github.com/steven2358/awesome-generative-ai",
  "difficulty": "Unspecified",
  "type": "Tool",
  "description": "A curated list of modern Generative Artificial Intelligence projects and services . Generative AI is defined as technology that creates original content such as images, sounds, and texts by using machine learning algorithms trained on large amounts of data . The list is organized into key domains including Text (Models, Chatbots, Search, Writing assistants), Coding (Assistants, Developer tools, Local LLM Deployment), Agents (Autonomous and Custom), Image, Video, and Audio . It highlights major milestones such as ChatGPT, GPT-4, LLaMA, and Sora . The resource covers applications in art, entertainment, marketing, academia, and computer science .",
  "categories": [
    "Generative AI",
    "Large Language Models (LLMs)",
    "AI Applications",
    "Multimodal AI",
    "AI Agents",
    "Deep Learning",
    "Introduction to AI and Fundamental Concepts"
  ]
},
{
  "name": "AGENTS.md: Open Format for Guiding Coding Agents (OpenAI)",
  "link": "https://github.com/openai/agents.md",
  "difficulty": "Unspecified",
  "type": "Tool",
  "description": "AGENTS.md is a simple, open format designed for guiding coding agents . It is conceptualized as a README for agents, serving as a dedicated and predictable place to provide context and instructions to help AI coding agents work on a project . The format provides structure for project governance by including elements such as Dev environment tips, Testing instructions (e.g., specifying run commands like `pnpm turbo run test`), and PR instructions (e.g., required title formats and pre-commit checks) . The repository also hosts a basic website that explains the project's goals .",
  "categories": [
    "AI Agents",
    "Agent Development",
    "Workflow Automation",
    "Systems Integration"
  ]
},
{
  "name": "All RL Algorithms from Scratch (Python Implementation)",
  "link": "https://github.com/FareedKhan-dev/all-rl-algorithms",
  "difficulty": "Beginner",
  "type": "Course",
  "description": "A collection of Python implementations of various Reinforcement Learning (RL) algorithms . The *primary* goal is educational: to gain a deep and intuitive understanding of how these algorithms work . The repository prioritizes readability and clarity over performance optimization . It uses basic libraries (NumPy, Matplotlib, PyTorch) and serves as an interactive textbook for RL . It includes 18 algorithm notebooks  covering foundational and advanced RL methods, such as Q-Learning, SARSA, REINFORCE, PPO (Proximal Policy Optimization), A2C/A3C, DDPG, SAC (Soft Actor-Critic), TRPO, DQN, MADDPG, QMIX, HAC, MCTS, and PlaNet . A comprehensive cheat sheet summarizing key concepts and algorithms is also provided .",
  "categories": [
    "Machine Learning",
    "Deep Learning",
    "AI Agents",
    "Introduction to AI and Fundamental Concepts"
  ]
},
{
  "name": "Awesome Computer Vision (Curated Resource List)",
  "link": "https://github.com/jbhuang0604/awesome-computer-vision",
  "difficulty": "Unspecified",
  "type": "Tool",
  "description": "A curated list of awesome computer vision resources . The list is comprehensive, including sections for Books , Courses (e.g., Stanford's Convolutional Neural Networks for Visual Recognition) , Papers , Software (e.g., OpenCV, Annotation tools) , Datasets (e.g., PASCAL, ImageNet, KITTI) , Tutorials, and resources for students (including advice on research and writing) . It covers numerous specialized topics such as Deep Vision, Object Detection, 3D Machine Learning, Neural Rendering, Visual Tracking, Semantic Segmentation, and SLAM (Simultaneous Localization and Mapping) [32-34].",
  "categories": [
    "Image Analysis with AI",
    "Deep Learning",
    "Machine Learning",
    "Multimodal AI",
    "AI Applications",
    "Introduction to AI and Fundamental Concepts"
  ]
},
{
  "name": "Awesome MCP Servers (Curated List)",
  "link": "https://github.com/punkpeye/awesome-mcp-servers/",
  "difficulty": "Unspecified",
  "type": "Tool",
  "description": "A curated list of awesome Model Context Protocol (MCP) servers . MCP is an open protocol that allows AI models to securely interact with local and remote resources through standardized server implementations, enabling AI capabilities via file access, database connections, and API integrations . The list focuses on production-ready and experimental servers , categorized extensively by function:*   Aggregators (e.g., MetaMCP, Magg: A meta-MCP server) .*   Biology/Medicine (e.g., BioMCP, FHIR servers) .*   Browser Automation (e.g., Playwright, Puppeteer servers) .*   Databases (e.g., PostgreSQL, Snowflake, Redis, Milvus, Qdrant integrations) [40-42].*   Coding Agents (e.g., CodeMCP, VSCode MCP servers, LeetCode solvers) .*   Developer Tools (e.g., GitKraken, Docker, GitHub, GitLab integrations) .*   Finance & Fintech (e.g., Alpaca, Binance, Bitcoin, trading agents) [46-48].*   Knowledge & Memory (e.g., Graphlit, Mem0, Pinecone Assistant, Zotero) .*   Security (e.g., Ghidra, VirusTotal, Sentry, Semgrep integration) .It also lists official MCP clients, tutorials (e.g., Quickstart), and frameworks like FastMCP . The repository has considerable community support with 71.9k stars .",
  "categories": [
    "MCP (Model Context Protocol)",
    "AI Agents",
    "Agent Development",
    "Systems Integration",
    "Workflow Automation",
    "AI Deployment",
    "Large Language Models (LLMs)"
  ]
},
{
  "name": "Microsoft Hands-on Course: Model Context Protocol (MCP)",
  "link": "https://www.youtube.com/playlist?list=PLlrxD0HtieHjYfVUpGl_-ai7D6FRBjV-d",
  "difficulty": "Unspecified",
  "type": "Course",
  "description": "This is a full hands-on course released by Microsoft focusing on the Model Context Protocol (MCP), described as an essential resource for building real-world AI agents [Query]. The course covers MCP fundamentals and architecture, as well as security best practices for safe deployment [Query]. It includes step-by-step instructions for building the first MCP server, developing, testing, and deploying applications with real tools [Query]. The curriculum addresses advanced workflows for scalable and multimodal agents [Query], contributing to the ecosystem (tools, docs, and code), and includes lessons from early adopters and real-world case studies [Query]. Practical laboratories are conducted in Visual Studio Code, Docker, and Google Cloud [Query]. The course features an SDK walkthrough for FastMCP (Python) and details end-to-end agent deployment with Azure and browser automation [Query]. Bonus modules cover adaptive learning and AI documentation workflows [Query].",
  "categories": [
    "MCP (Model Context Protocol)",
    "AI Agents",
    "Agent Development",
    "AI Deployment",
    "LLMOps",
    "Systems Integration",
    "Workflow Automation",
    "Large Language Models (LLMs)",
    "Multimodal AI"
  ]
},
{
  "name": "LEANN: Low-Storage Vector Index for Private RAG (97% Savings)",
  "link": "https://github.com/yichuan-w/LEANN",
  "difficulty": "Unspecified",
  "type": "Tool",
  "description": "LEANN is an innovative vector database that democratizes personal AI by enabling RAG (Retrieval-Augmented Generation) on Everything . It is capable of running a fast, accurate, and 100% private RAG application on a personal device, achieving up to 97% less storage than traditional solutions without accuracy loss [1, 3-5]. This is accomplished through core techniques like graph-based selective recomputation and high-degree preserving pruning, which compute embeddings on-demand instead of storing them . The system ensures complete privacy as data never leaves the user's laptop .LEANN can index and semantic search across diverse data sources, including :*   Your personal file system and documents (.pdf, .txt, .md) .*   Email (Apple Mail) .*   Browser history (Chrome) .*   Chat history (WeChat, iMessage) .*   Agent memory (ChatGPT, Claude conversations) .*   Codebase (with AST-aware code chunking for Python, Java, C#, and TypeScript) .LEANN is a semantic search MCP service fully compatible with Claude Code . It supports flexible deployment, working with local inference engines like Ollama (recommended for privacy) and LM Studio, as well as cloud APIs compatible with OpenAI . It offers both a declarative Python API and a powerful Command Line Interface (CLI) .",
  "categories": [
    "RAG (Retrieval-Augmented Generation)",
    "AI Agents",
    "MCP (Model Context Protocol)",
    "Large Language Models (LLMs)",
    "AI Deployment",
    "Systems Integration",
    "Document Processing",
    "Data Analysis",
    "Deep Learning"
  ]
},
{
  "name": "Supermemory MCP - Universal Memory across LLMs",
  "link": "https://github.com/supermemoryai/supermemory-mcp",
  "difficulty": "Unspecified",
  "type": "Tool",
  "description": "Supermemory MCP (Model Context Protocol) is an open-source project designed to provide Universal Memory across different Large Language Models (LLMs) . The philosophy is that while 'Your memories are in ChatGPT... [they are] nowhere else,' and this tool makes those memories available to every single LLM .Key features include:*   Portability: Users can carry their memories to any MCP client, ensuring consistency regardless of the LLM application used .*   Accessibility: The service is described as being built on the Supermemory API, extremely fast and scalable, and is completely free to use with no logins or paywalls required .*   Setup: The setup process is extremely simple, often requiring just one command to set it up . Users can get started by visiting the corresponding application website .*   Self-Hosting: Self-hosting is supported by obtaining an API key and adding it to the `.env` file .The repository has strong community support, boasting 1.5k stars and 143 forks , and is primarily developed using TypeScript (99.6%) . It is licensed under the MIT license . A warning notes that MCP v1 is being deprecated, and users should get the latest version from the Supermemory app website .",
  "categories": [
    "MCP (Model Context Protocol)",
    "AI Agents",
    "Agent Memory",
    "Large Language Models (LLMs)",
    "Systems Integration",
    "AI Deployment",
    "LLMOps"
  ]
},
{
  "name": "Gemini for Google Workspace: Prompting Guide 101 (Quick-Start Handbook)",
  "link": "https://services.google.com/fh/files/misc/gemini-for-google-workspace-prompting-guide-101.pdf",
  "difficulty": "Beginner",
  "type": "Book",
  "description": "This quick-start handbook (October 2024 edition) provides foundational skills for writing effective prompts when using Gemini for Google Workspace, emphasizing collaboration with AI to boost productivity and creativity without sacrificing privacy or security . Gemini is integrated into apps like Gmail, Docs, Sheets, Meet, and Slides, with access to personal knowledge in Drive . Effective prompts should consider four main areas: Persona, Task, Context, and Format . Key tips include using natural language, being specific and iterative, keeping requests concise (fruitful prompts average around 21 words), making it a conversation, using documents (via tagging `@file name`), and using the 'Make this a power prompt' feature in Gemini Advanced . The guide assures that user content is protected and never used to train or improve Gemini models . It provides scenario-based examples for various roles, including Customer Service, Executives, HR Managers, Marketing Specialists, and Project Managers .",
  "categories": [
    "Prompt Engineering",
    "Generative AI",
    "Large Language Models (LLMs)",
    "AI Assistants",
    "AI Applications",
    "Workflow Automation",
    "Introduction to AI and Fundamental Concepts",
    "AI Ethics"
  ]
},
{
  "name": "AI in the Enterprise: Lessons from Seven Frontier Companies (OpenAI)",
  "link": "https://cdn.openai.com/business-guides-and-resources/ai-in-the-enterprise.pdf",
  "difficulty": "Unspecified",
  "type": "Book",
  "description": "This guide distills seven essential lessons for enterprise AI adoption based on insights from numerous customer deployments . It emphasizes that the future of work involves sophisticated, complex, and interconnected AI workflows . Key lessons include: 1) Start with Evals (using rigorous, structured processes to measure performance against use cases, e.g., Morgan Stanley) ; 2) Embed AI into products (e.g., Indeed used GPT-4o mini to personalize job matching, resulting in a 20% increase in applications started) ; 3) Start now and invest early (AI value compounds, e.g., Klarna's AI assistant handles two-thirds of service chats) ; 4) Customize and Fine-tune models (tailoring models to specific data for improved accuracy and domain expertise, like Lowe's search improvement) ; 5) Get AI in the hands of experts (enabling employees to build their own custom GPTs, e.g., BBVA employees created over 2,900 custom GPTs) ; 6) Unblock developers (automating the software development lifecycle, e.g., Mercado Libre built the Verdi platform) ; and 7) Set bold automation goals (automating entire workflows) . The guide highlights the Operator agentic tool, which uses a virtual browser to automate workflows without custom APIs .",
  "categories": [
    "AI Applications",
    "Generative AI",
    "Large Language Models (LLMs)",
    "LLMOps",
    "Workflow Automation",
    "AI Deployment",
    "AI Agents",
    "Business Automation"
  ]
},
{
  "name": "A Practical Guide to Building Agents (OpenAI)",
  "link": "https://cdn.openai.com/business-guides-and-resources/a-practical-guide-to-building-agents.pdf",
  "difficulty": "Intermediate",
  "type": "Book",
  "description": "This guide assists product and engineering teams in building reliable and effective LLM agents, which are systems that independently accomplish complex, multi-step tasks on a user's behalf . Agents are particularly suited for workflows where deterministic, rule-based approaches fail, such as those involving complex decisions or unstructured data . The three core components of an agent are the Model (LLM for reasoning), Tools (external APIs or systems, including computer-use models for legacy UIs) , and Instructions (clear guidelines and guardrails) . Orchestration patterns covered include Single-Agent Systems (a single model executing a workflow in a loop) and Multi-Agent Systems (distributed workflows via the Manager Pattern or Decentralized Handoffs) . A critical section is dedicated to Guardrails, described as a layered defense mechanism (e.g., Relevance Classifier, Safety Classifier, Rules-based protections) to ensure agents operate safely and predictably in production .",
  "categories": [
    "AI Agents",
    "Agent Development",
    "Multi-Agent Systems",
    "Large Language Models (LLMs)",
    "Workflow Automation",
    "Systems Integration",
    "AI Deployment",
    "AI Ethics"
  ]
},
{
  "name": "Building Effective Agents: Simple, Composable Patterns (Anthropic Engineering Guide)",
  "link": "https://www.anthropic.com/engineering/building-effective-agents",
  "difficulty": "Unspecified",
  "type": "Course",
  "description": "This post shares Anthropic's learning from working with dozens of teams building LLM agents, concluding that the most successful implementations use simple, composable patterns rather than complex frameworks . It defines an important architectural distinction between Workflows (predefined code paths) and Agents (LLMs dynamically direct their own processes and tool usage) . The core component is the Augmented LLM, enhanced with retrieval, tools, and memory . Recommended agentic workflows include Prompt Chaining , Routing (classifies input and directs it to a specialized task) , and Evaluator-Optimizer (one LLM generates a response, another provides iterative feedback) . Agents are recommended for open-ended problems where the required number of steps is difficult to predict . Three core implementation principles are emphasized: maintaining simplicity, prioritizing transparency (showing planning steps), and carefully crafting the agent-computer interface (ACI) through thorough tool documentation and testing . Promising applications include Customer Support and Coding Agents (e.g., solving SWE-bench tasks) .",
  "categories": [
    "AI Agents",
    "Agent Development",
    "Large Language Models (LLMs)",
    "Workflow Automation",
    "Systems Integration",
    "Prompt Engineering"
  ]
},
{
  "name": "Identifying and Scaling AI Use Cases (OpenAI Business Guide)",
  "link": "https://cdn.openai.com/business-guides-and-resources/identifying-and-scaling-ai-use-cases.pdf",
  "difficulty": "Unspecified",
  "type": "Book",
  "description": "This guide provides a framework for organizations to find and scale AI use cases that deliver clear value, drawing on insights from over 300 successful customer implementations . It suggests focusing on three areas where AI can create 'super-assistants': Repetitive low value tasks, Skill bottlenecks (e.g., data analysis, coding), and Navigating ambiguity (ideation/brainstorming) [201-204]. The core of the guide presents the Six Use Case Primitives that apply across all departments and disciplines: Content creation, Automation, Research, Coding, Data analysis, and Ideation/strategy . The resource details how to prioritize ideas using the Impact/Effort Framework (scoring value vs. effort)  and encourages thinking about embedding AI into multi-step workflows (Workflow Mapping) rather than just single tasks . Specific agentic capabilities highlighted include Deep Research (multi-step internet research agent)  and Operator (web automation agent) .",
  "categories": [
    "AI Applications",
    "Generative AI",
    "Workflow Automation",
    "Data Analysis",
    "Large Language Models (LLMs)",
    "AI Agents",
    "Introduction to AI and Fundamental Concepts",
    "Business Automation"
  ]
},
{
  "name": "601 Real-World Generative AI Use Cases from Industry Leaders (Google Cloud)",
  "link": "https://cloud.google.com/transform/101-real-world-generative-ai-use-cases-from-industry-leaders?hl=en",
  "difficulty": "Unspecified",
  "type": "Tool",
  "description": "This document showcases an extensive collection of real-world generative AI use cases, which expanded from 101 entries to 601 entries within one year, demonstrating the speed of enterprise AI adoption . The use cases are organized across 11 major industry groups (e.g., Automotive & Logistics, Financial Services, Healthcare, Retail) and categorized by six agent types: Customer, Employee, Creative, Code, Data, and Security . Examples include using conversational AI in vehicles (Mercedes Benz, General Motors) , enhancing employee productivity via Google Workspace/Gemini (Oxa, Rivian, Uber) , accelerating code development (Renault Group's Ampere, TCS) , combatting fraud (Citi, Deutsche Bank, Bradesco) , and creating hyper-personalized customer experiences (Spotify, Target, Wendy's) [221-223]. The document acts as a showcase of how leading organizations are enhancing their work with Google's AI solutions .",
  "categories": [
    "AI Applications",
    "Generative AI",
    "AI Agents",
    "Large Language Models (LLMs)",
    "Business Automation",
    "AI Deployment",
    "Workflow Automation",
    "Systems Integration",
    "Data Analysis"
  ]
},
{
  "name": "RAG-Anything: All-in-One Multimodal RAG Framework (HKUDS)",
  "link": "https://github.com/HKUDS/RAG-Anything",
  "difficulty": "Unspecified",
  "type": "Tool",
  "description": "RAG-Anything is a comprehensive, All-in-One Multimodal Document Processing RAG system built on LightRAG . It is designed to address the challenge that traditional RAG systems face when processing documents containing diverse multimodal content, such as text, images, tables, equations, charts, and multimedia . As a unified solution, it eliminates the need for multiple specialized tools by providing seamless processing and querying across all content modalities within a single integrated framework .Key Features and Architecture:*   Universal Document Support: It handles PDFs, Office documents (DOCX, PPTX, XLSX), images, and diverse file formats . Processing Office documents requires a separate LibreOffice installation .*   Multimodal Knowledge Graph: The system extracts multimodal entities and automatically discovers cross-modal relationships to enhance understanding, transforming document content into structured semantic representations .*   Specialized Content Analysis: It deploys dedicated processing units for heterogeneous data, including Visual Content Analyzer (integrating vision models for image analysis and caption generation), Structured Data Interpreter (for tables and statistical analysis), and a Mathematical Expression Parser (with native LaTeX support) .*   Intelligent Retrieval: It uses a Hybrid Intelligent Retrieval system, combining vector similarity search with graph traversal algorithms for comprehensive content retrieval .*   VLM-Enhanced Query Mode: When documents include images, the system seamlessly integrates them into the Vision Language Model (VLM) for advanced multimodal analysis, enabling enhanced RAG with text, images, tables, and equations .*   Deployment: The Python-based project (100.0% Python)  can be installed via PyPI (`pip install raganything`)  and has significant community support (8k stars, 904 forks) .",
  "categories": [
    "RAG (Retrieval-Augmented Generation)",
    "Multimodal AI",
    "Document Processing",
    "Large Language Models (LLMs)",
    "Deep Learning",
    "AI Applications",
    "Image Analysis with AI",
    "Data Analysis",
    "Systems Integration",
    "Workflow Automation"
  ]
},
{
  "name": "Mem0: Universal Memory Layer for AI Agents",
  "link": "https://github.com/mem0ai/mem0",
  "difficulty": "Unspecified",
  "type": "Tool",
  "description": "Mem0 ('mem-zero') is an open-source project designed as an intelligent memory layer that enhances AI assistants and agents, enabling personalized and continuously learning AI interactions . It is suitable for autonomous systems, customer support chatbots, and AI assistants . The project is licensed under the Apache-2.0 license .It achieves high performance, demonstrating +26% Accuracy over OpenAI Memory on the LOCOMO benchmark, delivering 91% Faster Responses, and resulting in 90% Lower Token Usage compared to full-context methods .Key features and concepts include:*   OpenMemory MCP: Mem0 implements OpenMemory MCP for local and secure memory management , providing a self-hosted option .*   Multi-Level Memory: It seamlessly retains User, Session, and Agent state with adaptive personalization .*   Developer-Friendly: It offers an intuitive API, cross-platform SDKs (via pip/npm installation), and a fully managed service option .*   LLM Support: Mem0 requires a Large Language Model (LLM) to function, defaulting to `gpt-4o-mini` from OpenAI but supporting a variety of other LLMs .*   Integrations: It supports integrations with major frameworks and applications, including Langgraph, CrewAI, a Browser Extension (for services like ChatGPT, Perplexity, and Claude), and a Live Demo of ChatGPT with Memory .",
  "categories": [
    "AI Agents",
    "Agent Development",
    "Agent Memory",
    "MCP (Model Context Protocol)",
    "Large Language Models (LLMs)",
    "LLMOps",
    "AI Deployment",
    "Systems Integration",
    "AI Applications",
    "Chatbots",
    "Workflow Automation",
    "RAG (Retrieval-Augmented Generation)",
    "Deep Learning"
  ]
},
{
  "name": "Parlant: LLM Agents Built for Control",
  "link": "https://github.com/emcie-co/parlant",
  "difficulty": "Unspecified",
  "type": "Tool",
  "description": "Parlant is an open-source framework (licensed under Apache-2.0 ) for developing LLM agents built for control, designed for real-world use and deployable in minutes . It aims to solve the problem of agents ignoring system prompts, hallucinating responses, failing on edge cases, and exhibiting unpredictable behavior in production .Instead of relying on complex system prompts, Parlant ensures compliance by allowing developers to define Behavioral Guidelines using natural language . These rules are contextually matched and reliably enforced .Key features that provide control and reliability include:*   Behavioral Guidelines: Easily defined rules that match relevant context (e.g., 'Customer asks about refunds' triggers an action) .*   Tool Integration: Reliable integration of external APIs, data fetchers, or backend services attached to specific interaction events . Conversational Journeys: Tools to lead the customer step-by-step toward a specific goal .  Canned Responses: Response templates used to eliminate hallucinations and guarantee consistency in style . Guardrails: Built-in mechanisms to prevent hallucination and off-topic responses .   Explainability: Full transparency to understand why and when each guideline was matched and followed .Parlant is production-ready from day one  and is used by companies in Financial Services (for compliance-first design), Healthcare (HIPAA-ready agents), E-commerce, and Legal Tech . The repository has strong community support, boasting 13.4k stars and 1.1k forks . It is primarily developed in Python (88.5%)  and offers a drop-in React widget for chat UI integration .",
  "categories": [
    "AI Agents" ,
    "Agent Development" ,
    "Large Language Models (LLMs)" ,
    "Generative AI" ,
    "AI Deployment" ,
    "LLMOps" ,
    "Workflow Automation" ,
    "Systems Integration" ,
    "AI Applications" ,
    "Chatbots" ,
    "AI Assistants" 
  ]
},
{
  "name": "MiniCPM-V 4.5: GPT-4o Level MLLM for Single Image, Multi Image and High-FPS Video Understanding on Your Phone",
  "link": "https://github.com/OpenBMB/MiniCPM-V",
  "difficulty": "Unspecified",
  "type": "Tool",
  "description": "MiniCPM-V is a series of efficient end-side multimodal Large Language Models (MLLMs) developed by THUNLP and ModelBest, designed to accept images, videos, and text inputs and deliver high-quality text outputs, aiming for strong performance and efficient deployment . The repository has garnered significant community support with 22k stars and 1.7k forks .Key Model (MiniCPM-V 4.5):This model has 8B parameters and, in vision-language capabilities, it outperforms proprietary models like GPT-4o-latest, Gemini-2.0 Pro, and open-source models like Qwen2.5-VL 72B . It is considered the most performant on-device multimodal model in the open-source community . New features include: *   Efficient High-FPS and Long Video Understanding (up to 96x compression rate for video tokens) using a Unified 3D-Resampler .*   Controllable Hybrid Fast/Deep Thinking modes .*   Strong Handwritten OCR and complex table/document parsing, surpassing proprietary models like GPT-4o-latest and Gemini 2.5 on OCRBench .*   Ability to process high-resolution images up to 1.8 million pixels .Omni-modal Model (MiniCPM-o 2.6):This end-to-end 8B parameter model achieves comparable performance to GPT-4o-202405 in vision, speech, and multimodal live streaming [11-14]. It supports bilingual real-time speech conversation with configurable voices, voice cloning, and style control . It exhibits superior token density (75% fewer tokens than most models for a 1.8M pixel image) .Deployment and Integration:MiniCPM-V and MiniCPM-o models are licensed under the Apache-2.0 license  and support various deployment methods :*   Efficient CPU Inference on local devices via official support for llama.cpp and Ollama [17-19].*   High-throughput inference via vLLM and SGLang [17-19].*   Quantized models are available in int4, GGUF, and AWQ formats .*   Fine-tuning support is provided via Transformers, LLaMA-Factory, SWIFT, and Align-Anything [17, 21-23].*   Includes optimized local iOS apps for iPhone and iPad .",
  "categories": [
    "Multimodal AI",
    "Large Language Models (LLMs)",
    "Generative AI",
    "Deep Learning",
    "AI Deployment",
    "Image Analysis with AI",
    "Model Management",
    "LLMOps",
    "Machine Learning"
  ]
},
{
  "name": "NVIDIA Deep Learning Institute (DLI) Training Catalog",
  "link": "https://www.nvidia.com/en-us/training/find-training/?Free+Courses=Free",
  "difficulty": "Unspecified",
  "type": "Course",
  "description": "This link directs to the comprehensive training catalog offered by the NVIDIA Deep Learning Institute (DLI), specifically filtered for free courses [Query]. The platform offers various formats, including Self Paced Courses, Instructor-Led Workshops, Learning Paths, and opportunities for Certification . The training is designed to help users acquire specific skills, keep pace with technology, and advance their careers . The curriculum covers critical AI and computing topics such as Accelerated Computing, Data Science, Deep Learning, Generative AI / LLMs, and Simulation / Modeling / Design . NVIDIA also focuses on topics like Agentic AI  and enterprise AI infrastructure .",
  "categories": [
    "Introduction to AI and Fundamental Concepts",
    "Deep Learning",
    "Generative AI",
    "Large Language Models (LLMs)",
    "Machine Learning",
    "AI Applications",
    "Data Analysis",
    "AI Agents",
    "AI Deployment",
    "LLMOps"
  ]
},
{
  "name": "Pocket Server: OS for AI Agents, Built for Your Pocket",
  "link": "https://github.com/yayasoumah/pocket-server",
  "difficulty": "Unspecified",
  "type": "Tool",
  "description": "Pocket Server is the local runtime of Pocket, a mobile operating system for AI agents . It allows a phone to host, control, and collaborate with agents against a local codebase in real time by exposing HTTP + WebSocket APIs . Its mission is to make agents native to the phone, prioritizing fast, local-first, and secure operation by providing OS-like primitives such as sessions, processes, filesystem, networking, and notifications . The project is licensed under the Apache-2.0 license and has strong community support with 715 stars and 58 forks .Core OS Capabilities:*   Agent Runtime and Tools: Supports coding agents that can be approved or auto-run with `bash`, `editor`, and `web-search` tools .*   Native Mobile Terminal: Features a fast, touch-optimized, multi-tab terminal session with smooth streaming, which can be resumed on a desktop terminal using the CLI .*   File System + Editor: Enables browsing, viewing, editing, and diffing files directly from the phone .*   Background Agents: Supports launching autonomous coding jobs on VMs from GitHub repositories, allowing monitoring, review of diffs, and approval of Pull Requests (PRs) .*   Context Injection: Injects project context into agent prompts by searching the working directory for `CLAUDE.md` (preferred by Anthropic) or `AGENTS.md` (preferred by OpenAI), capping the context size at ~100KB to maintain prompt responsiveness .*   Security and Access: Uses PIN pairing (local by default) and short-lived tokens for authentication over HTTP and WS . Optional remote access is available via a Cloudflare tunnel initiated with the `--remote` flag .Integration:It supports major AI providers, including OpenAI (GPT-5) and Anthropic (Claude), configured via environment variables (e.g., `OPENAI_API_KEY`) .",
  "categories": [
    "AI Agents",
    "Agent Development",
    "Systems Integration",
    "Workflow Automation",
    "AI Deployment",
    "Large Language Models (LLMs)",
    "LLMOps",
    "AI Applications"
  ]
},
{
  "name": "System Prompts and Models of AI Tools (Curated Collection)",
  "link": "https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools",
  "difficulty": "Unspecified",
  "type": "Tool",
  "description": "This GitHub repository is a comprehensive and massive collection of system prompts, internal tools, and underlying AI models of numerous commercial and open-sourced AI tools . It provides deep insights into the structure and functionality of these systems, boasting over 20,000+ lines of insights .The repository covers a vast array of AI tools, primarily focusing on coding agents and AI assistants :*   Coding/Developer Agents: Augment Code, Claude Code, CodeBuddy, Comet Assistant, Cursor Prompts, Devin AI, Junie, Kiro, Leap.new, Manus Agent Tools, Poke, Qoder, Replit, Same.dev, Trae, Traycer AI, VSCode Agent, Warp.dev, Windsurf, Xcode, and Z.ai Code [1, 3-5].*   AI Tools/Assistants: Cluely, Lovable, NotionAI, Orchids.app, Perplexity, dia, and v0 Prompts and Tools [1, 3-5].*   Other Prompts/Models: Anthropic and Amp .The goal of this collection is to provide the tools needed for building reliable Agents and Prompts . The project is highly recognized within the community, evidenced by 89.7k stars and 24.4k forks . It is licensed under the GPL-3.0 license . The creator also offers a security notice regarding the risk of exposed prompts and internal tools for AI startups . Updates are initially shared on Discord before appearing in the repository .",
  "categories": [
    "AI Agents",
    "Agent Development",
    "Prompt Engineering",
    "Large Language Models (LLMs)",
    "AI Applications",
    "Software Development",
    "Systems Integration",
    "Workflow Automation"
  ]
},
{
  "name": "Codebuff: Multi-Agent AI Coding Assistant (Terminal & SDK)",
  "link": "https://github.com/CodebuffAI/codebuff",
  "difficulty": "Unspecified",
  "type": "Tool",
  "description": "Codebuff is an open-source AI coding assistant (licensed under Apache-2.0) that edits entire codebases through natural language instructions, boasting 2.3k stars and 284 forks . It operates through a coordinated multi-agent approach to achieve better context understanding, more accurate edits, and fewer errors compared to single-model tools .Codebuff claims to beat Claude Code (61% vs 53%) on its evaluations across over 175 real-world coding tasks .Key Features:*   Multi-Agent Architecture: Coordinates specialized agents (e.g., File Explorer Agent, Planner Agent, Editor Agent, and Reviewer Agent) to plan and execute precise changes across the codebase .*   CLI Interface: Allows users to run coding tasks directly from the terminal (e.g., 'Fix the SQL injection vulnerability in user registration') .*   Customizability: Supports creating custom agents and defining complex workflows with TypeScript generators, offering maximum control over agent behavior, tool specification, and prompts .*   Production SDK: Provides a TypeScript SDK (`@codebuff/sdk`) to run agents in production, enabling integration into CI/CD pipelines, development environments, and applications .*   Model Agnostic: Supports any model on OpenRouter (including Claude, GPT, Qwen, and DeepSeek), avoiding vendor lock-in associated with single-provider tools .*   Agent Reusability: Promotes the composition and reuse of existing published agents, positioning its agents as 'the new MCP' .",
  "categories": [
    "AI Agents",
    "Agent Development",
    "Multi-Agent Systems",
    "Large Language Models (LLMs)",
    "Prompt Engineering",
    "MCP (Model Context Protocol)",
    "Workflow Automation",
    "Systems Integration",
    "AI Deployment",
    "LLMOps",
    "AI Applications",
    "Software Development"
  ]
},
{
  "name": "Deep Learning for Computer Vision 2025 (Stanford Online)",
  "link": "https://www.youtube.com/playlist?list=PLoROMvodv4rOmsNzYBMe0gJY2XS8AQg16",
  "difficulty": "Unspecified",
  "type": "Course",
  "description": "This is a free and complete course, titled 𝗗𝗘𝗘𝗣 𝗟𝗘𝗔𝗥𝗡𝗜𝗡𝗚 𝗙𝗢𝗥 𝗖𝗢𝗠𝗣𝗨𝗧𝗘𝗥 𝗩𝗜𝗦𝗜𝗢𝗡 𝟮𝟬𝟮𝟱, published by Stanford Online [Query]. It is described as an absolute luxury due to its accessibility and prestige, consisting of 18 sessions of approximately one hour each, with content that is fully updated [Query]. The comprehensive curriculum addresses: Fundamentals of visual recognition [Query], deep neural networks for classification and detection [Query], transformers applied to vision [Query], state-of-the-art diffusion models [Query], and visual language models (like those used by ChatGPT-4V) [Query]. The course also involves practical projects with networks of millions of parameters [Query]. This technology drives critical applications such as the autonomous vehicle revolution, medical imaging diagnosis, visual search systems, and augmented reality applications [Query]. The material is accessible worldwide and available with subtitles in various languages on YouTube [Query].",
  "categories": [
    "Introduction to AI and Fundamental Concepts" ,
    "Deep Learning" ,
    "Generative AI" ,
    "Machine Learning" ,
    "Multimodal AI" ,
    "AI Applications" ,
    "Image Analysis with AI" 
  ]
},
{
  "name": "AgentScope: Agent-Oriented Programming for Building LLM Applications",
  "link": "https://github.com/agentscope-ai/agentscope",
  "difficulty": "Unspecified",
  "type": "Tool",
  "description": "AgentScope is an open-source framework (licensed under Apache-2.0 ) designed for Agent-Oriented Programming for building Large Language Model (LLM) applications . It is structured to be easy for beginners yet powerful for experts . The framework has significant community support with 12.9k stars and 1k forks  and is developed primarily in Python (100.0%) .Core Principles and Architecture:*   Transparent to Developers: Transparency is the FIRST principle, ensuring that prompt engineering, API invocation, agent building, and workflow orchestration are visible and controllable, avoiding deep encapsulation or implicit 'magic' .*   Model Agnostic: Allows programming once and running with all models, supporting features like async invocation and streaming/non-streaming returns .*   Multi-Agent Oriented: Explicitly designed for multi-agent systems, utilizing `MsgHub` and pipelines for efficient message passing and workflow orchestration .*   LEGO-style Building: All components are modular and independent, allowing for high customizability of tools, prompts, agents, and workflows .Key Features (AgentScope 1.0):*   Agentic Capabilities: Supports agentic tools management, agent-controlled long-term memory control, and agentic RAG . It includes specialized agents like the `ReActAgent`, `Voice Agent`, `Deep Research Agent`, `Browser-use Agent`, and `Meta Planner Agent` .*   Realtime Steering: Natively supports realtime interruption with robust memory preservation .*   Planning: Includes a powerful new Plan module that supports ReAct-based long-term planning and manual plan specification .*   MCP Integration: Provides native support for the Model Context Protocol (MCP) with streamable HTTP/SSE/StdIO transport, supporting both stateful and stateless modes . Developers can obtain the MCP tool as a local callable function for fine-grained control .*   RAG: Features a dedicated RAG module that supports agentic and multimodal RAG .*   Deployment and Observability: Includes AgentScope Runtime for effective agent deployment with sandboxed tool execution for production-ready applications . It also features AgentScope Studio for tracing and visualization , and supports OpenTelemetry-based tracing across LLM, tools, and agents .",
  "categories": [
    "AI Agents",
    "Agent Development",
    "Multi-Agent Systems",
    "Large Language Models (LLMs)",
    "MCP (Model Context Protocol)",
    "RAG (Retrieval-Augmented Generation)",
    "AI Deployment",
    "LLMOps",
    "Systems Integration",
    "Workflow Automation",
    "Multimodal AI",
    "Deep Learning"
  ]
},
{
  "name": "How to build with Nano Banana: Complete Developer Tutorial (Google AI)",
  "link": "https://dev.to/googleai/how-to-build-with-nano-banana-complete-developer-tutorial-646",
  "difficulty": "Unspecified",
  "type": "Book",
  "description": "This is a comprehensive developer tutorial providing a walkthrough for integrating Gemini 2.5 Flash Image, also known by its codename Nano Banana, into applications using the Gemini Developer API . Nano Banana is described as a powerful new model for image generation and editing, offering state-of-the-art capabilities .The guide covers the following key areas:*   Environment Setup: Using the model in Google AI Studio for prototyping (free of charge) , obtaining an API key, setting up billing (API usage costs $0.039 per image) , and installing the Google Gen AI SDK for Python or JavaScript/TypeScript .*   Core Capabilities: Demonstrates Image creation from a text prompt (using the model ID `gemini-2.5-flash-image-preview`) , Image editing using both text and image inputs while maintaining character consistency , Photo restoration (including restoring and colorizing old photographs from 1932) , and working with multiple input images for complex tasks (e.g., changing clothing) .*   Conversational Editing: Explains how to use a `chats` session for iterative refinement and conversational image editing, noting that starting a new session with a consolidated prompt may be necessary to prevent 'drift' after many edits .*   Best Practices: Provides prompting tips, including the need to be Hyper-Specific, provide Context and Intent, Iterate and Refine, use Step-by-Step Instructions, use Positive Framing, and Control the Camera using photographic terms . The tutorial also provides examples of community-built applications for inspiration .",
  "categories": [
    "Generative AI",
    "Multimodal AI",
    "Image Analysis with AI",
    "AI Applications",
    "Prompt Engineering",
    "Large Language Models (LLMs)",
    "Introduction to AI and Fundamental Concepts",
    "AI Deployment"
  ]
},
{
"name": "How to make your website AI-visible: A step-by-step guide (Leonardo Ubbiali)",
"link": "https://www.linkedin.com/posts/leonardo-ubbiali_ai-crawlers-visibility-guide-activity-7373304284848877568-j-4r?utm_source=share&utm_medium=member_desktop&rcm=ACoAAADvoYgBR6OaUkic3fcx6maIpbJDW9ElVf0",
"difficulty": "Unspecified",
"type": "Course",
"description": "A detailed, step-by-step guide to make websites visible to AI crawlers . It covers why AI crawlers differ from Googlebot (no JavaScript support, no rendering wait) , how to pre-render JS-heavy pages so crawlers see the content, and methods to strip unnecessary bloat like analytics scripts, popups, and widgets . The guide explains how to inject smart metadata and JSON-LD (Article, Product, FAQ) with explicit code examples . It emphasizes that the solution must happen at the edge and details deployment using tools like Cloudflare Workers . Finally, it teaches how to debug AI visibility using curl, schema validators, and prompt testing in ChatGPT . This resource is aimed at professionals working in growth, SEO, or product management .",
"categories": [
"AI Applications",
"AI Deployment",
"Workflow Automation"
]
},
{
"name": "DreamFace - Fast AI Generator",
"link": "https://dreamfaceapp.com",
"difficulty": "Unspecified",
"type": "Tool",
"description": "An AI-powered platform for creating avatar videos, AI videos, and AI photos instantly . It allows users to input text or audio to generate high-quality videos with realistic avatars and voices, eliminating the need for actors or equipment . Additional features include Background Remover, Photo Enhance, Old Photo Restoration, and AI Filter capabilities . Available across mobile and web platforms, with an API for business use .",
"categories": [
"Generative AI",
"AI Applications",
"Content Generation with AI",
"Multimodal AI"
]
},
{
"name": "Thunderbit: AI Web Scraper",
"link": "https://thunderbit.com",
"difficulty": "Unspecified",
"type": "Tool",
"description": "An AI-powered Web Scraper designed for sales and operations teams to scrape leads and other data in two clicks . It features pre-built templates for popular sites (like Amazon and LinkedIn) , and uses AI to restructure, summarize, categorize, and translate the output data . It handles data export to popular tools like Google Sheets and Notion . Use cases span Lead Generation, Competitor Monitoring, Real Estate Aggregation, and Social Media analysis [10-12].",
"categories": [
"Web Scraping with AI",
"AI Applications",
"Workflow Automation",
"Data Analysis",
"AI Automation"
]
},
{
"name": "Macaron – World's First Personal AI Agent",
"link": "http://macaron.im",
"difficulty": "Unspecified",
"type": "Tool",
"description": "The world's first personal AI agent designed to help users 'live better' . It features Deep Memory and grows with the user, remembering details 'like a real friend' . it builds real-life tools and solutions based on simple requests, requiring no frustrating adjustments . The platform is developed using an in-house RL platform that supports large LLMs .",
"categories": [
"AI Agents",
"AI Assistants",
"Large Language Models (LLMs)",
"Agent Memory",
"AI Applications"
]
},
{
"name": "Vapi - Advanced Voice AI Agents API",
"link": "http://vapi.ai",
"difficulty": "Unspecified",
"type": "Tool",
"description": "An API-first platform for developers to build and deploy highly configurable Voice AI agents . It supports inbound calls (powering 400,000+ daily calls) , multilingual communication in over 100 languages , and tool calling to integrate external APIs and perform actions . It is built for enterprise scale, featuring 99.99% uptime, sub-500ms latency, AI guardrails, automated testing, and compliance (SOC2, HIPAA, PCI) [17-19].",
"categories": [
"AI Agents",
"AI Deployment",
"Systems Integration",
"AI Applications",
"Large Language Models (LLMs)",
"Workflow Automation"
]
},
{
"name": "Lovart | The Design Agent",
"link": "http://lovart.ai",
"difficulty": "Unspecified",
"type": "Tool",
"description": "The world's first Design Agent that automates the entire creative journey, from concept creation to final images, videos, 3D models, and audio . It plans and creates like a professional designer, calling the right tools and responding in dialogue via its ChatCanvas . Lovart can generate a complete brand identity, including logos, colors, and production-ready 3D models and animations, all from a single text prompt .",
"categories": [
"AI Agents",
"Generative AI",
"AI Applications",
"Content Generation with AI",
"Multimodal AI",
"Workflow Automation"
]
},
{
"name": "Droxy: AI-Powered Employee for Customer Interactions",
"link": "http://try.droxy.ai",
"difficulty": "Unspecified",
"type": "Tool",
"description": "An all-in-one platform for building and deploying reliable, customer-facing AI agents across multiple channels (website, phone, messaging, comments) [24-26]. Droxy handles customer messages and calls 24/7  and offers features like Automated Lead Collection, Seamless Human Hand-off, and Smart Product Recommendations . Agents are trained on company knowledge from various sources (websites, PDFs, YouTube)  and use advanced NLP to ensure human-like interactions with a customizable tone and style . It supports communication in over 95 languages .",
"categories": [
"AI Agents",
"AI Applications",
"Workflow Automation",
"Systems Integration",
"Large Language Models (LLMs)",
"Chatbots"
]
},
{
"name": "ChatGPT 'Absolute Mode' Custom Instructions (Prompt Engineering Guide)",
"link": "https://www.reddit.com/r/PromptEngineering/comments/1nei9ev/this_prompt_turned_chatgpt_into_what_it_should_be/",
"difficulty": "Unspecified",
"type": "Course",
"description": "A popular collection of custom system instructions ('Absolute Mode') designed to radically alter ChatGPT's behavior to provide clear, accurate, and to-the-point answers . The goal is to restore independent, high-fidelity thinking in the user, leading toward 'model obsolescence via user self-sufficiency' . The core instructions enforce a blunt, directive style by eliminating soft asks, conversational transitions, emotional softening, emojis, filler, hype, and engagement-boosting behaviors . Replies must terminate immediately after delivering information, with no closures . Complementary instructions emphasized by community members include demanding absolute precision and honesty, prohibiting speculation or invention, explicitly stating when information is inaccessible or unverifiable, and requiring the citation of diverse, verifiable sources and direct URLs . These instructions are added to the ChatGPT settings under 'Custom Instructions' .",
"categories": [
"Prompt Engineering",
"Large Language Models (LLMs)",
"Introduction to AI and Fundamental Concepts",
"Prompt Optimization"
]
},
{
"name": "Genkit: Open-Source AI-Powered App Framework (Firebase/Google)",
"link": "https://github.com/firebase/genkit",
"difficulty": "Unspecified",
"type": "Tool",
"description": "Genkit is an open-source framework for building full-stack AI-powered applications . It was built and is used in production by Google's Firebase . It provides SDKs for JavaScript/TypeScript and Go (both production-ready), and Python (Alpha) . Genkit offers a unified interface for integrating AI models from providers such as Google, OpenAI, Anthropic, and Ollama . Developers can rapidly build and deploy production-ready chatbots, automations, and recommendation systems using streamlined APIs for multimodal content, structured outputs, tool calling, and agentic workflows . Key features include AI workflows, RAG (Retrieval-Augmented Generation), type-safe data generation, and prompt templating . It supports deployment to environments like Cloud Functions for Firebase or Google Cloud Run . Development is accelerated via a local CLI and Developer UI for prompt testing, debugging with detailed execution traces, and rapid iteration . It also supports production monitoring to track performance and error rates . The project has 4.3k stars and 484 forks  and is licensed under Apache-2.0 .",
"categories": [
"AI Applications",
"AI Deployment",
"Large Language Models (LLMs)",
"AI Agents",
"RAG (Retrieval-Augmented Generation)",
"Multimodal AI",
"Workflow Automation",
"Systems Integration",
"Prompt Engineering",
"Agent Development",
"LLMOps"
]
},
{
"name": "Alibaba WAN 2.5 Model Collection (WaveSpeedAI)",
"link": "https://wavespeed.ai/collections/wan-2-5",
"difficulty": "Unspecified",
"type": "Tool",
"description": "WAN 2.5 is a state-of-the-art text/image-to-video generation model from Alibaba, available on the DashScope platform . It converts text or images into lip-synced HD videos (480p, 720p, or 1080p resolution) in a single step . The models produce high-quality videos complete with synchronized audio from simple text or image prompts . Compared to Google Veo3, WAN 2.5 is designed to be more affordable and offers faster video generation speeds . Key advantages include One-pass A/V sync to generate video with voiceover and lip-sync in a single run ; reliable multilingual A/V sync for Chinese and minor languages ; longer, more flexible output (up to 10 seconds compared to ~8 seconds on Veo 3) ; and support for audio-driven control using voice, sound effects (SFX), or background music (BGM) as generation references . The collection includes models for `text-to-video`, `image-to-video`, and faster variants of both, as well as `text-to-image` . It is suitable for creating Shorts (6–10s hooks for TikTok/Reels), Ads & E-commerce (product hero shots + CTA), and Explainers/Tutorials (step-by-step with on-beat voiceover) .",
"categories": [
"Generative AI",
"Content Generation with AI",
"Multimodal AI",
"AI Deployment",
"AI Applications",
"Image Analysis with AI"
]
},
{
"name": "Python for Data Analysis, 3E (Open Access Edition)",
"link": "https://wesmckinney.com/book/",
"difficulty": "Unspecified",
"type": "Book",
"description": "The 3rd edition of Python for Data Analysis is available as an 'Open Access' HTML version on this site . This edition was initially published in August 2022  and has been updated for pandas 2.0.0 and Python 3.10 . The content updates primarily focus on reflecting changes within the pandas library since 2017 . The code examples referenced in the book are MIT-licensed and can be found on GitHub or Gitee, along with the supporting datasets . The updates, such as the one for pandas 2.0.0, are fixed periodically .",
"categories": [
"Data Analysis"
]
},
{
"name": "Prompt Manual, Version 3 (Alfredo Vela Zancada)",
"link": "https://www.linkedin.com/posts/alfredovela_manual-del-prompt-versi%C3%B3n-3-activity-7377224206230511617-rj5s?utm_source=share&utm_medium=member_desktop&rcm=ACoAAADvoYgBR6OaUkic3fcx6maIpbJDW9ElVf0",
"difficulty": "Unspecified",
"type": "Book",
"description": "This is the 3rd version of the 'Prompt Manual' (ebook), released to help users create quality prompts and obtain better results from Generative AI tools like ChatGPT . The manual contains 17% more content than its previous version . It offers advice intended to assist both those already working in the world of AI and those taking their first steps, whether for professional or personal activities . Topics suggested for inclusion emphasize advanced prompting techniques, such as asking the AI to find all flaws, risks, and blind spots in an idea or project , and the importance of learning to prompt the AI to generate a prompt itself, leveraging the idea that the AI knows itself best . Other suggestions include ensuring prompts use unambiguous, complete examples .",
"categories": [
"Prompt Engineering",
"Large Language Models (LLMs)",
"Introduction to AI and Fundamental Concepts",
"AI Applications",
"Generative AI",
"Prompt Optimization"
]
},
{
"name": "Intelligent World 2035",
"link": "https://www.huawei.com/en/giv",
"difficulty": "Unspecified",
"type": "Book",
"description": "This report explores the technological exploration driving civilization toward a more intelligent world, highlighting how Generative AI is opening up possibilities previously unimaginable . It details 10 technological leaps leading to 2035, including the path to AGI (Artificial General Intelligence) through 'going physical'  and the role of Embodied AI in extending AI into the physical world . Key trends include AI agents driving an industrial revolution , the evolution of the mobile Internet into multi-agent collaboration , and the necessity of an Agentic Internet to connect the expected 900 billion AI agents by 2035 . The report also covers profound shifts in industries like Healthcare (shifting from treatment to protection via AI intervention) , Manufacturing (using AI for generative design and intelligent factories) , Finance (implementing 'human + agent' collaboration models) , and Education (AI integrated for personalized learning) . Furthermore, it discusses rethinking computing beyond the von Neumann architecture , the shift of data toward 'intelligence' , and the vital necessity of addressing AI ethics and cybersecurity to ensure AI serves humanity .",
"categories": [
"Introduction to AI and Fundamental Concepts",
"Generative AI",
"AI Agents",
"AI Applications",
"AI Deployment",
"Workflow Automation",
"AI Ethics"
]
},
{
"name": "Microsoft Agent Framework (GitHub Repository)",
"link": "https://github.com/microsoft/agent-framework",
"difficulty": "Unspecified",
"type": "Tool",
"description": "Microsoft's comprehensive multi-language framework for building, orchestrating, and deploying AI agents . It provides full support for both Python and C#/.NET implementations with consistent APIs . The framework supports everything from simple chat agents to complex multi-agent workflows orchestrated using a graph-based system . This graph system connects agents and deterministic functions and features streaming, checkpointing, human-in-the-loop, and time-travel capabilities . Key features include built-in OpenTelemetry integration for distributed tracing, monitoring, and debugging (Observability) , flexible middleware for request/response processing , and support for multiple LLM providers . It also includes DevUI, an interactive developer UI for agent testing and debugging workflows , and experimental packages for cutting-edge features in AF Labs, such as benchmarking and reinforcement learning . The project has 2.3k stars and 244 forks and is licensed under the MIT license .",
"categories": [
"AI Agents",
"Agent Development",
"Multi-Agent Systems",
"Workflow Automation",
"AI Deployment",
"Systems Integration",
"Large Language Models (LLMs)",
"LLMOps"
]
}


]
