[
  {
    "name": "Google AI Courses",
    "link": "https://www.cloudskillsboost.google/paths/118",
    "difficulty": "Beginner",
    "type": "Course",
    "description": "A series of 5 courses offered by Google to learn about generative AI from the ground up, starting with an introduction to AI and progressing to a solid understanding of the field .",
    "categories": [
      "Introduction to AI and Fundamental Concepts",
      "Generative AI"
    ]
  },
  {
    "name": "Microsoft AI Course",
    "link": "https://microsoft.github.io/AI-For-Beginners/",
    "difficulty": "Beginner",
    "type": "Course",
    "description": "Covers the basics of AI, as well as more advanced topics such as neural networks and deep learning .",
    "categories": [
      "Introduction to AI and Fundamental Concepts",
      "Machine Learning",
      "Deep Learning"
    ]
  },
  {
    "name": "Introduction to AI with Python (Harvard University)",
    "link": "https://www.edx.org/learn/artificial-intelligence/harvard-university-cs50-s-introduction-to-artificial-intelligence-with-python",
    "difficulty": "Beginner",
    "type": "Course",
    "description": "A comprehensive 7-week course that explores the fundamental concepts and algorithms of AI, concluding with knowledge of AI principles and machine learning libraries .",
    "categories": [
      "Introduction to AI and Fundamental Concepts",
      "Machine Learning"
    ]
  },
  {
    "name": "Generative AI for Everyone",
    "link": "https://www.deeplearning.ai/courses/generative-ai-for-everyone/",
    "difficulty": "Beginner",
    "type": "Course",
    "description": "A free resource to get started with Generative AI .",
    "categories": [
      "Introduction to AI and Fundamental Concepts",
      "Generative AI"
    ]
  },
  {
    "name": "Getting Started with LLMs",
    "link": "https://www.deeplearning.ai/short-courses/getting-started-with-mistral/",
    "difficulty": "Beginner",
    "type": "Course",
    "description": "A free resource to begin working with Large Language Models (LLMs) .",
    "categories": [
      "Introduction to AI and Fundamental Concepts",
      "Large Language Models (LLMs)"
    ]
  },
  {
    "name": "AI Python for Beginners",
    "link": "https://www.deeplearning.ai/short-courses/ai-python-for-beginners/",
    "difficulty": "Beginner",
    "type": "Course",
    "description": "A free resource focused on teaching Python for beginners in the context of AI .",
    "categories": [
      "Introduction to AI and Fundamental Concepts"
    ]
  },
  {
    "name": "OpenAI AI Academy - OpenAI, LLMs & ChatGPT",
    "link": "https://academy.openai.com/public/collections",
    "difficulty": "Beginner",
    "type": "Course",
    "description": "Part of OpenAI Academy's free course offerings, introducing users to OpenAI, LLMs, and ChatGPT .",
    "categories": [
      "Introduction to AI and Fundamental Concepts",
      "Large Language Models (LLMs)"
    ]
  },
  {
    "name": "OpenAI AI Academy - Introduction to GPTs",
    "link": "https://academy.openai.com/public/collections/chatgpt-at-work-2025-02-14",
    "difficulty": "Beginner",
    "type": "Course",
    "description": "Another course from the OpenAI Academy, focused on introducing GPT models .",
    "categories": [
      "Introduction to AI and Fundamental Concepts",
      "Generative AI"
    ]
  },
  {
    "name": "Big Data, Artificial Intelligence, and Ethics (University of California - Davis)",
    "link": "https://www.coursera.org/learn/big-data-ai-ethics",
    "difficulty": "Intermediate",
    "type": "Course",
    "description": "A 4-module course covering Big Data opportunities, introducing IBM Watson, and addressing the limitations of AI .",
    "categories": [
      "Introduction to AI and Fundamental Concepts",
      "Big Data",
      "AI Ethics"
    ]
  },
  {
    "name": "Deep Reinforcement Learning Course (Hugging Face)",
    "link": "https://huggingface.co/learn/deep-rl-course/unit0/introduction",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "A free and open-source course that teaches Deep Reinforcement Learning from beginner to expert . It includes theoretical and practical content with popular libraries like Stable Baselines3, as well as agent training in various environments . Please note that this course is in a low-maintenance state, and some features like 'AI vs AI' challenges and the 'Leaderboard' are not operational . It recommends dedicating 3-4 hours per week per chapter .",
    "categories": [
      "Introduction to AI and Fundamental Concepts",
      "Deep Learning",
      "AI Agents"
    ]
  },
  {
    "name": "Prompt Engineering for ChatGPT (Vanderbilt University / Coursera)",
    "link": "https://www.coursera.org/learn/prompt-engineering",
    "difficulty": "Beginner",
    "type": "Course",
    "description": "A 6-module course designed for beginners looking to improve their skills for writing effective prompts .",
    "categories": [
      "Prompt Engineering"
    ]
  },
  {
    "name": "AI Applications and Prompt Engineering (edX)",
    "link": "https://www.edx.org/learn/computer-programming/edx-ai-applications-and-prompt-engineering",
    "difficulty": "Beginner",
    "type": "Course",
    "description": "An introductory course that not only covers the basics of prompt engineering but also teaches how to apply them to create AI applications .",
    "categories": [
      "Prompt Engineering"
    ]
  },
  {
    "name": "OpenAI AI Academy - Introduction to Prompt Engineering",
    "link": "https://academy.openai.com/public/videos/introduction-to-prompt-engineering-2025-02-13",
    "difficulty": "Beginner",
    "type": "Course",
    "description": "A fundamental course from the OpenAI Academy to learn the basics of prompt engineering .",
    "categories": [
      "Prompt Engineering"
    ]
  },
  {
    "name": "Prompt Engineering Guide (GitHub Repo)",
    "link": "https://github.com/dair-ai/Prompt-Engineering-Guide",
    "difficulty": "Beginner",
    "type": "Course",
    "description": "A comprehensive repository containing guides, articles, classes, notebooks, and other resources for learning and mastering prompt engineering .",
    "categories": [
      "Prompt Engineering"
    ]
  },
  {
    "name": "Anthropic's Interactive Prompt Engineering Tutorial - Beginner Chapters",
    "link": "https://github.com/anthropics/prompt-eng-interactive-tutorial",
    "difficulty": "Beginner",
    "type": "Course",
    "description": "This interactive tutorial from Anthropic includes initial chapters on basic prompt structure, being clear and direct, and assigning roles to the model .",
    "categories": [
      "Prompt Engineering"
    ]
  },
  {
    "name": "Anthropic's Interactive Prompt Engineering Tutorial - Intermediate Chapters",
    "link": "https://github.com/anthropics/prompt-eng-interactive-tutorial",
    "difficulty": "Intermediate",
    "type": "Course",
    "description": "Includes chapters on how to separate data from instructions, format model output and 'speak for Claude', the use of precognition ('thinking step by step'), and the utilization of examples .",
    "categories": [
      "Prompt Engineering"
    ]
  },
  {
    "name": "ChatGPT Prompt Engineering for Devs (OpenAI / DeepLearning AI)",
    "link": "https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/",
    "difficulty": "Intermediate",
    "type": "Course",
    "description": "A collaborative course between OpenAI and DeepLearning AI, focusing on best practices and hands-on experience in prompt engineering for developers .",
    "categories": [
      "Prompt Engineering"
    ]
  },
  {
    "name": "Anthropic's Interactive Prompt Engineering Tutorial - Advanced Chapters",
    "link": "https://github.com/anthropics/prompt-eng-interactive-tutorial",
    "difficulty": "Advanced",
    "type": "Course",
    "description": "This section addresses advanced techniques such as avoiding hallucinations and the construction of complex prompts for industry use cases .",
    "categories": [
      "Prompt Engineering"
    ]
  },
  {
    "name": "OpenAI AI Academy - Advanced Prompt Engineering",
    "link": "https://academy.openai.com/public/videos/advanced-prompt-engineering-2025-02-13",
    "difficulty": "Advanced",
    "type": "Course",
    "description": "A course within the OpenAI Academy that focuses on advanced prompt engineering techniques .",
    "categories": [
      "Prompt Engineering"
    ]
  },
  {
    "name": "AI Agents for Beginners (GitHub Repo)",
    "link": "https://github.com/microsoft/ai-agents-for-beginners",
    "difficulty": "Beginner",
    "type": "Course",
    "description": "A free 11-lesson course specifically designed for beginners who want to start building AI agents .",
    "categories": [
      "AI Agents"
    ]
  },
  {
    "name": "AI Agentic Design Patterns with AutoGen",
    "link": "https://www.deeplearning.ai/short-courses/ai-agentic-design-patterns-with-autogen/",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "A course available through DeepLearning AI that explores design patterns for AI agents using the AutoGen library .",
    "categories": [
      "AI Agents"
    ]
  },
  {
    "name": "Multi AI Agent Systems with crewAI",
    "link": "https://www.deeplearning.ai/short-courses/multi-ai-agent-systems-with-crewai/",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "A course of DeepLearning AI that focuses on the creation of systems with multiple AI agents using crewAI .",
    "categories": [
      "AI Agents"
    ]
  },
  {
    "name": "AI Agents in LangGraph",
    "link": "https://www.deeplearning.ai/short-courses/ai-agents-in-langgraph/",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "A course of DeepLearning AI that explores the use of LangGraph to build AI agents.",
    "categories": [
      "AI Agents"
    ]
  },
  {
    "name": "Fundamentals of AI agents using RAG and LangChain",
    "link": "https://www.coursera.org/learn/fundamentals-of-ai-agents-using-rag-and-langchain",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "A resource for learning the fundamentals of AI agents integrating RAG and LangChain .",
    "categories": [
      "AI Agents",
      "RAG (Retrieval-Augmented Generation)"
    ]
  },
  {
    "name": "Building Agentic RAG with LlamaIndex",
    "link": "https://www.deeplearning.ai/short-courses/building-agentic-rag-with-llamaindex/",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "A course of DeepLearning AI focused on the construction of RAG systems with agentic capabilities using LlamaIndex .",
    "categories": [
      "AI Agents",
      "RAG (Retrieval-Augmented Generation)"
    ]
  },
  {
    "name": "GenAI Agents (GitHub Repo)",
    "link": "https://github.com/NirDiamant/GenAI_Agents",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "A GitHub repository that provides tutorials and implementations for various Generative AI Agent techniques, covering from basic to advanced concepts .",
    "categories": [
      "AI Agents",
      "Generative AI"
    ]
  },
  {
    "name": "n8n AI Agents Tutorial (8-hour video)",
    "link": "https://youtu.be/Ey18PDiaAYI",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "A detailed video tutorial that teaches how to build AI agents using the n8n platform. It covers practical topics such as the creation of a RAG Chatbot, automation of Customer Support, and automated content generation for LinkedIn .",
    "categories": [
      "AI Agents",
      "RAG (Retrieval-Augmented Generation)",
      "Data Analysis and Specific Tool Usage",
      "Other AI Topics and Applications"
    ]
  },
  {
    "name": "Serverless Agentic Workflows with Amazon Bedrock (DeepLearning AI)",
    "link": "https://www.deeplearning.ai/short-courses/serverless-agentic-workflows-with-amazon-bedrock/",
    "difficulty": "Advanced",
    "type": "Course",
    "description": "A DeepLearning AI course on implementing serverless agentic workflows using Amazon Bedrock .",
    "categories": [
      "AI Agents",
      "LLMOps"
    ]
  },
  {
    "name": "Agentic Pattern Course by Miguel Otero",
    "link": "https://www.youtube.com/playlist?list=PLacQJwuclt_sK_pUPzBpfeWyiL1QOSMRQ",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "A youtube course based on practice and real-world projects for learning agentic patterns .",
    "categories": [
      "AI Agents"
    ]
  },
  {
    "name": "LLMOps (Google Cloud / DeepLearning AI)",
    "link": "https://www.deeplearning.ai/short-courses/llmops/",
    "difficulty": "Advanced",
    "type": "Course",
    "description": "A DeepLearning AI course in collaboration with Google Cloud that delves into the LLMOps pipeline, from pre-processing training data to adapting a supervised tuning pipeline to train and deploy a custom LLM .",
    "categories": [
      "LLMOps",
      "Large Language Models (LLMs)"
    ]
  },
  {
    "name": "OpenAI AI Academy - ChatGPT for Data Analysis",
    "link": "https://academy.openai.com/public/videos/chatgpt-for-data-analysis-2025-02-13",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "A course from the OpenAI Academy focused on using ChatGPT for data analysis tasks .",
    "categories": [
      "Data Analysis and Specific Tool Usage",
      "Large Language Models (LLMs)"
    ]
  },
  {
    "name": "ChatGPT Advanced Data Analysis (Coursera)",
    "link": "https://www.coursera.org/learn/chatgpt-advanced-data-analysis",
    "difficulty": "Beginner",
    "type": "Course",
    "description": "Mentioned in relation to the Code Interpreter .",
    "categories": [
      "Data Analysis and Specific Tool Usage",
      "Large Language Models (LLMs)"
    ]
  },
  {
    "name": "Hands on Large Language Models (GitHub Repo)",
    "link": "https://github.com/HandsOnLLM/Hands-On-Large-Language-Models",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "This repository contains code examples that complement the book 'Hands-On Large Language Models,' covering from the introduction to language models to fine-tuning techniques .",
    "categories": [
      "Other AI Topics and Applications",
      "Large Language Models (LLMs)"
    ]
  },
  {
    "name": "Prompt Engineering for VLMS",
    "link": "https://www.deeplearning.ai/short-courses/prompt-engineering-for-vision-models/",
    "difficulty": "Beginner",
    "type": "Course",
    "description": "A free resource for prompt engineering with Visual Language Models (VLMs) .",
    "categories": [
      "Prompt Engineering"
    ]
  },
  {
    "name": "Generative AI for Everyone",
    "link": "https://www.deeplearning.ai/courses/generative-ai-for-everyone/",
    "difficulty": "Beginner",
    "type": "Course",
    "description": "A free resource to get started with Generative AI.",
    "categories": [
      "Introduction to AI and Fundamental Concepts",
      "Generative AI"
    ]
  },
  {
    "name": "Generative AI Specialization for Data Analysts",
    "link": "https://www.coursera.org/specializations/generative-ai-for-data-analysts",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "A specialization focused on Generative AI for data analysts.",
    "categories": [
      "Generative AI",
      "Data Analysis"
    ]
  },
  {
    "name": "Microsoft AI Product Manager Professional Certificate",
    "link": "https://www.coursera.org/professional-certificates/microsoft-ai-product-manager",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "A professional certificate program for AI product managers.",
    "categories": [
      "AI Applications"
    ]
  },
  {
    "name": "Generative AI with Large Language Models",
    "link": "https://www.coursera.org/learn/generative-ai-with-llms",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "A course focusing on generative AI using Large Language Models.",
    "categories": [
      "Generative AI",
      "Large Language Models (LLMs)"
    ]
  },
  {
    "name": "Generative AI: Fundamentals of Signal Engineering",
    "link": "https://www.coursera.org/learn/generative-ai-prompt-engineering-for-everyone",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "A course covering the fundamentals of signal engineering in the context of Generative AI.",
    "categories": [
      "Generative AI",
      "Introduction to AI and Fundamental Concepts"
    ]
  },
  {
    "name": "IBM AI Developer Professional Certificate",
    "link": "https://www.coursera.org/professional-certificates/applied-artifical-intelligence-ibm-watson-ai",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "A professional certificate program for AI developers.",
    "categories": [
      "AI Applications"
    ]
  },
  {
    "name": "Machine Learning Specialization",
    "link": "https://www.coursera.org/specializations/machine-learning-introduction",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "A specialization focused on Machine Learning.",
    "categories": [
      "Machine Learning"
    ]
  },
  {
    "name": "Generative AI Specialization for Data Engineers",
    "link": "https://www.coursera.org/specializations/generative-ai-for-data-engineers",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "A specialization focused on Generative AI for data engineers.",
    "categories": [
      "Generative AI",
      "Data Analysis"
    ]
  },
  {
    "name": "Generative AI Specialization for Data Scientists",
    "link": "https://www.coursera.org/specializations/generative-ai-for-data-scientists",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "A specialization focused on Generative AI for data scientists.",
    "categories": [
      "Generative AI",
      "Data Analysis"
    ]
  },
  {
    "name": "IBM Generative AI Specialization for Cybersecurity Professionals",
    "link": "https://www.coursera.org/specializations/generative-ai-for-cybersecurity-professionals",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "An IBM specialization focused on Generative AI for cybersecurity professionals.",
    "categories": [
      "Generative AI",
      "AI Applications"
    ]
  },
  {
    "name": "Generative AI Specialization for Software Developers",
    "link": "https://www.coursera.org/specializations/generative-ai-for-software-developers",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "A specialization focused on Generative AI for software developers.",
    "categories": [
      "Generative AI",
      "AI Applications"
    ]
  },
  {
    "name": "Google AI Fundamentals Specialization",
    "link": "https://www.cloudskillsboost.google/paths/118",
    "difficulty": "Beginner",
    "type": "Course",
    "description": "A series of 5 courses offered by Google to learn about generative AI from the ground up, starting with an introduction to AI and progressing to a solid understanding of the field .",
    "categories": [
      "Introduction to AI and Fundamental Concepts",
      "Generative AI"
    ]
  },
  {
    "name": "Automation with Generative AI Specialization",
    "link": "https://www.coursera.org/specializations/generative-ai-automation",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "A specialization focused on automation using Generative AI.",
    "categories": [
      "Workflow Automation",
      "Generative AI"
    ]
  },
  {
    "name": "Introduction to Artificial Intelligence (AI)",
    "link": "https://www.coursera.org/learn/introduction-to-ai",
    "difficulty": "Beginner",
    "type": "Course",
    "description": "An introductory course to Artificial Intelligence. The specific destination link for this general course title is not explicitly provided in the source material .",
    "categories": [
      "Introduction to AI and Fundamental Concepts"
    ]
  },
  {
    "name": "MCP: Build Rich-Context AI Apps with Anthropic",
    "link": "https://learn.deeplearning.ai/courses/mcp-build-rich-context-ai-apps-with-anthropic",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "This course, built in partnership with Anthropic, teaches the core concepts of the Model Context Protocol (MCP) and how to implement it in your AI applications. MCP is an open protocol that standardizes how Large Language Model (LLM) applications can get access to context in terms of tools and data resources, based on a client-server architecture. It defines how communication takes place between an MCP client, hosted inside your own LLM application, and an MCP server that exposes tools, data resources, and prompt templates to your application. MCP originated as part of an internal Anthropic project to extend the capabilities of Claude Desktop, allowing it to interact with local file systems and other external systems. The protocol was found useful in many AI applications and was subsequently published as open source to make it available to more developers.The MCP ecosystem is growing rapidly and includes a number of MCP services developed by the open-source community and Anthropic's MCP team . It is model agnostic and designed to be easy to plug into multiple applications. For instance, a research assistant agent could connect to GitHub, Google Drive, and File System services via MCP to access and summarize data, without you having to write custom LLM tools.The course curriculum includes a deep dive into the MCP client-server architecture, making a chatbot application MCP compatible, building and testing an MCP server, and connecting the chatbot to it. You will also learn to connect your chatbot to other trusted third-party servers to extend its capabilities, re-use your MCP server with other MCP applications like Claude Desktop, and deploy your MCP server remotely. The instructor for this course is Elie Schoppik, Head of Technical Education at Anthropic. This technology significantly eases the process for LLM application developers to connect their systems to various tools and data resources, and for tool/data providers to make their offerings accessible to many developers.",
    "categories": [
      "MCP (Model Context Protocol)",
      "Large Language Models (LLMs)",
      "AI Agents",
      "AI Deployment"
    ]
  },
  {
    "name": "#2 Programming 'Hello World' with MCP: Python implementation and integration with Claude AI",
    "link": "https://www.youtube.com/watch?v=9-v2WOby2-Q&list=PLnNbmcjjevxsnAfDfQmDmhBYBWAt0rbMs&index=2",
    "difficulty": "Intermediate",
    "type": "Course",
    "description": "This youtube video provides a step-by-step guide on how to create a Model Context Protocol (MCP) client and server from scratch using Python. It covers the essential setup, including initializing a Python environment, creating a virtual environment, and installing necessary libraries such as FasMCP, MCP_Types, asyncio, and Pathlib. The tutorial demonstrates how to build an MCP server, defining tools (e.g., a 'Hello World' tool) and their descriptions, which are crucial for clients and Large Language Models (LLMs) to understand when to call them. It then walks through the process of creating an MCP client that connects to the server, makes requests to its tools, and processes the text-based responses. The video also includes a section on error handling for robust application development. A key highlight is the demonstration of how to integrate the custom-built MCP server with Claude Desktop by configuring the claude_desktop_config file, allowing Claude AI to interact with your local MCP server and its defined tools. This practical 'Hello World' example serves as a fundamental base for developing more complex AI applications that leverage the MCP client-server architecture. The MCP itself is an open protocol that standardizes how LLM applications can access context, tools, and data resources through a client-server model.",
    "categories": [
      "MCP (Model Context Protocol)",
      "AI Agents",
      "Large Language Models (LLMs)",
      "AI Deployment",
      "Systems Integration",
      "Workflow Automation"
    ]
  },
  {
    "name": "David Kriesel’s A Brief Introduction to Neural Networks",
    "link": "https://www.linkedin.com/posts/michael-erlihson-phd-8208616_a-brief-introduction-to-neural-networks-ugcPost-7342131671187501056-B_xA/",
    "difficulty": "Unspecified",
    "type": "Book",
    "description": "A mathematically grounded text that delves into the foundations of neural networks . It explains how neural networks actually work, layer by layer, and equation by equation, without assuming hype or shortcuts . The book covers a range of topics, from biology to perceptrons, backpropagation, RBFs, SOMs, Hopfield nets, and ART, providing clear derivations and precise illustrations . It is notable for balancing rigor and clarity, making it comprehensible for both math-inclined readers who can follow the formalism directly, and non-math-inclined readers who are carried forward by its prose . This resource also includes a complete Java neural network framework (SNIPE) for practical learning through coding . It addresses classic and less common architectures carefully, including delta rules, gradient procedures, growing RBF networks, and reinforcement learning with real examples . This foundational work is highly recommended for building a serious intuition about neural networks, rather than merely tuning them, and is noted to not age despite not being a new book .",
    "categories": [
      "Introduction to AI and Fundamental Concepts",
      "Deep Learning",
      "Machine Learning",
      "AI Agents"
    ]
  },
  {
    "name": "AI Frontier",
    "link": "https://www.linkedin.com/newsletters/ai-frontier-6861893793654935552/",
    "difficulty": "Unspecified",
    "type": "Newsletter",
    "description": "This is a witty, informative newsletter featuring curated AI news, the latest innovations and learning resources. It is published weekly and created by Steve Nouri, who is known for building the largest AI community, being an advisor to Fortune 500 companies, having 2 million followers, and being a keynote speaker.",
    "categories": [
      "Introduction to AI and Fundamental Concepts",
      "AI Applications"
    ]
  },
  {
    "name": "IBM AI Newsletter",
    "link": "https://www.linkedin.com/newsletters/ai-newsletter-7321517150936920066/",
    "difficulty": "Unspecified",
    "type": "Newsletter",
    "description": "This newsletter provides the latest in AI from IBM and the open source world, curated and delivered every two weeks. It is created by Jacobo Garnacho Pérez, who is a Principal Data Platform Sales Manager at IBM and a University Professor, with a passion for AI and Quantum. Featured editions include IBM AI June Highlights and Latest AI News, as well as coverage of IBM THINK 2025.",
    "categories": [
      "Introduction to AI and Fundamental Concepts",
      "AI Applications"
    ]
  },
  {
    "name": "Brain News",
    "link": "https://www.linkedin.com/newsletters/brain-news-6877555549521616896/",
    "difficulty": "Unspecified",
    "type": "Newsletter",
    "description": "This newsletter covers Marketing, Design, Business, and a wide variety of other topics ('Newsletter de todo lo que leo sobre Marketing, Diseño, Negocios y un largo etcétera') . It is published weekly  and is created by Andrés Karp, who is identified as a LinkedIn Top Voice specializing in Strategic Marketing, Artificial Intelligence, and Data . Past editions of the newsletter include a series titled ʙʀᴀɪɴs ɴᴇᴡs ʙᴏᴏᴋ: ʙᴜɪʟᴛ ɪɴ ᴘᴜʙʟɪᴄ .",
    "categories": [
      "Introduction to AI and Fundamental Concepts",
      "AI Applications",
      "Data Analysis"
    ]
  },
  {
    "name": "ACI.dev",
    "link": "https://github.com/aipotheosis-labs/aci",
    "difficulty": "Unspecified",
    "type": "Tool",
    "description": "ACI.dev is an open-source platform designed to connect AI agents to any tool for VibeOps and Agent Workflows . It provides capabilities for building AI agents through function calling or its unified Model Context Protocol (MCP) server . The platform is model agnostic  and aims to solve complex AI agent engineering challenges by offering multi-tenant authentication management and granular permissions .Key features of ACI.dev include:   Unified MCP Server: Allows connecting an MCP client to over 600+ integrations through a single connection, enabling AI agents to search, plan, and execute tasks based on intent .   Workflow Discovery: Grants AI agents the flexibility to dynamically discover the best tools and workflows for their current task .   Secure Authentication / Managed Agent Authentication: Facilitates end-users authorizing their AI agents with account access via OAuth, with ACI.dev handling token management and OAuth client setups .   Secure Agent Secrets Manager: Provides a mechanism for end-users to securely store and manage credentials for web browsing AI agents, with customizable access policies .   Natural Language Permissions: Enables setting granular permissions using natural language, which helps prevent erroneous API execution and significantly improves agent reliability .Pre-built tool integrations include popular services like Gmail, HubSpot, Notion, and Slack . ACI.dev aims to provide everything necessary to create production-ready AI agents that can securely interact with various tools and services .",
    "categories": [
      "AI Agents",
      "Agent Development",
      "MCP (Model Context Protocol)",
      "Systems Integration",
      "Workflow Automation",
      "AI Deployment"
    ]
  },
  {
    "name": "n8n Workflows Collection (DragonJAR)",
    "link": "https://github.com/DragonJAR/n8n-workflows-es",
    "difficulty": "Unspecified",
    "type": "Tool",
    "description": "This GitHub repository, a fork of @Zie619's initiative, is a consolidated collection of n8n workflows ('Colección de Flujos de Trabajo de n8n'). It gathers workflows from various sources, including the official n8n.io site and its community forum, public GitHub examples, blogs, and other websites. Its primary purpose is to serve as a resource for inspiration, learning, and reuse of workflows in your own n8n projects .Key features and improvements implemented in this collection include:   Descriptions in Spanish: Each .json file has a clear description in Spanish, detailing the actions it performs .   Duplicate Elimination: A unique hash validation process ensures that each workflow is unique in content .   File Renaming: File names have been updated for more accurate functionality descriptions, aiding in search and selection .To use a workflow, you typically import the desired .json file into your n8n instance, adjust any necessary credentials or webhook URLs, then save and execute . The repository also offers interactive support through a Custom GPT and an MCP (Model Context Protocol) server . The MCP server allows interaction with this knowledge base of documented n8n flows, with instructions for use in various AI-powered tools like Cursor, Claude Desktop, and VSCode . Contributions are welcome via pull requests .The collection features a wide variety of workflows, including those for AI reports, marketing automation, integrations with services like Gumroad, MailerLite, LINE, ChatGPT, Google Drive, Connectwise, Microsoft Teams, Google Calendar, Outlook, Airtable, Notion, LinkedIn, Stripe, HubSpot, Slack, and many more [6-653]. It also includes complex AI-driven automations such as multi-agent AI conversations, AI-powered content creation for social media, email analysis and summarization with AI, AI agents for various tasks (e.g., calendar management, sales, HR), document processing (PDF to text/markdown), image generation and analysis, sentiment analysis, and web scraping.",
    "categories": [
      "n8n",
      "Workflow Automation",
      "Systems Integration",
      "AI Agents",
      "RAG (Retrieval-Augmented Generation)",
      "Large Language Models (LLMs)",
      "AI Applications",
      "Data Analysis",
      "Document Processing",
      "Image Analysis with AI",
      "Web Scraping with AI",
      "MCP (Model Context Protocol)"
    ]
  },
  {
    "name": "The Unwind AI",
    "link": "https://www.theunwindai.com/",
    "difficulty": "Unspecified",
    "type": "Newsletter",
    "description": "The Unwind AI is an online platform that serves as a go-to source for the latest AI news, cutting-edge tools, and in-depth tutorials specifically for AI Developers . It provides real-time updates on the rapidly evolving AI landscape . While not a traditional linear course, its continuous stream of tutorials and news updates provides a learning resource for developers.The platform features a popular \"Daily Unwind\" section that offers concise summaries of impactful developments across various AI domains :   AI Agents & Workflows: Includes discussions on topics such as Microsoft’s Deep Research Agent for Large Codebases, general-purpose AI agents for long-running tasks, Software Development Agents in Your Terminal, Agentic Browsers that see all your tabs, and Multi-Agent AI Systems, including those built with 1000+ tools [1-3]. It also covers AI Super Agents in your browser and always-on AI agents that monitor the web .   Model Context Protocol (MCP): Highlights practical applications and potential, such as connecting React applications to an MCP server in just 3 lines of code, the potential for 500K+ AI apps to act as MCP servers, using an MCP server to help stop AI hallucination, and how to connect AI agents to 10,000+ tools via MCP [1-3]. The platform also notes the availability of an AI agent course as an MCP server and the Hugging Face MCP Server for Models, Datasets, and Papers .   Large Language Models (LLMs): Covers updates on new models like Gemini 2.5 Flash and Mistral's reasoning model, discussions on Apple's perspective on reasoning LLMs, and practical aspects like GitHub making Copilot project-specific .   Emerging AI Tools & Resources: Provides updates on developer-centric tools such as Claude Code now available in VS Code, Apple opensourcing a Docker alternative for Mac, Vibe code browser automation scripts, and free open-source alternatives to various services .   Memory-Augmented Generation & RAG: Explores concepts like an Operating System for Memory-augmented generation and the availability of an opensource plug-and-play RAG stack .In addition to daily news, The Unwind AI offers an \"AI Tutorial\" section that provides step-by-step instructions to empower users to build real-world AI applications . An example tutorial highlighted is on building a fully functional, 100% open-source Agentic RAG App with Reasoning . The platform aims to be an essential resource for developers looking to stay ahead and master AI development .",
    "categories": [
      "AI Agents",
      "MCP (Model Context Protocol)",
      "Large Language Models (LLMs)",
      "RAG (Retrieval-Augmented Generation)",
      "AI Applications",
      "Workflow Automation",
      "Systems Integration",
      "Prompt Engineering",
      "Introduction to AI and Fundamental Concepts"
    ]
  },
  {
    "name": "Firecrawl",
    "link": "https://www.firecrawl.dev/",
    "difficulty": "Unspecified",
    "type": "Tool",
    "description": "Firecrawl is an open-source API and platform designed to transform websites into LLM-ready data . It offers industry-leading web scraping and crawling capabilities to enhance AI applications .Key features and functionalities of Firecrawl include:   Web Scraping and Search: It allows users to search the web and scrape the results with a single API call . It can extract content in markdown and JSON formats, and even capture screenshots .   Advanced Crawling: Firecrawl can gather clean data from all accessible subpages, even without a sitemap . It handles complex web elements such as rotating proxies, orchestration, rate limits, and JavaScript-blocked content, ensuring zero configuration for the user .   Dynamic Content & Reliability: The tool is built for reliability and scales with user needs, handling JavaScript, Single-Page Applications (SPAs), and dynamic content loading with minimal setup . It also features a 'Smart Wait' function to intelligently wait for content to load, making scraping faster and more dependable .   Media Parsing & Actions: Firecrawl can parse and output content from web-hosted PDFs, DOCX, and HTML files . It supports various actions to interact with web pages before extraction, including click, scroll, write, wait, and press .   Integration: It is fully integrated with existing tools and workflows . Developers can easily start using it by installing the @mendable/firecrawl-js package .Firecrawl is used for various AI-powered solutions , including:   AI Chats: Powering AI assistants with real-time, accurate web content .   Lead Enrichment: Enhancing sales data with web information .   MCPs (Model Context Protocols): Adding powerful scraping capabilities to code editors .   AI Platforms: Enabling customers to build AI apps with web data .   Deep Research: Extracting comprehensive information for in-depth research .Users have reported significant benefits, such as simplifying data preparation, being 50 times faster than alternatives like Apify for web scraping, and providing major savings in time and money by reducing token consumption (e.g., saving 2/3 tokens and allowing GPT-3.5 Turbo usage over GPT-4) [7-9]. Firecrawl offers a free plan (500 credits) and various paid plans with increasing credits and concurrent browser support, with transparent pricing and add-ons like auto-recharge and credit packs.",
    "categories": [
      "Web Scraping with AI",
      "Large Language Models (LLMs)",
      "AI Applications",
      "AI Assistants",
      "MCP (Model Context Protocol)",
      "Workflow Automation",
      "Systems Integration",
      "Data Analysis"
    ]
  },
  {
    "name": "Awesome LLM Apps",
    "link": "https://github.com/Shubhamsaboo/awesome-llm-apps",
    "difficulty": "Unspecified",
    "type": "Tool",
    "description": "The Awesome LLM Apps repository is a curated collection of impressive applications built using Large Language Models (LLMs) . These applications leverage various advanced AI concepts including Retrieval Augmented Generation (RAG), AI Agents (both single and multi-agent teams), Model Context Protocol (MCP), and Voice Agents . The repository features apps developed with leading models from OpenAI, Anthropic, Google, and a range of open-source models like DeepSeek, Qwen, or Llama that can be run locally .This collection serves as a valuable resource to:   Discover practical and creative applications of LLMs across diverse domains, from code repositories to email inboxes .   Explore apps that combine LLMs with AI Agents, Agent Teams, MCP, and RAG .   Learn from well-documented projects and contribute to the expanding open-source ecosystem of LLM-powered applications .The repository organizes its featured AI projects into several key categories, showcasing a wide array of functionalities:   AI Agents: Includes a variety of 'Starter AI Agents' (e.g., AI Blog to Podcast Agent, AI Data Analysis Agent, AI Travel Agent) and 'Advanced AI Agents' (e.g., AI Deep Research Agent, AI System Architect Agent, AI Financial Coach Agent), as well as 'Autonomous Game Playing Agents' (e.g., AI Chess Agent) . The collection also features a 'Web Scrapping AI Agent (Local & Cloud)' .   Multi-agent Teams: Highlights systems where multiple AI agents collaborate, such as 'AI Competitor Intelligence Agent Team', 'AI Finance Agent Team', and 'Multimodal Coding Agent Team' .   Voice AI Agents: Focuses on agents designed for voice interaction, including 'AI Audio Tour Agent' and 'Customer Support Voice Agent' .   MCP AI Agents: Demonstrates agents integrated with the Model Context Protocol, such as 'Browser MCP Agent', 'GitHub MCP Agent', and 'Notion MCP Agent' .   RAG (Retrieval Augmented Generation): Features various implementations of RAG, including 'Agentic RAG', 'Corrective RAG (CRAG)', 'Hybrid Search RAG', and 'Local RAG Agent' .   LLM Apps with Memory Tutorials: Provides examples of LLM applications capable of maintaining memory, like 'AI ArXiv Agent with Memory' and 'Local ChatGPT Clone with Memory' .   Chat with X Tutorials: Offers guides on building LLM applications that can chat with various data sources, such as 'Chat with GitHub', 'Chat with Gmail', 'Chat with PDF', and 'Chat with YouTube Videos' .   LLM Fine-tuning Tutorials: Includes resources like 'Llama 3.2 Fine-tuning' .To get started, users can clone the repository, navigate to the desired project directory, install dependencies using pip install -r requirements.txt, and follow the project-specific instructions provided in each README.md file . The repository encourages contributions, inviting users to create issues or submit pull requests for new apps or improvements . The project is primarily developed in Python (59.7%), with significant contributions in JavaScript (31.8%) and TypeScript (7.9%) . It has garnered substantial community support, with 45.7k stars and 5.2k forks .",
    "categories": [
      "AI Agents",
      "RAG (Retrieval-Augmented Generation)",
      "Large Language Models (LLMs)",
      "MCP (Model Context Protocol)",
      "AI Applications",
      "Workflow Automation",
      "Systems Integration",
      "Prompt Engineering",
      "Web Scraping with AI",
      "Data Analysis"
    ]
  },
  {
    "name": "Google ADK (Agent Development Kit) Free Course by Carlos Alarcón",
    "link": "https://www.youtube.com/playlist?list=PLgQnGGtCss_gvACLOw9F-amI_8CFjcVBN",
    "difficulty": "Beginner",
    "type": "Course",
    "description": "This free YouTube course by Carlos Alarcón teaches step-by-step how to build intelligent AI agents capable of executing tasks, using tools, and accessing external information using Google's Agent Development Kit (ADK) . The first three available classes cover an introduction to Google ADK and AI agents, advanced handling of Large Language Models (LLMs), and the creation of tools and custom functions . Future classes are planned to cover Agent-to-Agent (A2A) communication, Model Context Protocol (MCP), and deployments . Each class includes practical examples and provides a GitHub repository with code for hands-on learning . This course is ideal for developers, AI enthusiasts, and teams looking to incorporate AI agents into their workflows .",
    "categories": [
      "AI Agents",
      "Agent Development",
      "Large Language Models (LLMs)",
      "MCP (Model Context Protocol)",
      "AI Deployment",
      "Workflow Automation",
      "Systems Integration",
      "Introduction to AI and Fundamental Concepts",
      "Machine Learning",
      "Multi-Agent Systems"
    ]
  },
  {
    "name": "Anthropic Prompt Engineering Overview Guide",
    "link": "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview",
    "difficulty": "Beginner",
    "type": "Course",
    "description": "This overview guide from Anthropic provides comprehensive insights into prompt engineering for Claude models. It outlines fundamental assumptions before starting prompt engineering, such as having clear success criteria and empirical testing methods. The guide emphasizes that prompt engineering is significantly faster, more resource-efficient, and cost-effective than fine-tuning for controlling model behavior. Key advantages include maintaining model updates, time-saving, minimal data needs, flexibility for rapid iteration, better domain adaptation, improved comprehension of external content like retrieved documents, preservation of general knowledge, and increased transparency. The guide organizes prompt engineering techniques from most broadly effective to more specialized, suggesting a specific order for troubleshooting performance, which includes being clear and direct, using examples (multishot prompting), letting the model think (chain of thought), utilizing XML tags, assigning roles (system prompts), prefilling responses, chaining complex prompts, and tips for long contexts. It also touches upon related topics such as tools (e.g., Bash, Code execution, Web search) and their implementation, the Model Context Protocol (MCP), various use cases like customer support and legal summarization, strategies for testing and evaluation, and methods to strengthen guardrails against issues like hallucinations and prompt leaks. The guide also references interactive tutorials available on GitHub and Google Sheets for a hands-on learning experience.",
    "categories": [
      "Prompt Engineering",
      "Large Language Models (LLMs)",
      "AI Agents",
      "MCP (Model Context Protocol)",
      "AI Deployment",
      "Workflow Automation",
      "Systems Integration",
      "Introduction to AI and Fundamental Concepts",
      "AI Applications"
    ]
  },
  {
    "name": "Microsoft ML for Beginners Curriculum",
    "link": "https://github.com/microsoft/ML-For-Beginners",
    "difficulty": "Beginner",
    "type": "Course",
    "description": "This is a 12-week, 26-lesson curriculum from Microsoft Cloud Advocates, designed for beginners to learn classic Machine Learning. The curriculum includes 52 quizzes and emphasizes a hands-on, project-based pedagogy with a common theme of exploring world cultures through data. It primarily uses Scikit-learn and focuses on traditional ML techniques, intentionally avoiding deep learning (which is covered in their separate 'AI for Beginners' curriculum). Each lesson provides pre- and post-quizzes, written instructions, solutions, assignments, and knowledge checks. Projects progressively increase in complexity, covering topics such as introduction to ML, fairness, regression (e.g., North American pumpkin prices), building web applications, classification (e.g., Asian/Indian cuisines, recommender apps), clustering (e.g., Nigerian Musical Tastes), Natural Language Processing (NLP) including sentiment analysis and translation, Time Series forecasting (ARIMA, SVR), and Reinforcement Learning (Q-Learning, Gym). The course is primarily in Python, with many lessons also available in R (using R Markdown files with '.rmd' extension). Students are encouraged to fork and clone the repository to complete exercises. Supplemental resources, including video walkthroughs on the Microsoft Developer YouTube channel and a PDF version of the curriculum, are available. The repository also has offline access options and encourages community contributions, boasting significant community support with 73.2k stars and 16k forks.",
    "categories": [
      "Machine Learning",
      "Introduction to AI and Fundamental Concepts",
      "AI Applications",
      "Data Analysis",
      "AI Agents"
    ]
  },
  {
    "name": "Neural Networks: Zero to Hero by Andrej Karpathy",
    "link": "https://github.com/karpathy/nn-zero-to-hero",
    "difficulty": "Intermediate",
    "type": "Course",
    "description": "This repository by Andrej Karpathy focuses on building modern deep learning systems from scratch, including the construction of GPTs (Generative Pre-trained Transformers). It is designed for those who have grasped the foundations of AI/ML and are ready to dive deeper into deep learning.",
    "categories": [
      "Deep Learning",
      "Large Language Models (LLMs)",
      "Machine Learning"
    ]
  },
  {
    "name": "DL Paper Implementations",
    "link": "https://github.com/labmlai/annotated_deep_learning_paper_implementations",
    "difficulty": "Intermediate",
    "type": "Course",
    "description": "This repository provides well-documented PyTorch implementations of over 60 research papers covering advanced deep learning architectures such as Transformers, Generative Adversarial Networks (GANs), and Diffusion models. It is suitable for those who have a fundamental understanding of AI, ML, and DL and wish to study how leading architectures work.",
    "categories": [
      "Deep Learning",
      "Large Language Models (LLMs)",
      "Generative AI"
    ]
  },
  {
    "name": "Made With ML",
    "link": "https://github.com/GokuMohandas/Made-With-ML",
    "difficulty": "Intermediate",
    "type": "Course",
    "description": "This resource teaches the entire process of designing, developing, deploying, and iterating on real-world Machine Learning systems. It focuses on practical aspects of MLOps, Continuous Integration/Continuous Delivery (CI/CD), and best practices for moving ML projects from notebooks to production.",
    "categories": [
      "LLMOps",
      "AI Deployment",
      "Machine Learning",
      "AI Applications"
    ]
  },
  {
    "name": "Hands-on LLMs",
    "link": "https://github.com/HandsOnLLM/Hands-On-Large-Language-Models",
    "difficulty": "Intermediate",
    "type": "Course",
    "description": "This visually rich repository provides comprehensive coverage of Large Language Models (LLMs), including topics such as tokenization, fine-tuning, and Retrieval-Augmented Generation (RAG). It is designed for learners who have explored GPTs and LLMs and are looking to apply their knowledge hands-on.",
    "categories": [
      "Large Language Models (LLMs)",
      "RAG (Retrieval-Augmented Generation)",
      "Prompt Engineering",
      "AI Agents"
    ]
  },
  {
    "name": "Advanced RAG Techniques",
    "link": "https://github.com/NirDiamant/RAG_Techniques",
    "difficulty": "Advanced",
    "type": "Course",
    "description": "This repository covers over 30 advanced methods designed to make Retrieval-Augmented Generation (RAG) systems faster, smarter, and more accurate. Techniques like HyDE and GraphRAG are included, making it suitable for those who already have a good grasp of RAG systems and wish to enhance their capabilities.",
    "categories": [
      "RAG (Retrieval-Augmented Generation)",
      "AI Agents",
      "Large Language Models (LLMs)"
    ]
  },
  {
    "name": "AI Agents for Beginners by Microsoft",
    "link": "https://github.com/microsoft/ai-agents-for-beginners",
    "difficulty": "Beginner",
    "type": "Course",
    "description": "This is a hands-on course specifically designed for beginners who want to learn how to build AI agents. It covers fundamental concepts and practical implementation using frameworks such as AutoGen.",
    "categories": [
      "AI Agents"
    ]
  },
  {
    "name": "Agents Towards Production",
    "link": "https://github.com/NirDiamant/agents-towards-production",
    "difficulty": "Advanced",
    "type": "Course",
    "description": "This practical playbook provides guidance on shipping AI agents to production environments. It covers essential aspects such as agent memory management, orchestration, deployment strategies, and security considerations for building robust and deployable AI agents.",
    "categories": [
      "AI Agents",
      "Agent Development",
      "AI Deployment",
      "LLMOps",
      "Agent Memory",
      "Workflow Automation"
    ]
  },
  {
    "name": "AI Engineering Hub",
    "link": "https://github.com/patchy631/ai-engineering-hub",
    "difficulty": "Advanced",
    "type": "Course",
    "description": "This hub offers over 70 real-world examples, tutorials, and agent applications that users can build, adapt, and deploy. It serves as a comprehensive resource for truly mastering Large Language Models (LLMs), Retrieval-Augmented Generation (RAG), and AI agents through practical projects.",
    "categories": [
      "AI Agents",
      "Large Language Models (LLMs)",
      "RAG (Retrieval-Augmented Generation)",
      "AI Applications",
      "Workflow Automation"
    ]
  },
  {
    "name": "Large Language Models: A Survey",
    "link": "https://doi.org/10.48550/arXiv.2402.06196",
    "difficulty": "Advanced",
    "type": "Book",
    "description": "This is a comprehensive research article on Large Language Models (LLMs), authored by Shervin Minaee, Tomas Mikolov, and Richard Socher, among others . It was initially submitted on February 9, 2024, and last revised on March 23, 2025 . The paper explores the rapid evolution of LLMs since the release of ChatGPT in November 2022, highlighting their powerful performance on a wide range of natural language processing tasks .The article delves into how LLMs acquire their general-purpose language understanding and generation capabilities by training billions of parameters on massive amounts of text data, as predicted by scaling laws . The research reviews the most prominent LLM families, including GPT, LLaMA, and PaLM, and discusses their features, contributions, and limitations . It also provides an overview of techniques developed for building and augmenting LLMs . Furthermore, the paper analyzes popular datasets used for training, fine-tuning, and evaluating LLMs, and reviews widely used evaluation metrics. It includes a comparison of the performance of several popular LLMs on a set of representative benchmarks. The paper concludes by discussing open challenges and future research directions in the field of LLMs. The topics covered are Computer Science and Language (cs.CL) and Artificial Intelligence (cs.AI).",
    "categories": [
      "Large Language Models (LLMs)",
      "Deep Learning",
      "Machine Learning",
      "Introduction to AI and Fundamental Concepts",
      "AI Applications"
    ]
  },
  {
    "name": "Postman's AI Agent Builder",
    "link": "https://www.postman.com/explore/mcp-generator",
    "difficulty": "Unspecified",
    "type": "Tool",
    "description": "Postman’s new AI Agent Builder is a powerful platform that enables users to turn any API (from over 100,000 available APIs) into an MCP (Model Context Protocol) server in seconds, with no code required . This tool is designed to simplify the process of connecting existing APIs to AI agents and Large Language Models (LLMs) .Upon creation, the custom MCP server generated by this builder is ready for immediate use in various AI-powered applications such as Cursor, Windsurf, Claude Desktop, Docker, and more .The process for generating and deploying an MCP server is streamlined into three easy steps :1.  Selection and Download: Users begin by accessing the platform via the provided link and can mix and match any desired API endpoints. The builder then generates a custom zip file for download. This comprehensive zip file includes a README with setup instructions, the selected endpoints, all necessary files to run the MCP server locally (compatible with environments like Cursor, Windsurf, and Docker), and an .env file with prefilled variables, requiring only the addition of API keys .2.  Local Setup: Once the zip file is unzipped, users open their terminal, run npm install in the root folder to install dependencies, and then insert their API keys into the .env file .3.  Connection: The final step involves navigating to an MCP-compatible application, such as Postman Desktop, Claude, or Cursor. Users select the MCP option, point to their mcpServer.js file, and click 'connect' to bring their custom MCP server live and operational .This tool has been demonstrated to effectively crawl entire websites and map product URLs using Firecrawl’s map_multiple_urls endpoint via a custom MCP server . Users have also highlighted its utility, with one reporting successfully converting text into audio and saving MP3 files locally using the OpenAI API through this builder . It is recognized as a 'great addition to the ecosystem' for MCP functionality .",
    "categories": [
      "AI Agents",
      "Agent Development",
      "MCP (Model Context Protocol)",
      "Systems Integration",
      "Workflow Automation",
      "AI Deployment",
      "API Management"
    ]
  },
  {
    "name": "Prompt Engineering with Llama Models",
    "link": "https://www.deeplearning.ai/short-courses/prompt-engineering-with-llama-2/",
    "difficulty": "Beginner",
    "type": "Course",
    "description": "A resource focusing on prompt engineering specifically with Llama models .",
    "categories": [
      "Prompt Engineering",
      "Large Language Models (LLMs)"
    ]
  },
  {
    "name": "Structured LLM Output",
    "link": "https://www.deeplearning.ai/short-courses/getting-structured-llm-output/",
    "difficulty": "Beginner",
    "type": "Course",
    "description": "A resource covering techniques for generating structured output from Large Language Models (LLMs) . This often involves specific prompt engineering strategies.",
    "categories": [
      "Large Language Models (LLMs)",
      "Prompt Engineering"
    ]
  },
  
  {
    "name": "LLM Basics and Architecture",
    "link": "https://huggingface.co/learn/llm-course/en/chapter1/1",
    "difficulty": "Intermediate",
    "type": "Course",
    "description": "A resource delving into the fundamental concepts and architectural designs of Large Language Models (LLMs) .",
    "categories": [
      "Large Language Models (LLMs)",
      "Introduction to AI and Fundamental Concepts"
    ]
  },
  {
    "name": "In-depth on LLM Agents",
    "link": "https://llmagents-learning.org/f24",
    "difficulty": "Intermediate",
    "type": "Course",
    "description": "A comprehensive resource for understanding Large Language Model (LLM) agents in detail .",
    "categories": [
      "AI Agents",
      "Large Language Models (LLMs)"
    ]
  },
  {
    "name": "Agent Memory",
    "link": "https://www.deeplearning.ai/short-courses/llms-as-operating-systems-agent-memory/",
    "difficulty": "Intermediate",
    "type": "Course",
    "description": "A resource focused on the concept and implementation of memory mechanisms for AI agents .",
    "categories": [
      "Agent Memory",
      "AI Agents"
    ]
  },
    {
    "name": "Evaluation of AI Agents",
    "link": "https://www.deeplearning.ai/short-courses/evaluating-ai-agents/",
    "difficulty": "Advanced",
    "type": "Course",
    "description": "A resource dedicated to the methods and strategies for evaluating the performance and reliability of AI agents .",
    "categories": [
      "AI Agents"
    ]
  },
  {
    "name": "Multi-Agent System Design",
    "link": "https://www.deeplearning.ai/short-courses/multi-ai-agent-systems-with-crewai/",
    "difficulty": "Advanced",
    "type": "Course",
    "description": "A resource covering the principles and practices of designing systems with multiple collaborating AI agents .",
    "categories": [
      "Multi-Agent Systems",
      "AI Agents"
    ]
  },
  
  {
    "name": "Reliable AI via Guardrails",
    "link": "https://www.deeplearning.ai/short-courses/safe-and-reliable-ai-via-guardrails/",
    "difficulty": "Advanced",
    "type": "Course",
    "description": "A resource focused on ensuring the reliability of AI systems through the implementation of guardrails . This touches upon AI Ethics and practical AI Applications.",
    "categories": [
      "AI Ethics",
      "AI Applications"
    ]
  },

  {
    "name": "No-Code Tools for Building Agents (Demo)",
    "link": "https://www.youtube.com/watch?v=6N5HLxqJ-cY",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "A demo showcasing a full workflow using no-code tools to build AI agents, highlighting their potential for boosting efficiency and scaling operations without needing to code . This falls under workflow automation.",
    "categories": [
      "AI Agents",
      "Workflow Automation"
    ]
  },
  {
  "name": "Gemini for Education",
  "link": "https://edu.google.com/ai/gemini-for-education/",
  "difficulty": "Unspecified",
  "type": "Tool",
  "description": "Gemini for Education is a private and secure generative AI assistant from Google, specifically designed to transform teaching, learning, and administrative tasks for educators, students, and staff . It is included free of charge in all Google Workspace for Education editions for qualifying institutions . Built with Gemini 2.5 Pro, incorporating LearnLM, it is optimized for learning, making it the world's leading model for learning . The tool provides enterprise-grade data protection, ensuring user data is not human-reviewed or used to train AI models and supports compliance with regulations like COPPA, FERPA, HIPPA, and FedRamp . Students under 18 receive a distinct product experience with fine-tuned guardrails, youth AI literacy resources, and trustworthy response features .Key capabilities for teaching include: rapidly drafting lesson plans, differentiating course material, and generating assessments and practice materials . For learning, it offers on-demand support for concepts, personalized practice quizzes for exam prep, and assistance with writing, brainstorming, and research, including citation generation . For administrative work, Gemini helps save time on communications, streamline tasks like summarizing documents and analyzing data, and enhance research efforts .Specific features include:- Deep Research: Provides detailed reports with citations and allows for follow-up questions to sharpen insights .- Gems: Enables users to create or utilize customized AI experts on any topic without coding, acting as a learning coach or brainstorming partner .- Gemini Canvas: An interactive space for creating quizzes, study guides, visual timelines, interactive prototypes, code snippets, and scripts .- Audio Overviews: Converts any file or Deep Research reports into podcasts for on-the-go learning .- Gemini Live: Facilitates real-time voice conversations for brainstorming, concept simplification, and presentation rehearsal, with screen/camera sharing for tailored help .Access to Gemini for Education is available via gemini.google.com or the Gemini mobile app .",
  "categories": [
    "AI Assistants",
    "AI in Education",
    "Generative AI",
    "Large Language Models (LLMs)",
    "AI Applications",
    "Content Generation with AI",
    "AI Ethics",
    "Workflow Automation",
    "Data Analysis"
  ]
},
{
  "name": "Mathematical theory of deep learning",
  "link": "https://doi.org/10.48550/arXiv.2407.18384",
  "difficulty": "Unspecified",
  "type": "Book",
  "description": "This book provides an introduction to the mathematical analysis of deep learning . It covers fundamental results in approximation theory, optimization theory, and statistical learning theory, which are the three main pillars of deep neural network theory . Serving as a guide for students and researchers in mathematics and related fields, the book aims to equip readers with foundational knowledge on the topic . It prioritizes simplicity over generality, and presents rigorous yet accessible results to help build an understanding of the essential mathematical concepts underpinning deep learning . The authors are Philipp Petersen and Jakob Zech . The paper was submitted on 25 Jul 2024 (v1) and last revised on 7 Apr 2025 (v3) .",
  "categories": [
    "Deep Learning",
    "Machine Learning",
    "Introduction to AI and Fundamental Concepts"
  ]
},
{
  "name": "CS229 Lecture Notes",
  "link": "https://cs229.stanford.edu/main_notes.pdf",
  "difficulty": "Unspecified",
  "type": "Book",
  "description": "These lecture notes, authored by Andrew Ng and Tengyu Ma and updated on June 11, 2023, provide a comprehensive introduction to machine learning, covering both foundational concepts and advanced topics. The material is mathematically grounded and delves into the theoretical underpinnings of various algorithms and models.The notes are structured into five main parts:- Supervised Learning: This section covers linear regression, classification (including logistic regression), generalized linear models, generative learning algorithms (Gaussian Discriminant Analysis and Naive Bayes), kernel methods, and Support Vector Machines (SVMs). It explores concepts like cost functions, gradient descent, batch and stochastic gradient descent, normal equations, and probabilistic interpretations of models [2-12].- Deep Learning: This part introduces neural networks, including multi-layer perceptrons (MLPs), various activation functions (e.g., ReLU, sigmoid, tanh, GELU), and modern neural network modules such as residual connections and layer normalization. It details the backpropagation algorithm for efficient gradient computation in neural networks [13-17].- Generalization and Regularization: Here, the notes discuss the critical aspects of model performance on unseen data. Key topics include the bias-variance tradeoff, the double descent phenomenon, sample complexity bounds, different regularization techniques (like L2 regularization/weight decay), and implicit regularization effects of optimizers. Model selection methods such as cross-validation are also covered, along with an overview of Bayesian statistics in regularization [18-26].- Unsupervised Learning: This section explores methods for discovering patterns in unlabeled data. It includes clustering algorithms (e.g., k-means), Expectation-Maximization (EM) algorithms for density estimation (e.g., mixture of Gaussians), Principal Components Analysis (PCA) for dimensionality reduction and noise reduction, and Independent Components Analysis (ICA) for source separation [19, 27-32].- Reinforcement Learning and Control: The final part delves into how agents learn to make sequential decisions to maximize rewards. It defines Markov Decision Processes (MDPs) and covers algorithms like Value Iteration and Policy Iteration. It also addresses continuous state MDPs through discretization and value function approximation, and introduces Linear Quadratic Regulation (LQR), Differential Dynamic Programming (DDP), Linear Quadratic Gaussian (LQG), and Policy Gradient methods (REINFORCE) [33-42].Additionally, the notes touch upon Self-supervised learning and Foundation Models, explaining the paradigm of pretraining and adaptation, pretraining methods in computer vision (supervised and contrastive learning), and pretrained large language models (Transformers, zero-shot learning, and in-context learning) [33, 43-47].",
  "categories": [
    "Introduction to AI and Fundamental Concepts",
    "Machine Learning",
    "Deep Learning",
    "Large Language Models (LLMs)",
    "AI Ethics",
    "AI Agents",
    "RAG (Retrieval-Augmented Generation)",
    "AI Applications",
    "Data Analysis"
  ]
},
{
  "name": "RAGFlow",
  "link": "https://github.com/infiniflow/ragflow",
  "difficulty": "Unspecified",
  "type": "Tool",
  "description": "RAGFlow is an open-source RAG (Retrieval-Augmented Generation) engine based on deep document understanding [1-3]. It provides a streamlined RAG workflow designed for businesses of any scale, combining Large Language Models (LLMs) to deliver truthful question-answering capabilities backed by well-founded citations from various complex formatted data .Key features of RAGFlow include:- Deep document understanding-based knowledge extraction from unstructured data with complicated formats, capable of finding relevant information within unlimited tokens .- Template-based chunking that is intelligent and explainable, offering numerous template options .- Grounded citations with reduced hallucinations, providing visualization of text chunking for human intervention, quick access to key references, and traceable citations .- Compatibility with heterogeneous data sources, supporting a wide array of formats such as Word, slides, Excel, TXT, images, scanned copies, structured data, and web pages .- An automated and effortless RAG workflow that includes configurable LLMs and embedding models, multiple recall paired with fused re-ranking, and intuitive APIs for seamless business integration .Recent updates have added a Python/JavaScript code executor component to the Agent , support for cross-language queries , the ability to use a multi-modal model for understanding images within PDF or DOCX files , and integration with Internet search (Tavily) for Deep Research-like reasoning . It also supports text-to-SQL statements through RAG .RAGFlow is designed for easy deployment, with prerequisites including Docker and Docker Compose , and offers pre-built Docker images in various editions . It also supports launching the service from source for development purposes . RAGFlow has gained significant community support, evidenced by its 61k stars and 6.2k forks on GitHub .",
  "categories": [
    "RAG (Retrieval-Augmented Generation)",
    "Large Language Models (LLMs)",
    "Deep Learning",
    "AI Applications",
    "Document Processing",
    "Content Generation with AI",
    "Workflow Automation",
    "Systems Integration",
    "Data Analysis",
    "AI Agents"
  ]
},
{
  "name": "AutoAgent: Fully-Automated and Zero-Code LLM Agent Framework",
  "link": "https://github.com/HKUDS/AutoAgent",
  "difficulty": "Unspecified",
  "type": "Tool",
  "description": "AutoAgent is a Fully-Automated and highly Self-Developing framework that enables users to create and deploy LLM agents through Natural Language Alone . It serves as a personal AI assistant, designed to be dynamic, extensible, customized, and lightweight .Key capabilities include:- Agentic-RAG with Native Self-Managing Vector Database: It outperforms industry-leading solutions like LangChain by being equipped with a native self-managing vector database .- Zero-Code Agent and Workflow Creation: Users can effortlessly build ready-to-use tools, agents, and workflows using natural language, requiring no coding [2-5].- Universal LLM Support: It seamlessly integrates with a wide range of LLMs, including OpenAI, Anthropic, Deepseek, vLLM, Grok, and Huggingface .- Flexible Interaction Modes: Supports both function-calling and ReAct interaction modes .- High Performance: Delivers comparable performance to many Deep Research Agents on the GAIA Benchmark . It is presented as a cost-effective, open-source alternative to OpenAI's Deep Research subscription .- User-Friendly Interface: Offers an easy-to-deploy CLI interface and handles file uploads for enhanced data interaction .The framework provides an out-of-the-box multi-agent system in its 'user mode' . Installation can be done via CLI or Docker , and it supports adding API keys for various LLMs and third-party tool platforms like RapidAPI . Future developments include expanding evaluations to SWE-bench and WebArena, supporting Computer-Use agents with GUI interaction, integrating with more tool platforms (e.g., Composio), adding code sandboxes (e.g., E2B), and developing a comprehensive web interface . AutoAgent is primarily written in Python (99.8%) . It has garnered significant community interest with 5.7k stars and 774 forks on GitHub .",
  "categories": [
    "AI Agents",
    "Agent Development",
    "Multi-Agent Systems",
    "Large Language Models (LLMs)",
    "RAG (Retrieval-Augmented Generation)",
    "Workflow Automation",
    "Systems Integration",
    "AI Applications",
    "Content Generation with AI",
    "Deep Learning"
  ]
},
{
  "name": "LLaMA-Factory: Unified Efficient Fine-Tuning of 100+ LLMs & VLMs (ACL 2024)",
  "link": "https://github.com/hiyouga/LLaMA-Factory",
  "difficulty": "Unspecified",
  "type": "Tool",
  "description": "LLaMA-Factory is an open-source tool designed for Unified Efficient Fine-Tuning of over 100 Large Language Models (LLMs) and Vision Language Models (VLMs) [13-15]. It simplifies the fine-tuning process with both a zero-code CLI and an intuitive Web UI .Key features and capabilities include:- Extensive Model Support: Compatible with a wide array of models such as LLaMA, LLaVA, Mistral, Mixtral-MoE, Qwen, DeepSeek, Yi, Gemma, ChatGLM, and Phi, among others . It provides day-N support for cutting-edge models .- Integrated Training Approaches: Supports various methods including (continuous) pre-training, (multimodal) supervised fine-tuning, reward modeling, and advanced preference learning algorithms like PPO, DPO, KTO, ORPO, and SimPO .- Scalable Resource Optimization: Offers 16-bit full-tuning, freeze-tuning, LoRA, and 2/3/4/5/6/8-bit QLoRA, leveraging multiple quantization techniques (AQLM/AWQ/GPTQ/LLM.int8/HQQ/EETQ) .- Advanced Algorithms & Practical Tricks: Integrates sophisticated algorithms such as GaLore, BAdam, APOLLO, Adam-mini, Muon, DoRA, LongLoRA, LLaMA Pro, Mixture-of-Depths, LoRA+, LoftQ, and PiSSA [14, 23-29]. It also includes performance-enhancing tricks like FlashAttention-2, Unsloth, Liger Kernel, RoPE scaling, and NEFTune .- Wide Task Support: Handles diverse tasks including multi-turn dialogue, tool using, image understanding, visual grounding, video recognition, and audio understanding .- Experiment Monitoring & Faster Inference: Supports various experiment monitors (LlamaBoard, TensorBoard, Wandb, MLflow, SwanLab)  and provides faster inference via OpenAI-style API, Gradio UI, and CLI with vLLM or SGLang workers .- Hardware Compatibility: Works across different hardware, including CUDA, Ascend NPU, and AMD ROCm [35-40].LLaMA-Factory is utilized by major companies like Amazon, NVIDIA, and Aliyun . It has achieved significant community traction with 55k stars and 6.8k forks on GitHub . The underlying research is detailed in the paper 'LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models' .",
  "categories": [
    "Machine Learning",
    "Deep Learning",
    "Large Language Models (LLMs)",
    "AI Applications",
    "Model Management",
    "LLMOps",
    "AI Automation",
    "Content Generation with AI",
    "Multimodal AI"
  ]
},
{
  "name": "Transformer Lab: Open Source Application for Advanced LLM + Diffusion Engineering",
  "link": "https://github.com/transformerlab/transformerlab-app",
  "difficulty": "Unspecified",
  "type": "Tool",
  "description": "Transformer Lab is an open-source application designed for Advanced LLM + Diffusion Engineering, allowing users to interact, train, fine-tune, and evaluate large language models on their own computer . This app is supported by Mozilla through the Mozilla Builders Program .Its features are accessible through a simple cross-platform GUI and include:- One-click Model Downloads: Easily download hundreds of popular models such as DeepSeek, Llama3, Qwen, Phi4, Gemma, Mistral, Mixtral, and Command-R, and any LLM from Hugging Face .- Flexible Fine-tuning and Training: Supports fine-tuning using MLX on Apple Silicon and Huggingface on GPU .- RLHF and Preference Optimization: Incorporates DPO, ORPO, SIMPO, and Reward Modeling .- Cross-Operating System Compatibility: Available as Windows, MacOS, and Linux applications .- Comprehensive Chat Functionality: Offers chat, completions, preset prompts, chat history, parameter tweaking, batched inference, and tool use/function calling (in alpha) .- Multiple Inference Engines: Utilizes MLX on Apple Silicon, Huggingface Transformers, vLLM, and Llama CPP .- Model Evaluation and RAG: Includes model evaluation capabilities and Retrieval-Augmented Generation (RAG) with a drag-and-drop file UI, compatible with various engines .- Dataset Building: Enables users to build datasets by pulling from hundreds of common Hugging Face datasets or providing their own via drag-and-drop .- Utilities and API: Features embedding calculation, a full REST API, and the ability to run in the cloud (with the UI locally or entirely on a single machine) .- Model Conversion and Plugins: Allows conversion of models between Huggingface, MLX, and GGUF formats, and supports custom plugins for extended functionality .- Developer Tools: Provides an embedded Monaco Code Editor for plugin editing and viewing behind-the-scenes operations, along with prompt editing and inference logs .This project is actively developed and has a strong community presence with 3.7k stars and 325 forks on GitHub . It is primarily written in TypeScript (96.7%) and JavaScript (2.8%) .",
  "categories": [
    "Large Language Models (LLMs)",
    "Deep Learning",
    "Generative AI",
    "Machine Learning",
    "AI Applications",
    "Model Management",
    "AI Deployment",
    "Prompt Engineering",
    "RAG (Retrieval-Augmented Generation)",
    "AI Agents",
    "Data Analysis",
    "Multimodal AI"
  ]
},
{
  "name": "xpander.ai: Backend-as-a-Service for AI Agents",
  "link": "https://github.com/xpander-ai/xpander.ai",
  "difficulty": "Unspecified",
  "type": "Tool",
  "description": "xpander.ai is a framework-agnostic Backend-as-a-Service (BaaS) infrastructure for autonomous AI agents . It abstracts away infrastructure complexity, allowing users to focus on building intelligent, effective, and production-ready AI agents .Its core offerings include:- Comprehensive Agent Capabilities: Provides memory, tools, multi-user state, various agent triggering options (MCP, A2A, API, Web interfaces), storage, and agent-to-agent messaging .- Framework Flexibility: Compatible with popular agent frameworks like OpenAI ADK, Agno, CrewAI, and LangChain, as well as direct LLM APIs .- Robust Tool Integration: Offers access to a comprehensive MCP-compatible tools library and pre-built integrations .- Scalable Hosting & Deployment: Enables effortless deployment and scaling of agents on managed infrastructure .- Real-time Event Management: Features event streaming capabilities for Slackbots, ChatUIs, Agent2Agent communication, and Webhook integrations .- API Guardrails: Implements strong guardrails using an Agent-Graph-System to define and manage dependencies between API actions for tool-use .xpander.ai offers quick setup for integrating a backend into agents, providing Python and Node.js SDKs, and a CLI for agent creation and deployment [53-56]. It supports deploying Docker containers to the cloud . The platform showcases featured open-source AI agents built using its services, such as a Kubernetes operations agent, a Coding Agent, and an NVIDIA Meeting Recorder . It has 362 stars and 57 forks on GitHub  and is mainly developed in Jupyter Notebook (58.5%), Python (36.2%), and JavaScript (3.8%) .",
  "categories": [
    "AI Agents",
    "Agent Development",
    "Multi-Agent Systems",
    "Agent Memory",
    "MCP (Model Context Protocol)",
    "AI Deployment",
    "Workflow Automation",
    "Systems Integration",
    "AI Applications"
  ]
},
{
    "name": "VibeKit: Run coding agents in a secure sandbox",
    "link": "https://github.com/superagent-ai/vibekit",
    "difficulty": "Unspecified",
    "type": "Tool",
    "description": "VibeKit es un SDK sencillo para ejecutar agentes de codificación de forma segura en un entorno aislado (sandbox) . Está diseñado para integrar potentes agentes de codificación como Claude Code, OpenAI Codex, Gemini CLI y SST Opencode en tus aplicaciones o flujos de trabajo . Permite generar y ejecutar código real de forma segura, transmitir la salida a interfaces de usuario en tiempo real y operar con aislamiento y flexibilidad, principalmente en la nube (con soporte local planificado) .Entre sus características clave se incluyen:   Sandboxing seguro: Proporciona un entorno robusto para la ejecución segura de código .   SDK 'drop-in': Facilita la integración con diversos agentes de codificación .   Ejecución basada en la nube: Permite el funcionamiento de agentes en la nube (con soporte local planificado) .   Automatización de GitHub: Soporta funciones como la creación de ramas, commits y pull requests .   Historial de prompts y continuidad de contexto: Mantiene el contexto conversacional para los agentes .   Salida en streaming: Permite actualizaciones en tiempo real a las interfaces de usuario .   Soporte OpenTelemetry: Para trazabilidad y métricas .   Compatibilidad con tiempos de ejecución de sandboxes: Compatible con E2B, Daytona, Northflank, Cloudflare y Dagger .   Ejecución de comandos arbitrarios: Capacidad de ejecutar cualquier comando dentro de entornos sandbox .Los casos de uso de VibeKit incluyen la creación de herramientas internas de depuración, el lanzamiento de funciones impulsadas por IA, la automatización de tareas de codificación repetitivas y la prueba segura de la salida de LLMs en entornos de producción o prototipos . El proyecto tiene licencia MIT  y está desarrollado principalmente en TypeScript (97.8%) . Ha captado el interés de la comunidad con 772 estrellas y 103 forks en GitHub .",
    "categories": [
        "AI Agents",
        "Agent Development",
        "Systems Integration",
        "AI Applications",
        "Large Language Models (LLMs)",
        "Workflow Automation",
        "AI Deployment"
    ]
},
{
    "name": "Context Engineering Template (GitHub Repo)",
    "link": "https://github.com/coleam00/context-engineering-intro",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "This repository is presented as a comprehensive template for getting started with Context Engineering, a discipline focused on designing the context that AI coding assistants need to complete tasks from beginning to end . It claims to be \"10 times better than prompt engineering and 100 times better than 'vibe coding'\" , offering a complete system that includes documentation, examples, rules, patterns, and validation, unlike prompt engineering which is limited to task formulation . Its benefits include reducing AI failures (mostly context failures), ensuring consistency with project patterns, enabling complex features, and allowing autocorrection through validation loops . While centered around Claude Code , the strategy is applicable to any AI coding assistant . The template structure includes .claude/ for commands and settings, PRPs/ for Product Requirements Prompts, examples/ for code examples (critical for success as AI performs better with patterns) , CLAUDE.md for global rules , INITIAL.md for feature requests , and README.md . It provides a quick start guide  and a step-by-step guide  covering setting up global rules (e.g., project awareness, code structure, testing, style, documentation standards) , creating initial feature requests (e.g., specific functionality, examples, documentation links, other considerations) [11-13], generating comprehensive PRPs (Product Requirements Prompts)  which are blueprints with context, steps, validation, and test requirements , and executing PRPs for feature implementation . The repository has 7.3k stars and 1.5k forks on GitHub  and is primarily developed in Python (53.3%) and TypeScript (46.6%) . It explicitly notes that it doesn't focus on RAG and tools for context engineering as more is planned for the future . Best practices include being explicit in INITIAL.md, providing comprehensive examples (both what to do and what not to do, and error handling patterns), using validation gates (PRPs include test commands), leveraging documentation, and customizing CLAUDE.md .",
    "categories": [
      "Prompt Engineering",
      "AI Agents",
      "Large Language Models (LLMs)",
      "AI Deployment",
      "Workflow Automation"
    ]
  },
  {
    "name": "GitHub MCP Server",
    "link": "https://github.com/github/github-mcp-server",
    "difficulty": "Unspecified",
    "type": "Tool",
    "description": "This is the official GitHub MCP Server designed to connect AI tools directly to the GitHub platform . It empowers AI agents, assistants, and chatbots to interact with repositories and code files, manage issues and Pull Requests (PRs), analyze code, and automate workflows, all through natural language interactions . Key use cases include repository management (browsing, searching, analyzing commits), issue & PR automation (creating, updating, triaging bugs, reviewing code), CI/CD & workflow intelligence (monitoring GitHub Actions, analyzing failures), code analysis (security findings, Dependabot alerts), and team collaboration (discussions, notifications) . It is built for developers wanting to connect AI tools to GitHub context and capabilities, from simple natural language queries to complex multi-step agent workflows . The server can be implemented as a remote GitHub-hosted server  or locally (requiring Docker) . It supports secure management of Personal Access Tokens (PATs) using environment variables for security best practices (e.g., minimum scopes, separate tokens, never committing, regular rotation, file permissions) . Users can enable or disable specific groups of functionalities via toolsets to control which GitHub API capabilities are available to AI tools . Available toolsets include context (strongly recommended), actions, code_security, dependabot, discussions, issues, notifications, orgs, pull_requests, repos, secret_protection, and users (all on by default) . A special all toolset enables all functionalities . The server also includes dynamic tool discovery (beta feature) to help models avoid confusion from too many tools  and can run in read-only mode to prevent modifications . It supports GitHub Enterprise Server and Enterprise Cloud with data residency via the --gh-host flag or GITHUB_HOST environment variable . The tool descriptions can be overridden using a JSON config file or environment variables for internationalization (i18n) . The repository is developed primarily in Go (98.9%)  and is licensed under MIT . It has accumulated 19.3k stars and 1.6k forks on GitHub .",
    "categories": [
      "AI Agents",
      "MCP (Model Context Protocol)",
      "Systems Integration",
      "Workflow Automation",
      "AI Automation",
      "AI Deployment",
      "AI Applications"
    ]
  },
  {
    "name": "MCP Project End to End + Explanation of MCP (Sreemanti Dey)",
    "link": "https://www.youtube.com/watch?v=8dzuX4Y06Io",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "This resource is an \"End to End MCP Project\" that includes a detailed explanation of the Model Context Protocol (MCP) . It is part of a collection of practical Machine Learning and Data Science projects recommended by Sreemanti Dey for hands-on learning . The MCP is an open protocol that standardizes how Large Language Model (LLM) applications can access context in terms of tools and data resources, based on a client-server architecture [conversation history, drawing from 151]. This project likely guides users through the complete implementation of an MCP-based application [conversation history, implied by 93].",
    "categories": [
      "MCP (Model Context Protocol)",
      "AI Agents",
      "Large Language Models (LLMs)",
      "Machine Learning",
      "AI Deployment",
      "AI Applications",
      "Agent Development"
    ]
  },
  {
    "name": "RAG Project End to End + Explanation of RAG (Sreemanti Dey)",
    "link": "https://www.youtube.com/watch?v=ERijuxJAaoQ",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "This resource focuses on an \"End to End RAG Project\" with a detailed explanation of Retrieval Augmented Generation (RAG) . It is part of a collection of practical Machine Learning and Data Science projects recommended by Sreemanti Dey for hands-on learning . RAG is a technique that enhances the capabilities of LLMs by allowing them to retrieve relevant information from an external knowledge base to inform their responses, which reduces hallucinations and provides up-to-date information [conversation history, drawing from 135]. This project aims to offer a practical guide for building and deploying an RAG-based application from start to finish [conversation history, implied by 93].",
    "categories": [
      "RAG (Retrieval-Augmented Generation)",
      "AI Agents",
      "Large Language Models (LLMs)",
      "Machine Learning",
      "AI Applications",
      "Agent Development"
    ]
  },
  {
    "name": "Google Cloud Skills Boost Catalog",
    "link": "https://www.cloudskillsboost.google/",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "This is Google Cloud's comprehensive learning catalog, offering over 980+ learning activities designed to help users apply their skills in the Google Cloud console . The catalog features a variety of activity formats, including bite-size individual labs and multi-module courses that combine videos, documents, labs, and quizzes . Students are provided with temporary credentials to actual cloud resources, enabling hands-on practice with real Google Cloud environments . Users can earn badges for completing activities and define, track, and measure their success . Among its wide-ranging offerings, the catalog includes resources for learning about generative AI from the ground up and gaining a solid understanding of the field .",
    "categories": [
      "Introduction to AI and Fundamental Concepts",
      "Generative AI",
      "Machine Learning",
      "AI Deployment",
      "AI Applications",
      "Large Language Models (LLMs)"
    ]
  },
  {
    "name": "AI Engineering: Building Applications with Foundation Models (Chip Huyen, December 2024)",
    "link": "https://www.linkedin.com/posts/muhammad-zarar-4995aa246_aiengineering-activity-7347964728385626113-mtsL/?utm_source=share&utm_medium=member_desktop&rcm=ACoAAADvoYgBR6OaUkic3fcx6maIpbJDW9ElVf0",
    "difficulty": "Unspecified",
    "type": "Book",
    "description": "This book, authored by Chip Huyen and slated for December 2024, addresses the significant surge in AI applications and the reduced entry barrier for developers following advancements like ChatGPT . It positions AI as a powerful development tool accessible to everyone, noting that it's even possible to build applications without writing a single line of code . The book highlights that modern AI adoption is built upon existing techniques, such as language modeling (since the 1950s) and Retrieval-Augmented Generation (RAG), which leverages older retrieval technology . It emphasizes that the best practices for deploying traditional machine learning applications—systematic experimentation, rigorous evaluation, and relentless optimization for faster and cheaper models—remain crucial for foundation model-based applications . The book aims to help readers understand AI engineering and how it differs from traditional machine learning engineering, learn the process of developing an AI application including challenges and solutions, explore model adaptation techniques (prompt engineering, RAG, fine-tuning, agents, dataset engineering), examine latency and cost bottlenecks when serving foundation models, and choose the right model, dataset, evaluation benchmarks, and metrics . It has received positive feedback from experts, with comments like 'Chip Huyen nails it again' .",
    "categories": [
      "AI Applications",
      "Large Language Models (LLMs)",
      "Prompt Engineering",
      "RAG (Retrieval-Augmented Generation)",
      "AI Agents",
      "Machine Learning",
      "AI Deployment",
      "LLMOps",
      "Introduction to AI and Fundamental Concepts"
    ]
  },
  {
    "name": "GPT-4.1 Prompting Guide",
    "link": "N/A (OpenAI Documentation Guide)",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "The GPT-4.1 Prompting Guide provides important prompting tips derived from extensive internal testing to help developers fully leverage the improved abilities of the new GPT-4.1 model family . GPT-4.1 represents a significant step forward from GPT-4o in capabilities across coding, instruction following, and long context . While many typical best practices (like providing context examples, specific instructions, and inducing planning) still apply, GPT-4.1 is trained to follow instructions more closely and literally than its predecessors, making it highly steerable with well-specified prompts . The guide emphasizes building agentic workflows with GPT-4.1, noting its state-of-the-art performance on SWE-bench Verified (solving 55% of problems) . Key recommendations for agent prompts include Persistence (to ensure multi-message turns) , Tool-calling (to encourage tool use and reduce hallucination) , and optional Planning (to ensure explicit planning and reflection between tool calls) . Adhering to these simple instructions significantly improved internal SWE-bench Verified scores . The guide strongly encourages using the tools field in OpenAI API requests for passing tools to minimize errors and observed a 2% increase in pass rate . Developers should use clear names and detailed descriptions for tools and their parameters . Prompting-induced planning or \"chain-of-thought\" (CoT) can be effective to break down problems, improve output quality, and increase pass rates (e.g., 4% increase on SWE-bench Verified) by making the model \"think out loud\" . The guide also covers long context usage (up to 1M tokens)  and advises placing instructions at both the beginning and end of long context for optimal performance . For instruction following, GPT-4.1 exhibits outstanding performance, but developers may need to be explicit about what to do or not to do, as the model follows instructions more literally . A recommended workflow for developing instructions involves starting with high-level guidance, adding specific detail sections, and using ordered lists for workflow steps . It also highlights common failure modes, such as models hallucinating tool inputs if not enough information is available . The guide includes a sample prompt for SWE-bench Verified  and a customer service agent example , demonstrating best practices for workflow, problem-solving strategy, and instruction diversity . It provides general advice on prompt structure  and delimiters like Markdown, XML, and JSON [19-21]. The GPT-4.1 family also features substantially improved diff capabilities, with a recommended diff format (V4A diff format) extensively trained on, and a reference Python implementation for an apply_patch tool [22-25].",
    "categories": [
      "Introduction to AI and Fundamental Concepts",
      "Generative AI",
      "Large Language Models (LLMs)",
      "Prompt Engineering",
      "Prompt Optimization",
      "Prompting Strategies",
      "Interaction with LLMs",
      "AI Agents",
      "Agent Development",
      "AI Applications"
    ]
  },
  {
    "name": "WebMCP",
    "link": "https://github.com/MiguelsPizza/WebMCP",
    "difficulty": "Unspecified",
    "type": "Tool",
    "description": "WebMCP (Model Context Protocol - Browser-specific) is a tool that allows websites to function as MCP servers, exposing their existing functionality (e.g., APIs, forms, state) as structured tools that AI agents can directly call . It operates securely within the browser's existing authentication and security model [26-28]. For users, the MCP-B Chrome extension enables AI interactions on MCP-enabled sites by auto-detecting tools and allowing chat-based AI queries (e.g., \"Add to cart\") that execute tools across tabs using browser sessions for authentication . For website owners, implementation involves installing npm packages and adding an MCP server to register tools wrapping existing logic [28-30]. These tools can be dynamic and page-scoped, with the extension automatically injecting clients, making the site securely AI-ready within the browser sandbox . From an AI's perspective, it receives domain-prefixed tools from open tabs and calls them with JSON parameters for deterministic actions . The extension handles routing and navigation, with responses enabling chaining of actions . This approach supports dynamic updates and authentication via browser context, reducing errors compared to visual automation . The project provides quick start guides , a live demo , and examples like a simple todo app where AI agents can manage tasks (e.g., add, update, delete todos) . It also features a native host that bridges the browser to local MCP clients (e.g., Claude Desktop, Cursor), allowing tools from your website to be called from desktop apps . WebMCP emphasizes security, respecting the browser sandbox and same-origin policy, collecting no data, and allowing users to audit tool calls via the extension .",
    "categories": [
      "AI Agents",
      "Agent Development",
      "MCP (Model Context Protocol)",
      "Systems Integration",
      "Workflow Automation",
      "AI Deployment",
      "AI Applications"
    ]
  },
  {
    "name": "LLaMA-Factory: Unified Efficient Fine-Tuning of 100+ LLMs & VLMs",
    "link": "https://github.com/hiyouga/LLaMA-Factory",
    "difficulty": "Unspecified",
    "type": "Tool",
    "description": "LLaMA-Factory is a powerful open-source tool designed for the unified efficient fine-tuning of over 100 Large Language Models (LLMs) and Visual Language Models (VLMs) . It simplifies the fine-tuning process with both a zero-code Command Line Interface (CLI) and a user-friendly Web UI (LLaMA Board GUI) . The platform supports a diverse range of cutting-edge models including LLaMA, LLaVA, Mistral, Mixtral-MoE, Qwen, DeepSeek, Yi, Gemma, ChatGLM, Phi, and more, offering day-N support for newly released models . It integrates various training methods such as (Continuous) pre-training, (multimodal) supervised fine-tuning, reward modeling, PPO, DPO, KTO, and ORPO . For scalable and efficient resource utilization, LLaMA-Factory supports 16-bit full-tuning, freeze-tuning, LoRA, and 2/3/4/5/6/8-bit QLoRA through different quantization techniques (AQLM/AWQ/GPTQ/LLM.int8/HQQ/EETQ) . It also incorporates advanced algorithms like GaLore, BAdam, APOLLO, Muon, DoRA, LongLoRA, LLaMA Pro, Mixture-of-Depths, LoRA+, LoftQ, and PiSSA, alongside practical tricks such as FlashAttention-2, Unsloth, Liger Kernel, RoPE scaling, NEFTune, and rsLoRA for optimized performance [38, 45-47]. LLaMA-Factory is capable of handling a wide array of tasks, including multi-turn dialogue, tool using, image understanding, visual grounding, and video/audio recognition . It provides faster inference capabilities via an OpenAI-style API, Gradio UI, and CLI, with support for vLLM or SGLang workers . The tool is widely recognized and used by prominent organizations like Amazon, NVIDIA, and Aliyun . It includes comprehensive support for various datasets used in pre-training, supervised fine-tuning, and preference learning . Users can easily get started by installing it from source, pre-built Docker images, or setting up a virtual environment, with specific instructions for Windows and Ascend NPU users also provided [52-56].",
    "categories": [
      "Large Language Models (LLMs)",
      "Machine Learning",
      "Deep Learning",
      "AI Deployment",
      "AI Applications",
      "Generative AI",
      "LLMOps",
      "Prompt Engineering",
      "AI Agents"
    ]
  },
  {
    "name": "Google for Education: Impulsando la educación con la IA",
    "link": "https://edu.google.com/intl/ALL_es/ai/education/",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "Google for Education se compromete a hacer que la IA sea útil para todo el mundo en el ámbito educativo, tanto dentro como fuera de clase, aplicando la tecnología de forma responsable y con un enfoque centrado en las personas . La IA no busca reemplazar la experiencia, el conocimiento ni la creatividad de un docente, sino ser una herramienta útil para mejorar y enriquecer las experiencias de enseñanza y aprendizaje . Google garantiza la protección de datos reforzada en sus herramientas de IA, como Gemini, y asegura que los datos de cliente de los servicios principales de Google Workspace for Education no se usan sin permiso para entrenar modelos de IA generativa y de lenguaje extenso en los que se basan Gemini y otros sistemas externos .Para los docentes, la IA puede optimizar su labor, impulsando su creatividad y productividad al ahorrarles tiempo que pueden invertir en ellos mismos y en sus alumnos . Gemini, como asistente de IA integrado en Google Workspace for Education, ayuda a los docentes a ahorrar tiempo, obtener inspiración con ideas innovadoras y crear experiencias de aprendizaje atractivas en un entorno privado y seguro . Próximamente, la IA también sugerirá preguntas sobre vídeos de YouTube en Classroom para hacer las clases más atractivas . La gama Chromebook Plus es una opción económica y potente que integra lo mejor de la IA de Google para el profesorado y el personal .Para los alumnos, la IA ayuda a personalizar el aprendizaje al adaptarse a sus necesidades individuales . Las prácticas guiadas de Google Classroom pueden ofrecer comentarios automáticos en tiempo real y consejos útiles a los alumnos que tengan dificultades . Además, la IA integrada en los Chromebooks proporciona funciones avanzadas de conversión de texto a voz, dictado y subtítulos automáticos .La seguridad y privacidad son una prioridad fundamental . Google bloquea el 99.9% del spam, phishing y malware con sistemas de detección basados en IA, y los dispositivos ChromeOS no han registrado ataques de ransomware . La infraestructura de Google Workspace for Education cumple rigurosos estándares de cumplimiento como el RGPD, FERPA y COPPA . Google se toma muy en serio la seguridad de los usuarios, especialmente la de los niños, diseñando funciones y productos de IA con protecciones adaptadas a diferentes grupos de edad y realizando pruebas rigurosas para minimizar daños potenciales .Google ofrece una variedad de recursos de formación, kits de herramientas y guías sobre IA para docentes, incluyendo cursos sobre IA generativa, habilidades digitales aplicadas y cursos de IA y aprendizaje automático de Grow with Google . También se compromete a una colaboración continua con centros educativos, docentes y expertos para desarrollar y mejorar sus herramientas de IA, introduciendo nuevas funciones de forma gradual para que los centros puedan decidir qué es lo que más les conviene .",
    "categories": [
      "Introduction to AI and Fundamental Concepts",
      "Generative AI",
      "Large Language Models (LLMs)",
      "Prompt Engineering",
      "AI Agents",
      "AI Applications",
      "AI in Education",
      "AI Ethics",
      "Workflow Automation",
      "AI Deployment"
    ]
  },
  {
    "name": "MarkItDown",
    "link": "https://github.com/microsoft/markitdown",
    "difficulty": "Unspecified",
    "type": "Tool",
    "description": "MarkItDown es una utilidad ligera de Python diseñada para convertir varios tipos de archivos a formato Markdown, pensada principalmente para su uso con modelos de lenguaje grandes (LLMs) y otras pipelines de análisis de texto . Su objetivo es preservar la estructura y el contenido importante del documento, incluyendo encabezados, listas, tablas y enlaces, aunque no está destinada a conversiones de alta fidelidad para consumo humano, sino para ser procesada por herramientas de análisis de texto .Tipos de archivos compatibles: MarkItDown soporta la conversión de una amplia gama de formatos, tales como: PDF, PowerPoint, Word, Excel, imágenes (incluyendo metadatos EXIF y OCR), audio (metadatos EXIF y transcripción de voz), HTML, formatos basados en texto (CSV, JSON, XML), archivos ZIP (iterando sobre su contenido), URLs de YouTube y EPUBs, entre otros .¿Por qué Markdown? Este formato es muy similar al texto plano, con un marcado mínimo, pero permite representar la estructura fundamental del documento . Los LLMs principales, como GPT-4o de OpenAI, 'hablan' Markdown de forma nativa y a menudo lo incorporan en sus respuestas sin ser solicitados, lo que sugiere que han sido entrenados con grandes volúmenes de texto formateado en Markdown y lo entienden bien . Además, las convenciones de Markdown son altamente eficientes en el uso de tokens .Características clave y uso:   Integración con MCP (Model Context Protocol): MarkItDown ofrece un servidor MCP para la integración con aplicaciones LLM, como Claude Desktop . El MCP es un protocolo abierto que estandariza cómo las aplicaciones LLM acceden a herramientas y recursos de datos . Esto permite que MarkItDown añada capacidades de 'scraping' (rastreo web) potentes a editores de código a través de MCP .   Requisitos: Requiere Python 3.10 o superior y se recomienda el uso de un entorno virtual .   Instalación: Se instala fácilmente con pip install 'markitdown[all]' para incluir todas las dependencias opcionales, o se pueden instalar individualmente (ej. [pdf, docx, pptx]) [7-9]. También se puede instalar desde el código fuente .   Uso por línea de comandos (CLI): Permite la conversión de archivos como markitdown path-to-file.pdf -o document.md o el encadenamiento de contenido a través de 'pipes' .   Plugins: Soporta plugins de terceros, que están deshabilitados por defecto y se pueden habilitar mediante comandos específicos .   Integración con Azure Document Intelligence: Puede utilizarse con Microsoft Document Intelligence para conversiones avanzadas .   API de Python: Ofrece una API de Python para la conversión programática de documentos, incluyendo la integración con Azure Document Intelligence y la descripción de imágenes usando LLMs (ej. GPT-4o) .   Docker: Se puede construir y ejecutar en un contenedor Docker .Comunidad y contribuciones: El proyecto MarkItDown es de código abierto (licencia MIT) y acoge contribuciones [13-15]. Sigue el Código de Conducta de Código Abierto de Microsoft y anima a la comunidad a revisar 'issues' y 'pull requests' . El repositorio tiene una gran cantidad de soporte comunitario, con 69.5 mil estrellas y 3.7 mil 'forks' , y ha sido utilizado por 1.7 mil proyectos , con contribuciones de 65 desarrolladores . Su desarrollo es principalmente en Python .",
    "categories": [
      "Document Processing",
      "Large Language Models (LLMs)",
      "MCP (Model Context Protocol)",
      "AI Applications",
      "AI Agents",
      "Workflow Automation",
      "Systems Integration",
      "AI Deployment"
    ]
  },
    {
    "name": "MCP Toolbox for Databases",
    "link": "https://github.com/googleapis/genai-toolbox",
    "difficulty": "Unspecified",
    "type": "Tool",
    "description": "El MCP Toolbox for Databases es un servidor MCP (Model Context Protocol) de código abierto diseñado para bases de datos [1-3]. Su objetivo principal es simplificar y acelerar el desarrollo de herramientas de IA, haciéndolas más seguras al manejar complejidades como la gestión de conexiones, la autenticación y más . Originalmente llamado 'Gen AI Toolbox for Databases', fue renombrado para alinearse con su compatibilidad con MCP . Actualmente se encuentra en fase beta, lo que implica que podría haber cambios significativos hasta su primera versión estable (v1.0) .¿Por qué utilizar Toolbox?   Desarrollo simplificado: Permite integrar herramientas a un agente con menos de 10 líneas de código, reutilizar herramientas entre múltiples agentes o frameworks, y desplegar nuevas versiones de herramientas con mayor facilidad .   Mejor rendimiento: Implementa las mejores prácticas como el 'connection pooling' y la autenticación .   Seguridad mejorada: Ofrece autenticación integrada para un acceso más seguro a los datos .   Observabilidad de extremo a extremo: Proporciona métricas y trazas listas para usar con soporte integrado para OpenTelemetry .Optimización del flujo de trabajo con un asistente de IA para bases de datos :   Consultas en lenguaje natural: Permite interactuar con los datos utilizando lenguaje natural directamente desde el IDE, eliminando la necesidad de escribir SQL para preguntas complejas .   Automatización de la gestión de bases de datos: El asistente de IA puede generar consultas, crear tablas, añadir índices y gestionar la base de datos describiendo simplemente las necesidades de datos .   Generación de código consciente del contexto: Capacita al asistente de IA para generar código de aplicación y pruebas con un conocimiento profundo del esquema de la base de datos en tiempo real, acelerando el ciclo de desarrollo .   Reducción de la sobrecarga de desarrollo: Ayuda a reducir drásticamente el tiempo dedicado a configuraciones de base de datos tediosas, código repetitivo y migraciones de esquema propensas a errores .Arquitectura y funcionamiento: Toolbox se sitúa entre el framework de orquestación de la aplicación y la base de datos, actuando como un 'control plane' para modificar, distribuir o invocar herramientas. Centraliza la gestión de herramientas, permitiendo compartirlas entre agentes y aplicaciones y actualizarlas sin necesidad de redesplegar la aplicación .Instalación: Se puede instalar como un binario , como una imagen de contenedor Docker , usando Homebrew en macOS o Linux , o compilando desde el código fuente con Go .Configuración y herramientas: La configuración principal se realiza a través de un archivo tools.yaml . Este archivo define:   sources: Las fuentes de datos a las que Toolbox tendrá acceso (ej. PostgreSQL) .   tools: Las acciones que un agente puede realizar, incluyendo el tipo de herramienta, la(s) fuente(s) a la que afecta, los parámetros y la sentencia .   toolsets: Grupos de herramientas que pueden cargarse conjuntamente, útiles para definir diferentes conjuntos según el agente o la aplicación .Integración con aplicaciones (SDKs de cliente): Toolbox ofrece SDKs para varios frameworks y lenguajes:   Python: toolbox-core , toolbox-langchain , toolbox-llamaindex .   Javascript/Typescript: @toolbox-sdk/core con integraciones para LangChain/LangGraph  y Genkit .   Go: mcp-toolbox-sdk-go con integraciones para LangChain Go , Genkit , Go GenAI  y OpenAI Go .El proyecto es de código abierto bajo licencia Apache-2.0  y da la bienvenida a las contribuciones, siguiendo un Código de Conducta para Contribuidores . Cuenta con una comunidad activa en Discord . El repositorio de GitHub muestra un notable apoyo comunitario con 8 mil estrellas y 584 'forks', y 48 contribuidores . Está desarrollado principalmente en Go (99.4%) .",
    "categories": [
      "MCP (Model Context Protocol)",
      "AI Agents",
      "Large Language Models (LLMs)",
      "Generative AI",
      "Agent Development",
      "Systems Integration",
      "Workflow Automation",
      "AI Deployment",
      "Data Analysis",
      "LLMOps",
      "AI Applications"
    ]
  },
   {
    "name": "Vibe Kanban",
    "link": "https://github.com/microsoft/vibe-kanban",
    "difficulty": "Unspecified",
    "type": "Tool",
    "description": "Vibe Kanban is a Kanban board designed to manage AI coding agents [conversation history]. Its primary purpose is to simplify and optimize the workflow of human engineers who spend most of their time planning, reviewing, and orchestrating tasks for AI coding agents [conversation history].This board allows users to [conversation history]:   Easily switch between different coding agents (such as Claude Code, Gemini CLI, Codex, Amp, and others).   Orchestrate the execution of multiple coding agents, either in parallel or in sequence.   Quickly review work and launch development servers.   Track the status of tasks that coding agents are working on.   Centralize the configuration of Model Context Protocol (MCP) settings for coding agents.Key Features and Usage [conversation history]:   License: The project is open-source under the Apache-2.0 license.   Installation: Requires authentication with the desired coding agent, then running npx vibe-kanban in the terminal.   Documentation and Support: The latest documentation and user guides are available on its website. To report bugs or request features, an 'issue' should be opened in the GitHub repository.   Development: Prerequisites include Rust (latest stable version), Node.js (>=18), and pnpm (>=8). It can be built from the source code.   Environment Variables: Allows configuring build variables (such as GITHUB_CLIENT_ID for GitHub OAuth authentication and PostHog API keys for analytics) and runtime variables (such as server ports and host). It is possible to use a custom GitHub OAuth application for self-hosting or custom branding.   Community and Contributions: The project has considerable community support, with 3.4k stars and 297 'forks' on GitHub. It has had 8 contributors, and its code is developed primarily in Rust (59.8%) and TypeScript (37.3%). The repository has 48 releases, with the most recent being v0.0.55, dated July 20, 2025.   Related Topics: Includes agent, management, kanban, task-manager, and ai-agents.",
    "categories": [
      "AI Agents" ,
      "Agent Development" ,
      "Multi-Agent Systems" ,
      "MCP (Model Context Protocol)" ,
      "Workflow Automation" ,
      "Systems Integration" ,
      "AI Applications" ,
      "Large Language Models (LLMs)" ,
      "LLMOps" 
    ]
  },
   {
    "name": "Magnific AI",
    "link": "https://magnific.ai",
    "difficulty": "Unspecified",
    "type": "Tool",
    "description": "Magnific AI is a generative Artificial Intelligence tool designed to upscale, enhance, transform, and generate images, offering insanely high-resolution image upscaling that 'feels like magic' [1-3]. It allows users to reimagine as many details as they wish, guided by a text prompt and various parameters or controls [1-3].Key Features and Functionality:   Advanced AI Technology: It is powered by Generative AI to achieve incredibly high-detail image upscaling and enhancement . The technology aims to provide new levels of resolution and detail in AI generations, photos, and illustrations .   Process Control: Users can direct the upscaling process using a text description and sliders such as 'Creativity', 'HDR', and 'Resemblance' . The 'Creativity' slider specifically allows control over the level of 'hallucinations' (new details generated by the AI) that users want to add . Most artifacts, if they occur, can be controlled using these parameters .   Versatile Applications: Magnific AI is ideal for enhancing a wide range of image types, including portraits, illustrations, graphic designs, video game assets, landscapes, 3D renders, science fiction, fantasy, horror, films, photography, interior design, and food photoshoots . It has also been used for vintage photo restoration , beautifying Minecraft builds , and in the creation of AI-powered animations .   User-Oriented Design: The tool is designed to be accessible and user-friendly for creators of all backgrounds and skill levels . It features an intuitive interface, in-depth tutorials, and a community to support the user's creative journey .   Broad Target Audience: Magnific AI caters to professionals and enthusiasts in photography, graphic design, digital art, and illustration who require high-resolution images and meticulous detail enhancements . It also serves AI artists and creators looking to upscale their AI-generated images for more resolution and depth, as well as businesses and individuals aiming to enhance their visual content .   Developers: It was built by Javi Lopez (@javilopen) and Emilio Nicolas (@emailnicolas), two indie entrepreneurs known for other projects .   Community Reception and Capabilities: The project has received considerable community support and praise, with notable users like Elon Musk , Emad Mostaque , Claire Silver , and Beeple  expressing astonishment at the quality and detail it can add. It has demonstrated the ability to upscale images 8x, from 1.2MP to 77MP (resulting in 64x more pixels) , and even 16x upscaling/enhancement has been reported .   Pricing and Payments: Applicable prices are displayed on their pricing page, potentially in local currency, and VAT/local taxes may be added . An annual subscription offers two months free . Payments are accepted via credit or debit cards (including VISA, MasterCard, American Express, iDeal, SOFORT, and many more) . Financial processing is handled by Stripe, ensuring a 100% secure payment service without retaining card details . Currently, PayPal or cryptocurrency are not accepted .   Cancellation and Refunds: Subscriptions can be canceled at any time through Stripe's billing portal . However, refunds are not offered due to the substantial expenses associated with the AI-driven GPU processing time required for image upscaling, as their upstream providers do not offer refunds for this service . Users agree to waive their right to a refund for this reason upon signup .",
    "categories": [
      "Generative AI",
      "AI Applications",
      "Content Generation with AI",
      "Image Analysis with AI",
      "Prompt Engineering"
    ]
  },
   {
    "name": "Agent File (.af)",
    "link": "https://github.com/letta-ai/agent-file",
    "difficulty": "Unspecified",
    "type": "Tool",
    "description": "The Agent File (.af) is an open file format for serializing stateful AI agents with persistent memory and behavior [1-3]. Originally designed for the Letta framework, this standard provides a portable way to share agents with their complete state, enabling easy checkpointing and version control across compatible frameworks [1-3].Key Purposes of Agent File (.af):   Portability: Allows agents to be moved between different systems or deployed to new environments .   Collaboration: Facilitates sharing agents with other developers and the community .   Preservation: Enables archiving agent configurations to preserve your work .   Versioning: Provides a standardized format for tracking changes to agents over time .Included State: An .af file packages all components necessary to re-create the exact same stateful agent . This comprehensive state includes:   System prompts : Initial instructions that define the agent's behavior.   Editable memory : Such as personality and user information.   Tool configurations : Complete tool definitions including source code and JSON schema.   LLM settings : Like model name, context window limit, and embedding model name.   Message history : Complete chat history with an in_context field indicating if a message is in the current context window.   Memory blocks : In-context memory segments.   Tool rules : Definitions of how tools should be sequenced or constrained.   Environment variables : Configuration values for tool execution.Current Limitations & Roadmap:   The format currently does not support Passages (the units of Archival Memory in Letta/MemGPT), but support for them is on the roadmap .   Future plans for the .af format include support for MCP servers/configs, archival memory passages, data sources (e.g., files), migration support between schema changes, multi-agent .af files, and converters between different frameworks .Compatibility with other Frameworks: While originally for Letta, other frameworks could theoretically load .af files by converting the state into their own representations . Adding support involves mapping Agent File components to the framework's equivalent featureset and implementing import/export functionality . Concepts like context window blocks might need adaptation if not natively supported by other frameworks .Secrets Handling: When agents with associated secrets for tool execution are exported, the secrets are automatically set to null within the .af file for security .Usage with Letta:   Users can import and export .af files to and from any Letta Server (self-deployed with Docker, Letta Desktop, or via Letta Cloud) .   This can be done using the visual Agent Development Environment (ADE), or through REST APIs or developer SDKs (available for Python and TypeScript) [7-11].   Example agents are available for download from the repository, including MemGPT, Deep Research, Customer Support, Stateless Workflow, and Composio Tools agents, each with separate instructions for use .Community and Development:   The project is open-source under the Apache-2.0 license .   It has garnered significant community interest, boasting 875 stars and 80 forks on GitHub .   As of the source material, it has 3 contributors .   Community contributions are welcomed and encouraged through various means, such as sharing example agents by opening pull requests, joining discussions on their Discord server, providing feedback via GitHub issues (for suggestions, features, or compatibility challenges), and helping refine the format as it evolves .",
    "categories": [
      "AI Agents",
      "Agent Development",
      "Multi-Agent Systems",
      "Agent Memory",
      "MCP (Model Context Protocol)",
      "Workflow Automation",
      "Systems Integration"
    ]
  },
  {
    "name": "opencode: AI Coding Agent",
    "link": "https://github.com/sst/opencode",
    "difficulty": "Unspecified",
    "type": "Tool",
    "description": "opencode is an AI coding agent built for the terminal [1-3]. It is an open-source project licensed under the MIT license [3-5].Key Features and Philosophy:   Provider-Agnostic: Unlike tools coupled to specific AI providers, opencode is designed to work with various models, including Anthropic (recommended), OpenAI, Google, or even local models. This emphasizes its adaptability as models evolve and pricing changes .   Terminal-Focused User Interface (TUI): Developed by Neovim users and the creators of terminal.shop, opencode aims to push the boundaries of what's possible within the terminal environment .   Client/Server Architecture: This design allows for flexible deployment, such as running the agent on one computer while controlling it remotely, for example, from a mobile application. The TUI frontend is just one potential client .Installation:   Installation can be done via a YOLO curl command (curl -fsSL https://opencode.ai/install | bash), package managers like npm, bun, pnpm, yarn (npm i -g opencode-ai@latest), or homebrew for macOS (brew install sst/tap/opencode), and paru for Arch Linux (paru -S opencode-bin) .   It's advised to remove versions older than 0.1.x before installing .   The install script respects a priority order for the installation path: $OPENCODE_INSTALL_DIR, $XDG_BIN_DIR, $HOME/bin, and finally $HOME/.opencode/bin as a default fallback .Contributing:   While opencode is an opinionated tool and does not accept Pull Requests (PRs) for core features, contributions are welcomed for bug fixes, improvements to LLM performance, support for new providers, fixes for environment-specific quirks, missing standard behavior, and documentation .   To run opencode locally, Bun and Golang 1.24.x are required .Community and Development:   The project has garnered significant community interest with 15k stars and 918 forks on GitHub .   It lists 110 contributors .   The primary languages used in its development are TypeScript (46.4%) and Go (46.2%) .   Users can join the community via Discord, YouTube, or X.com .",
    "categories": [
      "AI Agents",
      "Agent Development",
      "Software Development",
      "Workflow Automation",
      "Systems Integration"
    ]
  },
  {
    "name": "Gemini for Google Workspace: Prompting Guide",
    "link": "https://www.gstatic.com/bricks/pdf/5ed3a163-8d70-4ece-96f0-0fe228765889/Gemini-for-google-workspace-prompting-guide_ES.pdf",
    "difficulty": "Beginner",
    "type": "Book",
    "description": "This guide, titled 'Introducción a la creación de instrucciones para Gemini for Google Workspace' (Introduction to Prompt Creation for Gemini for Google Workspace) , is an essential resource for learning to write effective prompts to maximize the benefits of Generative AI within Google Workspace applications . It aims to provide users with basic skills to enhance productivity and efficiency in daily tasks .Key Concepts of Prompting:   Prompt as a Question: A prompt is considered a question to initiate a conversation with an AI-powered assistant . The process often involves writing multiple prompts as the conversation progresses .   Four Main Areas for Effective Prompts: The guide highlights four crucial elements to consider: Persona, Task, Context, and Format . While not all four are always necessary, using some of them is beneficial . An example prompt utilizing all four areas is provided for Gmail and Google Docs .Tips for Effective Prompt Writing:   Use Natural Language: Write as if speaking to another person, using complete sentences .   Be Specific and Iterate: Clearly state what Gemini for Workspace needs to do (e.g., summarize, write, change tone, create content) and provide as much context as possible. Adjust and refine prompts iteratively if results are not satisfactory [13-15].   Be Brief and Avoid Complexity: Frame requests concisely and specifically, avoiding jargon. Successful prompts average 21 words .   Make it a Conversation: Engage in a back-and-forth dialogue with the AI to refine outputs .   Break Down Tasks: For multiple related tasks, divide them into separate prompts .   Set Limits: Include details like character counts or the number of options desired to generate specific results .   Assign a Role: Encourage creativity by assigning a persona to Gemini (e.g., 'You are a Program Manager in Google Cloud.') .   Ask for Feedback: In a conversation with Gemini, describe the project and desired outcome, then ask questions like 'What questions do you have for me that could help you provide the best result?' .   Consider the Tone: Adapt prompts to the target audience and desired tone (formal, informal, technical, creative, casual) .Integration and Capabilities with Google Workspace:   Gemini's Generative AI features are seamlessly integrated into everyday applications such as Gmail, Google Docs, Google Sheets, and Google Slides, and Google Meet . Users can also chat with Gemini at gemini.google.com .   It can assist with improving writing, organizing data, creating original images, summarizing information, displaying statistics, fostering connections, researching unfamiliar topics, detecting trends, identifying business opportunities, and synthesizing information .   Specific features include 'Help me write' (Docs, Gmail), 'Help me organize' (Sheets), 'Create image with Gemini' (Slides), and 'Create background images' (Meet) .Important Considerations:   Generative AI is still evolving, and responses can be unpredictable .   Always review AI-generated output for clarity, relevance, and accuracy before applying it . The final result remains the user's responsibility .Target Audience and Use Cases:   The guide includes practical situations and example prompts for various roles, including Customer Service, Executives and Entrepreneurs (e.g., CEO, COO, CMO, CTO, CIO), Human Resources (e.g., HR Manager, Recruiter), Marketing (e.g., Brand Manager, Digital Marketing Manager, Content Marketing Manager), Project Management, and Sales (e.g., Account Executive, Sales Director, Account Manager, Business Development Manager) .",
    "categories": [
      "Prompt Engineering",
      "Generative AI",
      "Large Language Models (LLMs)",
      "AI Assistants",
      "AI Applications",
      "Workflow Automation",
      "Introduction to AI and Fundamental Concepts"
    ]
  },
  {
    "name": "AI Business Consultant Agent with Gemini 2.5 Flash",
    "link": "https://www.theunwindai.com/p/build-an-ai-consultant-agent-with-gemini-2-5-flash",
    "difficulty": "Unspecified",
    "type": "Course",
    "description": "The AI Business Consultant Agent is a fully functional, open-source agentic application built for business consultation . This powerful AI business consultant is developed using Google's Agent Development Kit (ADK) and leverages the Gemini 2.5 Flash model for its AI capabilities . It is combined with Perplexity AI for real-time web research to provide current market data, trends, and competitor intelligence . The application is designed to offer real-time insights, data-driven strategies, and rapid responses to market changes for various business needs .Key Features of the AI Business Consultant Agent include :   Real-time Web Research: Gathers current market data and trends using Perplexity AI search.   Market Analysis: Processes research data to generate structured insights with confidence scores.   Strategic Recommendations: Creates actionable business advice, complete with timelines and implementation plans.   Risk Assessment: Identifies potential risks and provides mitigation strategies.   Interactive UI: Features a clean web interface powered by Google ADK for easy user consultation.   Evaluation System: Includes built-in evaluation and debugging capabilities with session tracking.How it Works: Users initiate a consultation by submitting business questions through the Google ADK web interface . The agent then conducts real-time web research using Perplexity AI , followed by market analysis to process the data and generate structured insights . Strategic recommendation tools are used to create actionable business advice . Finally, the agent synthesizes its web research findings and analysis results into a comprehensive consultation report, providing users with professional consultation that includes citations, action items, and measurable success metrics .Technical Details and Setup: To build this agent, users are recommended to have Python (version 3.10 or higher) installed, along with their Google Gemini and Perplexity API keys . Basic familiarity with Python programming is also a prerequisite . The code walkthrough involves cloning the awesome-llm-apps GitHub repository, navigating to the ai_consultant_agent folder, installing required dependencies using pip install -r requirements.txt, and setting up API keys as environment variables . The main implementation resides in ai_consultant_agent.py, while agent.py and __init__.py are essential files for Google ADK's web interface discovery system . The agent's behavior is defined by its instructions, which configure it as a \"senior AI business consultant specializing in market analysis and strategic planning\" . Once set up, the app can be launched via the Google ADK web interface, typically accessed at http://localhost:8000 .Community and Expandability: This project is part of the broader awesome-llm-apps repository, which has garnered significant community support with 45.7k stars and 5.2k forks on GitHub . Future expansion possibilities for the consultant agent include industry specialization, integration with business intelligence tools or CRM systems, implementation of advanced evaluation metrics, and the addition of collaboration features for team strategic planning sessions . The project is open-source and encourages community contributions and sharing on social channels .",
    "categories": [
      "AI Agents",
      "Agent Development",
      "Large Language Models (LLMs)",
      "AI Applications",
      "Workflow Automation",
      "Systems Integration",
      "Prompt Engineering",
      "Data Analysis",
      "Web Scraping with AI"
    ]
  },
    {
    "name": "RunAgent",
    "link": "https://github.com/runagent-dev/runagent",
    "difficulty": "Unspecified",
    "type": "Tool",
    "description": "RunAgent is an AI Agent Deployment Platform designed to simplify the serverless deployment of AI agents . It provides a powerful Command Line Interface (CLI), multi-language SDK support, and built-in agent invocation and streaming support . The platform aims to allow developers to focus on agent development rather than repetitive processes like implementing REST and streaming APIs .Core Purpose and Philosophy:   Universal AI Agent Platform: RunAgent allows you to write AI agents once in Python and access them natively from any language .   Simplified Deployment: It simplifies the serverless deployment of AI agents, whether locally or on its upcoming cloud platform .   Developer Focus: The platform is built for developers who are using AI agents to empower or enhance their products, freeing them from concerns about deployment complexities .Key Features:   Powerful CLI: Deploy AI agents using the RunAgent CLI with a simple configuration file . Commands include init, serve, deploy, run, setup, template, upload, start, and teardown .   Multi-language SDKs: Provides native-feeling SDKs for Python, JavaScript/TypeScript, Rust, and Go with full type safety, intellisense, automatic error handling, and seamless authentication . This eliminates the need to wrestle with REST APIs directly .   Native Streaming Support: One of its most powerful features is native streaming support across all languages, allowing real-time agent responses that feel natural in each programming language, using iterators in Python, async iterators in JavaScript, futures streams in Rust, and context-aware iterators in Go .   Local Development Environment: Offers a powerful local development environment with a built-in FastAPI server that includes hot reload, logging, monitoring, and debugging tools .   Framework Agnostic: RunAgent works with any Python AI framework, including LangChain, LangGraph, CrewAI, Letta, Agno, or custom frameworks, providing pre-built templates and specific guides .   Smart Architecture: Supports local development now and provides a clear path to production-scale serverless cloud deployment in the future .   Production Ready (Self-Hosting): The current open-source foundation is suitable for self-hosting and enterprise deployments .   Agent Lifecycle Management: Features smart project initialization, environment management, and one-command deployment .   Webhooks: Supports webhook integrations for event-driven architectures .How it Works (Quick Start):1.  Installation: Install the RunAgent CLI and Python SDK using pip install runagent .2.  Initialize from a Template: Create a new agent project using runagent init <project_name> with options for specific frameworks like --langgraph or --crewai .3.  Configure Your Agent: The crucial runagent.config.json file defines entrypoints, which are the functions from your codebase that will be exposed through the RunAgent server and accessible via SDKs . For streaming entrypoints, the tag should include a _stream suffix to indicate streaming capability . Example Python functions mock_response and mock_response_stream are shown as entrypoints .4.  Serve Your Agent Locally: Start a local development server using runagent serve . (if in the project directory) or runagent serve <project_dir/name> . This provides an agent_id, host:port URL, WebSocket endpoints, and a development dashboard for monitoring .5.  Use a Deployed Agent: Connect to your agent using the RunAgentClient from any RunAgent SDK, specifying the agent_id or host:port and the entrypoint_tag . You can then invoke the run method like a native function . For streaming functions, you can iterate over the returned object naturally.Target Audience & Use Cases:RunAgent is perfect for polyglot teams where Python AI experts collaborate with frontend (JS/TS) and backend (Rust/Go) developers . It is also suitable for microservices architecture, enabling centralized AI logic with distributed access, legacy integration for adding AI capabilities without rewrites, performance-critical apps by consuming Python AI logic from high-performance languages, and rapid prototyping with a seamless path to production .Roadmap and Future Plans:   Foundation (Available Now): The current open-source foundation provides cross-language SDKs, a local development server, framework-agnostic support, pre-built templates, a comprehensive CLI, real-time debugging, and is suitable for self-hosting and enterprise deployments .   Cloud Platform (Coming Q2 2025): This serverless cloud platform will offer one-command deployment, auto-scaling from zero to thousands of requests, a global edge network for minimal latency, built-in monitoring, webhook integrations, team collaboration features, usage analytics, and an API Gateway .   Enterprise Features (Coming 2025): An enterprise tier will provide private cloud deployment (AWS, GCP, Azure), advanced security (SOC2 Type II compliance, encryption), compliance features (GDPR, HIPAA), role-based access control, custom SLAs (99.9% uptime), custom runtime environments, enterprise integrations (SSO, LDAP), and advanced analytics .Community and Contribution:RunAgent is hosted on GitHub as runagent-dev/runagent . It has 247 stars and 32 forks , with 4 contributors . Its primary languages are Python (53.1%), Rust (30.5%), Go (9.7%), and TypeScript (5.4%) . RunAgent welcomes community contributions, including bug reports, feature requests, code contributions, documentation improvements, community support, and SDK development . Users can join the community via Discord, GitHub Discussions, or Twitter .License:RunAgent is licensed under the Elastic License 2.0 (ELv2) . This license allows free use for development, testing, and production, as well as modification and distribution . However, it cannot be offered as a managed service without permission .",
    "categories": [
      "AI Agents",
      "Agent Development",
      "AI Deployment",
      "LLMOps",
      "Large Language Models (LLMs)",
      "Systems Integration",
      "Workflow Automation"
    ]
  },
  {
    "name": "any-llm",
    "link": "https://github.com/mozilla-ai/any-llm",
    "difficulty": "Unspecified",
    "type": "Tool",
    "description": "any-llm is a single interface designed to communicate with different Large Language Model (LLM) providers [1-3]. Developed by mozilla-ai, it aims to address the fragmented ecosystem of LLM provider interfaces where variations exist despite the OpenAI API becoming a de facto standard . The project provides a light wrapper to handle these differences consistently .Key Features offered by any-llm include :   Simple, unified interface: Allows switching models with just a string change using a single function for all providers.   Developer friendly: Provides full type hints for better IDE support and clear, actionable error messages.   Leverages official provider SDKs: Reduces maintenance burden and ensures compatibility by utilizing official SDKs when available.   Framework-agnostic: Can be used across various projects and use cases.   Actively maintained: The tool is used in Mozilla AI's own product, any-agent, ensuring continued support.   No Proxy or Gateway server required: Eliminates the need for setting up additional services to interact with LLM providers.The motivation behind any-llm is to overcome limitations of existing solutions, such as LiteLLM (which reimplements provider interfaces potentially leading to compatibility issues), AISuite (lacks active maintenance and modern typing), framework-specific solutions (causing fragmentation), and proxy-only solutions like OpenRouter and Portkey (which require a hosted proxy) .To get started, users need Python 3.11 or newer and API keys for their chosen LLM provider . Installation is done via pip, allowing users to include specific providers or install support for all using 'any-llm-sdk[all]' . API keys can be set as environment variables (e.g., MISTRAL_API_KEY) or passed directly as an api_key parameter in the completion call . Basic usage involves importing the completion function from any_llm and specifying the model parameter as <provider_id>/<model_id> (e.g., mistral/mistral-small-latest), along with messages .any-llm is licensed under the Apache-2.0 license . The GitHub repository, mozilla-ai/any-llm, has garnered 558 stars and 31 forks, with 99.7% of its codebase in Python and 0.3% in Shell . Its topics include inference, text-completion, and LLMs .",
    "categories": [
      "Large Language Models (LLMs)",
      "Systems Integration",
      "AI Agents",
      "Agent Development",
      "AI Deployment",
      "LLMOps"
    ]
  }

]
